{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#easydel","title":"EasyDeL \ud83d\udd2e","text":"<p>EasyDeL is an open-source framework designed to enhance and streamline the training process of machine learning models. With a primary focus on Jax/Flax, EasyDeL aims to provide convenient and effective solutions for training Flax/Jax models on TPU/GPU for both serving and training purposes.</p>"},{"location":"#key-features","title":"Key Features","text":"<ol> <li> <p>Trainers: EasyDeL offers a range of trainers, including DPOTrainer, ORPOTrainer, SFTTrainer, and VideoCLM    Trainer, tailored for specific training requirements.</p> </li> <li> <p>Serving and API Engines: EasyDeL provides serving and API engines for efficiently using and serving large    language models (LLMs) in JAX, enabling seamless integration into various applications.</p> </li> <li> <p>Quantization Support: EasyDeL supports quantization methods for all models, allowing for efficient inference and    training.</p> </li> <li> <p>Bit Operation Support: EasyDeL supports 8, 6, and 4-bit operations for inference and training in JAX, optimizing    performance and resource utilization.</p> </li> <li> <p>Diverse Model Support: EasyDeL offers a wide range of models in JAX that have never been implemented before, such    as Falcon, Qwen2, Phi2, Mixtral, Qwen2Moe, Cohere, Dbrx, Phi3, and MPT.</p> </li> <li> <p>FlashAttention Integration: EasyDeL integrates FlashAttention in JAX for GPUs and TPUs, enhancing performance and    efficiency.</p> </li> <li> <p>Automatic LLM Serving: EasyDeL enables automatic serving of LLMs with mid and high-level APIs in both JAX and    PyTorch, simplifying deployment and integration.</p> </li> <li> <p>LLM Training and Fine-tuning: EasyDeL provides LLM trainer and fine-tuner capabilities in JAX, allowing for    efficient training and customization of language models.</p> </li> <li> <p>Video CLM Training and Fine-tuning: EasyDeL supports Video CLM trainer and fine-tuner for models such as Falcon,    Qwen2, Phi2, MPT, Mixtral, Grok-1, and Qwen2Moe, enabling advanced video-related applications.</p> </li> <li> <p>Performance Optimization: EasyDeL provides various features to enhance the training process and optimize     performance, such as LoRA (Low-Rank Adaptation of Large Language Models), RingAttention, FlashAttention, BlockWise     FFN, and Efficient Attention support (through the FJFormer backbone).</p> </li> <li> <p>Model Conversion: EasyDeL supports automatic conversion of models from JAX-EasyDeL to PyTorch-HF and vice versa,     facilitating seamless integration with different frameworks.</p> </li> </ol> <p>With its comprehensive set of features and tools, EasyDeL aims to streamline and accelerate the training and deployment of machine learning models, particularly in the domain of large language models and video-related applications.</p>"},{"location":"#what-makes-easydel-special","title":"What Makes EasyDeL \ud83d\udd2e Special","text":"<p>EasyDeL is built up on JAX and Flax and that's why EasyDeL can perform as fast and as easy as possible</p> <p>When comparing JAX to PyTorch and TensorFlow, there are several benefits to using JAX that are worth considering.</p> <ol> <li> <p>Performance: JAX provides excellent performance through its XLA (Accelerated Linear Algebra) backend, which can    optimize and compile your code for various hardware accelerators such as GPUs and TPUs. This can lead to significant    speed improvements for certain types of computations.</p> </li> <li> <p>Automatic Differentiation: JAX offers a powerful and flexible automatic differentiation system, which is    essential for training machine learning models. It allows for both forward-mode and reverse-mode automatic    differentiation, giving you more options for gradient computation.</p> </li> <li> <p>Functional Programming: JAX is built around functional programming concepts, which can lead to more composable    and modular code. This can make it easier to reason about your code and to create abstractions that are reusable    across different parts of your project.</p> </li> <li> <p>Interoperability with NumPy: JAX is designed to be compatible with NumPy, which means that you can often take    existing NumPy code and run it with minimal changes on JAX. This can be a significant advantage when transitioning    existing codebases to use JAX.</p> </li> <li> <p>Flexibility: JAX provides a high degree of flexibility, allowing you to drop down to lower-level abstractions    when needed. This can be particularly useful when implementing custom operations or experimenting with new research    ideas.</p> </li> </ol> <p>While JAX offers these benefits, it's important to note that PyTorch and TensorFlow have large and active communities, extensive libraries, and a wide range of pre-trained models, which can be advantageous in certain scenarios. Additionally, the choice of framework often depends on the specific requirements of the project and the familiarity of the team with a particular toolset.</p>"},{"location":"#hands-on-code-kaggle-examples","title":"Hands on Code Kaggle Examples","text":"<ol> <li>script for mindset of using EasyDeL    CausalLanguageModelTrainer on kaggle, but you can do much more.</li> <li>script for using and serving LLMs with EasyDeL    JAXServer API (Mixtral Example).</li> <li>script SuperVised Finetuning with EasyDeL.</li> </ol>"},{"location":"#citing-easydel","title":"Citing EasyDeL \ud83e\udd76","text":""},{"location":"#to-cite-this-project","title":"To cite this Project","text":"<pre><code>@misc{Zare Chavoshi_2023,\n    title={EasyDeL, an open-source library, is specifically designed to enhance and streamline the training process of machine learning models. It focuses primarily on Jax/Flax and aims to provide convenient and effective solutions for training Flax/Jax Models on TPU/GPU for both Serving and Training purposes.},\n    url={https://github.com/erfanzar/EasyDeL},\n    journal={EasyDeL Easy and Fast DeepLearning with JAX},\n    publisher={Erfan Zare Chavoshi},\n    author={Zare Chavoshi, Erfan},\n    year={2023}\n} \n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contributing to EasyDeL","text":"<p>Thank you for considering contributing to EasyDeL! We welcome your input. To ensure a smooth collaboration, please review and adhere to the following guidelines.</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":"<p>To contribute to EasyDeL, follow these steps: 1. Fork the repository. 2. Create a new branch for your feature or bug fix. 3. Make your changes and commit them with clear and descriptive messages. 4. Push your changes to your branch in your forked repository. 5. Submit a pull request to the main EasyDeL repository, detailing the changes you've made and the problem it solves.</p>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please adhere to the Apache Code of Conduct in all interactions related to EasyDeL.</p>"},{"location":"CONTRIBUTING/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you encounter a bug, please open an issue on the EasyDeL repository, providing a clear and detailed description of the issue, including steps to reproduce it.</p>"},{"location":"CONTRIBUTING/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>If you have ideas for enhancements, feel free to open an issue on the EasyDeL repository. Provide a clear and detailed description of your proposed enhancement.</p>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<p>To set up EasyDeL for development, follow the instructions in the README.md file.</p>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>When submitting a pull request, please ensure the following: - Your code follows the project's coding standards. - Your commits are accompanied by clear and descriptive messages. - Your pull request addresses a single issue or feature.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing to EasyDeL, you agree that your contributions will be licensed under the Apache License, Version 2.0.</p> <p>Thank you for your interest in contributing to EasyDeL! We appreciate your support.</p>"},{"location":"DataProcessing/","title":"DataProcessing","text":""},{"location":"DataProcessing/#data-processing","title":"Data Processing","text":"<p>here in this case you will see an example data required by EasyDeL to pre-train or fine-tune models</p> <pre><code>from datasets import load_dataset\nfrom easydel.data_preprocessing import DataProcessor, DataProcessorArguments\nfrom transformers import LlamaTokenizerFast\n\n\ndef main():\n    tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n    dataset = load_dataset(\"erfanzar/orca-lite\")\n    print(dataset)\n\n    #     DatasetDict({\n    #         train: Dataset({\n    #             features: ['user', 'gpt', 'system', 'llama_2_prompt_style', 'prompt_length'],\n    #             num_rows: 101397\n    #         })\n    #     })\n\n    processor_arguments = DataProcessorArguments(\n        max_position_embeddings=2048,\n        num_proc=6,\n        prompt_field='llama_2_prompt_style',\n\n    )\n\n    easydel_dataset = DataProcessor.process_data(\n        data=dataset['train'],\n        tokenizer=tokenizer,\n        arguments=processor_arguments,\n        field='train'\n    )\n    print(easydel_dataset)\n    # DatasetDict({\n    #     train: Dataset({\n    #         features: ['input_ids', 'attention_mask'],\n    #         num_rows: 101397\n    #     })\n    # })\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>now you can pass this data to Trainer and train your model \ud83d\ude07.</p>"},{"location":"EasyAttentionExample/","title":"AttentionModule","text":""},{"location":"EasyAttentionExample/#what-is-attentionmodule","title":"what is <code>AttentionModule</code>","text":"<p>AttentionModule is a EasyDeL module that can perform attention operation with different strategies to help user achieve the best possible performance and numerical stability, here are some strategies supported right now.</p> <ol> <li>Flash Attention TPU known as \"flash\"</li> <li>Flash Attention GPU known as \"cudnn\"</li> <li>Ring Attention to Support higher context length such 1 Million or above known as \"ring\"</li> <li>Normal Attention which use flax.linen.attention with shard map known as \"vanilla\"</li> <li>Splash Attention on TPUs which is known as \"splash\"</li> <li>Local Ring attention via \"local_ring\"</li> <li>Wise Ring attention via \"wise_ring\"</li> <li>sharded Attention with shard map known as \"sharded_vanilla\"</li> </ol>"},{"location":"EasyAttentionExample/#example-of-using-flash-attention-on-tpu","title":"Example of Using Flash Attention on TPU","text":"<pre><code>import jax\nimport flax.linen.attention as flt\nfrom fjformer import GenerateRNG\nfrom easydel.modules.attention_module import AttentionModule\nfrom easydel.modules.easydel_modelling_utils import EasyDeLPretrainedConfig\nfrom jax import numpy as jnp, random, lax\nimport math\n\nrng_gen = GenerateRNG(seed=42)\nconfig = EasyDeLPretrainedConfig(\n    axis_dims=(1, -1, 1, 1),\n    axis_names=(\"dp\", \"fsdp\", \"tp\", \"sp\"),\n    block_q=512,\n    block_k=512\n)\n\nBATCH_SIZE = len(jax.devices())\nNUM_ATTN_HEADS = 32\nCONTEXT_LENGTH = 8192\nHEAD_DIM = 256\n\n\ndef make_fake_input_data(\n        batch_size: int,\n        num_attention_head: int,\n        context_length: int,\n        head_dim: int,\n):\n    q = random.normal(next(rng_gen), (batch_size, context_length, num_attention_head, head_dim), dtype=jnp.float32)\n    k = random.normal(next(rng_gen), (batch_size, context_length, num_attention_head, head_dim), dtype=jnp.float32)\n    v = random.normal(next(rng_gen), (batch_size, context_length, num_attention_head, head_dim), dtype=jnp.float32)\n\n    attention_mask = jnp.ones((batch_size, context_length))\n    causal_mask = flt.make_causal_mask(attention_mask)\n\n    cm_ = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n    at_ = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), cm_.shape)\n    at_ = flt.combine_masks(at_, cm_)\n\n    attention_bias = lax.select(\n        at_ &gt; 0,\n        jnp.full(at_.shape, 0.0).astype(jnp.float32),\n        jnp.full(at_.shape, jnp.finfo(jnp.float32).min).astype(jnp.float32),\n    )\n\n    return (\n        q, k, v, attention_mask, causal_mask, attention_bias\n    )\n\n\nq, k, v, attention_mask, causal_mask, attention_bias = make_fake_input_data(\n    BATCH_SIZE,\n    NUM_ATTN_HEADS,\n    CONTEXT_LENGTH,\n    HEAD_DIM\n)\n\nflash_attention = AttentionModule(\n\n    block_k_major=config.block_k_major,\n    block_b=config.block_b,\n    block_q=config.block_q,\n    block_k=config.block_k,\n    block_q_major_dkv=config.block_q_major_dkv,\n    block_k_major_dkv=config.block_k_major_dkv,\n    block_k_major_dq=config.block_k_major_dq,\n    block_k_dkv=config.block_k_dkv,\n    block_q_dkv=config.block_q_dkv,\n    block_q_dq=config.block_q_dq,\n    block_k_dq=config.block_k_dq,\n    num_attention_heads=NUM_ATTN_HEADS,\n    attention_dropout=0.0,\n    head_dims=HEAD_DIM,\n    attention_partition_spec=config.attention_partition_spec,\n    shard_attention_computation=config.shard_attention_computation,\n    precision=lax.Precision(\"fastest\"),\n    force_float32_tpu=True,\n    attn_mechanism=\"flash\",\n    dtype=jnp.float32,\n    bias_partition_spec=config.bias_partition_spec,\n    key_partition_spec=config.key_partition_spec,\n    query_partition_spec=config.query_partition_spec,\n    generation_query_partition_spec=config.generation_query_partition_spec,\n    generation_bias_partition_spec=config.generation_bias_partition_spec,\n    value_partition_spec=config.value_partition_spec,\n    scan_ring_attention=config.scan_ring_attention,\n    mesh=config.jax_mesh(),\n    sm_scale=1 / math.sqrt(q.shape[-1]),\n)\n\nnormal_attention = AttentionModule(\n\n    block_k_major=config.block_k_major,\n    block_b=config.block_b,\n    block_q=config.block_q,\n    block_k=config.block_k,\n    block_q_major_dkv=config.block_q_major_dkv,\n    block_k_major_dkv=config.block_k_major_dkv,\n    block_k_major_dq=config.block_k_major_dq,\n    block_k_dkv=config.block_k_dkv,\n    block_q_dkv=config.block_q_dkv,\n    block_q_dq=config.block_q_dq,\n    block_k_dq=config.block_k_dq,\n    num_attention_heads=NUM_ATTN_HEADS,\n    attention_dropout=0.0,\n    head_dims=HEAD_DIM,\n    attention_partition_spec=config.attention_partition_spec,\n    shard_attention_computation=config.shard_attention_computation,\n    precision=lax.Precision(\"fastest\"),\n    force_float32_tpu=True,\n    attn_mechanism=\"normal\",\n    dtype=jnp.float32,\n    bias_partition_spec=config.bias_partition_spec,\n    key_partition_spec=config.key_partition_spec,\n    query_partition_spec=config.query_partition_spec,\n    generation_query_partition_spec=config.generation_query_partition_spec,\n    generation_bias_partition_spec=config.generation_bias_partition_spec,\n    value_partition_spec=config.value_partition_spec,\n    scan_ring_attention=config.scan_ring_attention,\n    mesh=config.jax_mesh(),\n    sm_scale=1 / math.sqrt(q.shape[-1]),\n)\n\nwith config.jax_mesh():\n    flash_attn_out = flash_attention(\n        query_states=q,\n        key_states=k,\n        value_states=v,\n        bias=attention_bias,\n        key_value_sequence_length=CONTEXT_LENGTH,\n        query_sequence_length=CONTEXT_LENGTH\n    )\n    normal_attn_out = normal_attention(\n        query_states=q,\n        key_states=k,\n        value_states=v,\n        bias=attention_bias,\n        key_value_sequence_length=CONTEXT_LENGTH,\n        query_sequence_length=CONTEXT_LENGTH\n    )\n\nprint(\n    flash_attn_out.attention_outputs[0, CONTEXT_LENGTH - 5, NUM_ATTN_HEADS - 1, HEAD_DIM - 10:]\n)\n# Array([-0.05915311,  0.0078501 ,  0.03785717,  0.0134844 ,  0.08464689,\n#        0.06667967, -0.02629154, -0.0180066 , -0.02972782,  0.02833381],      dtype=float32)\nprint(\n    normal_attn_out.attention_outputs[0, CONTEXT_LENGTH - 5, NUM_ATTN_HEADS - 1, HEAD_DIM - 10:]\n)\n\n# Array([-0.0590958 ,  0.00796138,  0.03789062,  0.01350671,  0.08461153,\n#        0.06662725, -0.0262386 , -0.01806086, -0.0296791 ,  0.02824247],      dtype=float32)\n</code></pre>"},{"location":"EasyStateExample/","title":"EasyState","text":""},{"location":"EasyStateExample/#easydelstate","title":"EasyDeLState","text":"<p>EasyDeLState is a cool feature in easydel and have a lot of options like storing <code>Model Parameters</code>, Optimizer State, Model Config, Model Type, Optimizer and Scheduler Configs</p> <p>Let see and examples of using EasyDeLState</p>"},{"location":"EasyStateExample/#fine-tuning","title":"Fine-tuning","text":"<p>Fine-tuning from a previous State or a new state</p> <pre><code>from easydel import (\n    AutoEasyDeLConfig,\n    EasyDeLState\n)\nfrom transformers import AutoTokenizer\nfrom jax import numpy as jnp, lax\nimport jax\n\nhuggingface_model_repo_id = \"REPO_ID\"\ncheckpoint_name = \"CKPT_NAME\"\n\nstate = EasyDeLState.from_pretrained(\n    pretrained_model_name_or_path=huggingface_model_repo_id,\n    filename=checkpoint_name,\n    optimizer=\"adamw\",\n    scheduler=\"none\",\n    tx_init=None,\n    device=jax.devices('cpu')[0],  # Offload Device\n    dtype=jnp.bfloat16,\n    param_dtype=jnp.bfloat16,\n    precision=lax.Precision(\"fastest\"),\n    sharding_axis_dims=(1, -1, 1, 1),\n    sharding_axis_names=(\"dp\", \"fsdp\", \"tp\", \"sp\"),\n    query_partition_spec=jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n    key_partition_spec=jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n    value_partition_spec=jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n    bias_partition_spec=jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n    attention_partition_spec=jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n    shard_attention_computation=False,\n    input_shape=(1, 1),\n    backend=None,\n    init_optimizer_state=False,\n    free_optimizer_state=True,\n    verbose=True,\n    state_shard_fns=None,\n)\n\nconfig = AutoEasyDeLConfig.from_pretrained(\n    huggingface_model_repo_id\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    huggingface_model_repo_id,\n    trust_remote_code=True\n)\n\nmax_length = config.max_position_embeddings\n\nconfigs_to_initialize_model_class = {\n    'config': config,\n    'dtype': jnp.bfloat16,\n    'param_dtype': jnp.bfloat16,\n    'input_shape': (8, 8)\n}\n</code></pre> <p><code>EasyDeLState</code> also has <code>.load_state()</code> and <code>.save_state()</code> with some other usable options like <code>.free_opt_state()</code> which free optimizer state or <code>.shard_params()</code> which shard parameters you can read docs in order to find out more about these options.</p>"},{"location":"EasyStateExample/#converting-to-huggingface-and-pytorch","title":"Converting to Huggingface and Pytorch","text":"<p>Let see how you can convert a EasyDeLMistral Model to Huggingface Pytorch Mistral Model from a trained State</p> <pre><code>\nfrom transformers import MistralForCausalLM\nfrom easydel import (\n    AutoEasyDeLConfig,\n    EasyDeLState,\n    easystate_to_huggingface_model\n)\nimport jax\n\nhuggingface_model_repo_id = \"REPO_ID\"\n\nconfig = AutoEasyDeLConfig.from_pretrained(\n    huggingface_model_repo_id\n)\nwith jax.default_device(jax.devices(\"cpu\")[0]):\n    model = easystate_to_huggingface_model(\n        state=EasyDeLState.load_state(\n            \"PATH_TO_CKPT\",\n            input_shape=(8, 2048)\n        ),  # You can Pass EasyDeLState here\n        base_huggingface_module=MistralForCausalLM,\n        config=config,\n    )\n\nmodel = model.half()  # it's a huggingface model now\n</code></pre>"},{"location":"EasyStateExample/#other-use-cases","title":"Other Use Cases","text":"<p><code>EasyDeLState</code> have a general use you can use it everywhere in easydel for example for a stand-alone model , serve, fine-tuning and many other features, it's up to you to test how creative you are \ud83d\ude07.</p>"},{"location":"FineTuningExample/","title":"Fine Tuning Example","text":""},{"location":"FineTuningExample/#finetuning-causal-language-model","title":"FineTuning Causal Language Model \ud83e\udd75","text":"<p>with using EasyDeL FineTuning LLM (CausalLanguageModels) are easy as much as possible with using Jax and Flax and having the benefit of <code>TPUs</code> for the best speed here's a simple code to use in order to finetune your own Model</p> <p>Days Has Been Passed and now using easydel in Jax is way more similar to HF/PyTorch Style now it's time to finetune our model.</p> <pre><code>import jax.numpy\nfrom easydel import (\n    TrainArguments,\n    CausalLanguageModelTrainer,\n    AutoEasyDeLModelForCausalLM,\n    EasyDeLOptimizers,\n    EasyDeLSchedulers,\n    EasyDeLGradientCheckPointers\n)\nfrom datasets import load_dataset\nimport flax\nfrom jax import numpy as jnp\nfrom transformers import AutoTokenizer\n\nhuggingface_repo_id_or_path = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(huggingface_repo_id_or_path, )\n\nmax_length = 2048\ntokenizer = AutoTokenizer.from_pretrained(\n    huggingface_repo_id_or_path,\n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel.config.add_basic_configurations(\n    attn_mechanism=\"flash\",  # Change to 'normal' if the model you are using\n    # don't support flash attention, or you don't want to apply flash attention for the model\n    block_b=1,\n    block_q=1024,\n    block_k=1024,\n    block_k_major=1024,\n)\n\nconfigs_to_initialize_model_class = {\n    \"config\": model.config,\n    \"dtype\": jnp.bfloat16,\n    \"param_dtype\": jnp.bfloat16,\n    \"input_shape\": (1, 1)\n}\n\ntrain_arguments = TrainArguments(\n    model_class=type(model),\n    model_name=\"my_first_model_to_train_using_easydel\",\n    num_train_epochs=3,\n    configs_to_initialize_model_class=configs_to_initialize_model_class,\n    learning_rate=5e-5,\n    learning_rate_end=1e-6,\n    optimizer=EasyDeLOptimizers.ADAMW,  # \"adamw\", \"lion\", \"adafactor\" are supported\n    scheduler=EasyDeLSchedulers.LINEAR,\n    # \"linear\",\"cosine\", \"none\" ,\"warm_up_cosine\" and \"warm_up_linear\"  are supported\n    weight_decay=0.01,\n    total_batch_size=64,\n    max_training_steps=None,  # None to let trainer Decide\n    do_train=True,\n    do_eval=False,  # it's optional but supported \n    backend=\"tpu\",  # default backed is set to cpu, so you must define you want to use tpu cpu or gpu\n    max_length=max_length,  # Note that you have to change this in the model config too\n    gradient_checkpointing=EasyDeLGradientCheckPointers.NOTHING_SAVEABLE,\n    sharding_array=(1, -1, 1, 1),  # the way to shard model across gpu,cpu or TPUs using sharding array (1, -1, 1, 1)\n    # everything training will be in fully FSDP automatic and share data between devices\n    remove_ckpt_after_load=True,\n    gradient_accumulation_steps=8,\n    loss_re_mat=\"\",\n    dtype=jnp.bfloat16\n)\n\n\ndef ultra_chat_prompting_process(\n        data_chunk\n):\n    user_part = [\n        chunk[\"content\"] for chunk in data_chunk[\"messages\"] if chunk[\"role\"] == \"user\"\n    ]\n    assistant_part = [\n        chunk[\"content\"] for chunk in data_chunk[\"messages\"] if chunk[\"role\"] == \"assistant\"\n    ]\n\n    prompt = \"\"\n\n    for uc, ac in zip(user_part, assistant_part):\n        prompt += f\"&lt;|user|&gt;\\n{uc}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n{ac}&lt;/s&gt;\\n\"\n\n    return {\"prompt\": prompt}\n\n\ntokenization_process = lambda data_chunk: tokenizer(\n    data_chunk[\"prompt\"],\n    add_special_tokens=False,\n    max_length=max_length,\n    padding=\"max_length\"\n)\n\ndataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\ndataset_train = dataset[\"train_gen\"].map(ultra_chat_prompting_process, num_proc=12)\ndataset_train = dataset_train.map(\n    tokenization_process,\n    num_proc=12,\n    remove_columns=dataset_train.column_names\n)\n\n# you can do the same for evaluation process dataset\n\ntrainer = CausalLanguageModelTrainer(\n    train_arguments,\n    dataset_train,\n    checkpoint_path=None\n)\n\noutput = trainer.train(flax.core.FrozenDict({\"params\": params}))\nprint(f\"Hey ! , here's where your model saved {output.checkpoint_path}\")\n</code></pre>"},{"location":"Install/","title":"Installing EasyDeL","text":"<p>EasyDeL uses FJFormer and JAX as main dependencies in order to run the scripts but there are some things that needs to be installed such as GO-lang to JAX specific platform installations, but you can simply install EasyDeL via pip:</p> <pre><code>pip install easydel\n</code></pre>"},{"location":"Install/#installing-jax","title":"Installing Jax","text":"<p>JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, jit.</p> <p>you can install other version too but easydel required at least version of 0.4.16</p>"},{"location":"Install/#tpu","title":"TPU","text":"<pre><code>!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -q\n</code></pre>"},{"location":"Install/#gpu","title":"GPU","text":""},{"location":"Install/#cuda-12","title":"CUDA-12","text":"<pre><code>pip install --upgrade pip\n# CUDA 12 installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>"},{"location":"Install/#cuda-11","title":"CUDA-11","text":"<pre><code>pip install --upgrade pip\n# CUDA 11 installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>"},{"location":"Install/#installing-go","title":"Installing GO","text":""},{"location":"Install/#note-this-library-needs-golang-to-run-for-some-tracking-stuff-on-tpugpucpu","title":"Note this Library needs golang to run (for some tracking stuff on TPU/GPU/CPU)","text":""},{"location":"Install/#ubuntu-go-installation","title":"Ubuntu GO installation","text":"<pre><code>sudo apt-get update &amp;&amp; apt-get upgrade -y\nsudo apt-get install golang -y \n</code></pre>"},{"location":"Install/#manjaroarch-go-installation","title":"Manjaro/Arch GO installation","text":"<pre><code>sudo pacman -Syyuu go\n</code></pre>"},{"location":"LoRA-TransferLearningExample/","title":"LoRA and Transfer Learning","text":""},{"location":"LoRA-TransferLearningExample/#easydelxrapture-for-layer-tuning-and-lora","title":"EasyDeLXRapTure for layer tuning and LoRA","text":"<p>in case of using LoRA and applying that on the EasyDeL models there are some other things that you might need to config on your own but a lot of things being handled by EasyDeL so let just jump into an example for LoRA fine-tuning section and use EasyDeLXRapTure in for mistral models with flash attention example</p> <pre><code>from flax.core import FrozenDict\nfrom easydel import (\n    TrainArguments,\n    CausalLanguageModelTrainer,\n    AutoEasyDeLModelForCausalLM,\n    EasyDeLOptimizers,\n    EasyDeLSchedulers,\n    EasyDeLGradientCheckPointers,\n    EasyDeLXRapTureConfig\n)\nfrom datasets import load_dataset\nimport flax\nfrom jax import numpy as jnp\nfrom transformers import AutoTokenizer\n\nhuggingface_repo_id_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(huggingface_repo_id_or_path, )\n\nmax_length = 8196\nmodel_parameters = FrozenDict({\"params\": params})\n\ndtype = jnp.bfloat16\nparam_dtype = jnp.bfloat16  # you can change that if you want \n\ntokenizer = AutoTokenizer.from_pretrained(\n    huggingface_repo_id_or_path,\n    trust_remote_code=True\n)\n\nmodel.config.add_basic_configurations(\n    attn_mechanism=\"flash\",  # Using FlashAttention\n    block_b=1,\n    block_q=1024,\n    block_k=1024,\n    block_k_major=1024,\n)\n\ntokenizer.pad_token = tokenizer.eos_token\nconfigs_to_initialize_model_class = {\n    \"config\": model.config,\n    \"dtype\": dtype,\n    \"param_dtype\": param_dtype,\n    \"input_shape\": (1, 1)\n}\n\nrapture = EasyDeLXRapTureConfig(\n    parameters=model_parameters,\n    lora_dim=64,\n    fully_fine_tune_parameters=[\"embed_tokens\"],  # Model layer to be fully fine tuned\n    lora_fine_tune_parameters=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # LoRA Layer Targets you can pass this to none\n    # For only Layer Tuning or transfer learning\n    verbose=True\n)\n\ntrain_arguments = TrainArguments(\n    model_class=type(model),\n    model_name=\"EasyDeL-Lora-Example\",\n    num_train_epochs=3,\n    configs_to_initialize_model_class=configs_to_initialize_model_class,\n    learning_rate=1e-4,  # Using higher learning rate is recommended\n    learning_rate_end=8e-5,\n    optimizer=EasyDeLOptimizers.ADAMW,  # \"adamw\", \"lion\", \"adafactor\" are supported\n    scheduler=EasyDeLSchedulers.LINEAR,\n    # \"linear\",\"cosine\", \"none\" ,\"warm_up_cosine\" and \"warm_up_linear\"  are supported\n    weight_decay=0.01,\n    total_batch_size=512,\n    max_training_steps=None,  # None to let trainer Decide\n    do_train=True,\n    do_eval=False,  # it's optional but supported \n    backend=\"tpu\",  # default backed is set to cpu, so you must define you want to use tpu cpu or gpu\n    max_length=max_length,  # Note that you have to change this in the model config too\n    gradient_checkpointing=EasyDeLGradientCheckPointers.NOTHING_SAVEABLE,\n    sharding_array=(1, -1, 1, 1),  # the way to shard model across gpu,cpu or TPUs using sharding array (1, -1, 1, 1)\n    # everything training will be in fully FSDP automatic and share data between devices\n    remove_ckpt_after_load=True,\n    gradient_accumulation_steps=1,\n    loss_re_mat=\"\",\n    dtype=dtype,\n    param_dtype=param_dtype,\n    rapture_config=rapture,\n    merge_lora_rapture_parameters=True  # turning this off is still not supported and not recommended to do so\n    # What this does ? this will merge the lora parameters with the original model parameters and the end of training\n)\n\n\ndef ultra_chat_prompting_sample(\n        data_chunk\n):\n    user_part = [\n        chunk[\"content\"] for chunk in data_chunk[\"messages\"] if chunk[\"role\"] == \"user\"\n    ]\n    assistant_part = [\n        chunk[\"content\"] for chunk in data_chunk[\"messages\"] if chunk[\"role\"] == \"assistant\"\n    ]\n\n    prompt = \"\"\n\n    for uc, ac in zip(user_part, assistant_part):\n        prompt += f\"&lt;|user|&gt;\\n{uc}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n{ac}&lt;/s&gt;\\n\"\n\n    return {\"prompt\": prompt}\n\n\ntokenization_process = lambda data_chunk: tokenizer(\n    data_chunk[\"prompt\"],\n    add_special_tokens=False,\n    max_length=max_length,\n    padding=\"max_length\"\n)\n\ndataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\ndataset_train = dataset[\"train_gen\"].map(ultra_chat_prompting_process, num_proc=12)\ndataset_train = dataset_train.map(\n    tokenization_process,\n    num_proc=12,\n    remove_columns=dataset_train.column_names\n)\n\n# you can do the same for evaluation process dataset\n\ntrainer = CausalLanguageModelTrainer(\n    train_arguments,\n    dataset_train,\n    checkpoint_path=None\n)\n\noutput = trainer.train()  # you should not pass the parameters in Trainer.train anymore when\n# you are using LoRA or transfer Learning\nprint(f\"Hey ! , here's where your model saved {output.checkpoint_path}\")\n</code></pre>"},{"location":"Parameter-Quantization/","title":"What's 8-bit quantization? How does it help ?","text":"<p>Quantization in the context of deep learning is the process of constraining the number of bits that represent the weights and biases of the model.</p> <p>Weights and Biases numbers that we need in backpropagation.</p> <p>In 8-bit quantization, each weight or bias is represented using only 8 bits as opposed to the typical 32 bits used in single-precision floating-point format (float32).</p>"},{"location":"Parameter-Quantization/#why-does-it-use-less-gputpu-memory","title":"Why does it use less GPU/TPU Memory?","text":"<p>The primary advantage of using 8-bit quantization is the reduction in model size and memory usage. Here's a simple explanation:</p> <p>A float32 number takes up 32 bits of memory. A 8-bit quantized number takes up only 8 bits of memory. So, theoretically, you can fit 4 times more 8-bit quantized numbers into the same memory space as float32 numbers. This allows you to load larger models into the GPU memory or use smaller GPUs that might not have been able to handle the model otherwise.</p> <p>The amount of memory used by an integer in a computer system is directly related to the number of bits used to represent that integer.</p> <p>Memory Usage for 8-bit Integer A 8-bit integer uses 8 bits of memory.</p> <p>Memory Usage for 32-bit Integer A 32-bit integer uses 32 bits of memory.</p> <p>Conversion to Bytes To convert these to bytes (since memory is often measured in bytes):</p> <ul> <li>1 byte = 8 bits</li> <li>8-bit integer would use ( 8/8 = 1 ) bytes.</li> <li>A 16-bit integer would use ( 16/8 = 2 ) bytes.</li> </ul>"},{"location":"Parameter-Quantization/#example-of-using-parameters-quantization-in-easydel","title":"Example of Using Parameters Quantization in EasyDeL","text":"<p>in case of serving models or using them with <code>JAX</code> The Easiest and the best way you can find is EasyDeL (you can explore more if you want) you have 4 ways to use models</p> <ol> <li>Create The Pipeline and everything from scratch yourself.</li> <li>Use JAXServer API from EasyDeL.</li> <li>use ServeEngine from EasyDeL.</li> <li>use builtin generate method from HuggingFace Transformers and EasyDeL</li> </ol> <p>let assume we want to run a 7B model on only 12 GB of vram let just jump into codding</p>"},{"location":"Parameter-Quantization/#using-quantized-model-via-generate-function","title":"Using Quantized Model via generate Function","text":"<p>let assume we want to run <code>Qwen/Qwen1.5-7B-Chat</code></p> <pre><code>from jax import numpy as jnp\nfrom easydel import AutoEasyDeLModelForCausalLM, create_generate_function\n\nfrom transformers import AutoTokenizer, GenerationConfig\n\nimport pickle\nimport torch\n\nrepo_id = \"Qwen/Qwen1.5-7B-Chat\"\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n    repo_id,\n    sharding_axis_dims=(1, 1, 1, -1),\n    config_kwargs=dict(\n        gradient_checkpointing=\"\",\n        use_scan_mlp=False,  # Turn this one if you want to go beyond 32K sequence length.\n        shard_attention_computation=True,\n        use_sharded_kv_caching=True\n    ),\n    dtype=jnp.float16,\n    param_dtype=jnp.float16,\n    auto_shard_params=True,\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\"  # this one will be passed to transformers.AutoModelForCausalLM\n)\n\n# params is now an 8 Bit pytree.\n\ntokenizer = AutoTokenizer.from_pretrained(repo_id)\nmesh = model.config.jax_mesh()\n\ngen_fn = create_generate_function(\n    model,\n    GenerationConfig(\n        do_sample=True,\n        max_new_tokens=512,\n        pad_token_id=tokenizer.pad_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        temperature=0.2,\n        top_p=0.95,\n        top_k=10,\n        num_beams=1\n    ),\n    {\"params\": params},\n    return_prediction_only=True\n)\n\ntokenizer.padding_side = \"left\"\nencoded = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": \"generate an story about stars\"}],\n    return_tensors=\"np\",\n    return_dict=True,\n    max_length=512,\n    padding=\"max_length\",\n    add_generation_prompt=True\n)\n\nrep = 1  # in case that you are using fsdp instead of sequence sharing change this to your fsdp mesh shape \ninput_ids, attention_mask = encoded.input_ids.repeat(rep, 0), encoded.attention_mask.repeat(rep, 0)\nwith mesh:\n    response = gen_fn(\n        {\"params\": params},\n        input_ids,\n        attention_mask\n    )\n\n    response_string = tokenizer.decode(response[0], skip_special_tokens=True)\nprint(\n    f\"Model Response:\\n{response_string}\"\n)\n\n# you want to save these quantized parameters for later?\n\npickle.dump((model, params, tokenizer), open(\"EasyDeL-Qwen7B-Chat\", \"wb\"))\n\n# And load that like this ;)\n\n(model, params, tokenizer) = pickle.load(open(\"EasyDeL-Qwen7B-Chat\", \"wb\"))\n\n</code></pre>"},{"location":"generated-cli-cli/","title":"cli.cli","text":""},{"location":"generated-cli-train-cl_train_cli/","title":"cli.train.cl_train_cli","text":""},{"location":"generated-data_preprocessing-data_processor/","title":"data_preprocessing.data_processor","text":""},{"location":"generated-etils-auto_tx/","title":"etils.auto_tx","text":""},{"location":"generated-etils-auto_tx/#src.python.easydel.etils.auto_tx.get_optimizer_and_scheduler","title":"<code>get_optimizer_and_scheduler(optimizer, scheduler, steps, learning_rate=1e-05, learning_rate_end=1e-05, gradient_accumulation_steps=1, extra_optimizer_kwargs=None, weight_decay=0.02, warmup_steps=0)</code>","text":"<p>The get_optimizer_and_scheduler function is a helper function that returns an optimizer and scheduler     based on the parameters passed to it.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>AVAILABLE_OPTIMIZERS</code> <p>AVAILABLE_OPTIMIZERS: Choose the optimizer</p> required <code>scheduler</code> <code>AVAILABLE_SCHEDULERS</code> <p>AVAILABLE_SCHEDULERS: Determine the learning rate scheduler</p> required <code>steps</code> <code>int</code> <p>int: Specify the number of steps in the training process</p> required <code>learning_rate</code> <code>float</code> <p>float: Set the learning rate for the optimizer</p> <code>1e-05</code> <code>learning_rate_end</code> <code>float</code> <p>float: Set the final learning rate</p> <code>1e-05</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate the gradients before updating the weights</p> <code>1</code> <code>extra_optimizer_kwargs</code> <code>Optional[dict]</code> <p>dict | None: Pass extra arguments to the optimizer</p> <code>None</code> <code>weight_decay</code> <code>float</code> <p>float: Set the weight decay for adamw optimizer</p> <code>0.02</code> <code>warmup_steps</code> <code>int</code> <p>int: Specify the number of steps to warm up the learning rate</p> <code>0</code> <p>Returns:</p> Type Description <p>A tuple of two objects: (Optimizer and scheduler)</p> Source code in <code>src/python/easydel/etils/auto_tx.py</code> <pre><code>def get_optimizer_and_scheduler(\n        optimizer: AVAILABLE_OPTIMIZERS,\n        scheduler: AVAILABLE_SCHEDULERS,\n        steps: int,\n        learning_rate: float = 1e-5,\n        learning_rate_end: float = 1e-5,\n        gradient_accumulation_steps: int = 1,\n        extra_optimizer_kwargs: Optional[dict] = None,\n        weight_decay: float = 0.02,\n        warmup_steps: int = 0\n):\n    \"\"\"The get_optimizer_and_scheduler function is a helper function that returns an optimizer and scheduler\n        based on the parameters passed to it.\n\n    Args:\n        optimizer: AVAILABLE_OPTIMIZERS: Choose the optimizer\n        scheduler: AVAILABLE_SCHEDULERS: Determine the learning rate\n            scheduler\n        steps: int: Specify the number of steps in the training process\n        learning_rate: float: Set the learning rate for the optimizer\n        learning_rate_end: float: Set the final learning rate\n        gradient_accumulation_steps: int: Accumulate the gradients\n            before updating the weights\n        extra_optimizer_kwargs: dict | None: Pass extra arguments to the\n            optimizer\n        weight_decay: float: Set the weight decay for adamw optimizer\n        warmup_steps: int: Specify the number of steps to warm up the\n            learning rate\n\n    Returns:\n        A tuple of two objects: (Optimizer and scheduler)\n    \"\"\"\n    if extra_optimizer_kwargs is None:\n        extra_optimizer_kwargs = {}\n    if optimizer == EasyDeLOptimizers.ADAFACTOR:\n        if scheduler == EasyDeLSchedulers.LINEAR:\n            tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate_end,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                steps=steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.COSINE:\n            tx, sc = fjformer.optimizers.get_adafactor_with_cosine_scheduler(\n                learning_rate=learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.NONE:\n            tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.WARM_UP_COSINE:\n            tx, sc = fjformer.optimizers.get_adafactor_with_warm_up_cosine_scheduler(\n                learning_rate=learning_rate,\n                steps=steps,\n                weight_decay=weight_decay,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.WARM_UP_LINEAR:\n            tx, sc = fjformer.optimizers.get_adafactor_with_warmup_linear_scheduler(\n                learning_rate_start=learning_rate,\n                steps=steps,\n                learning_rate_end=learning_rate_end,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                warmup_steps=warmup_steps,\n                **extra_optimizer_kwargs\n\n            )\n\n        else:\n            raise ValueError(\n                \"seems like you have choose wrong type or unavailable scheduler\"\n            )\n    elif optimizer == EasyDeLOptimizers.LION:\n        if scheduler == EasyDeLSchedulers.LINEAR:\n            tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate_end,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.COSINE:\n            tx, sc = fjformer.optimizers.get_lion_with_cosine_scheduler(\n                learning_rate=learning_rate,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                steps=steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.NONE:\n            tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.WARM_UP_COSINE:\n            tx, sc = fjformer.optimizers.get_lion_with_warm_up_cosine_scheduler(\n                learning_rate=learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n\n        elif scheduler == EasyDeLSchedulers.WARM_UP_LINEAR:\n            tx, sc = fjformer.optimizers.get_lion_with_with_warmup_linear_scheduler(\n                learning_rate_start=learning_rate,\n                steps=steps,\n                learning_rate_end=learning_rate_end,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                warmup_steps=warmup_steps,\n                **extra_optimizer_kwargs\n            )\n        else:\n            raise ValueError(\n                \"seems like you have choose wrong type or unavailable scheduler\")\n    elif optimizer == EasyDeLOptimizers.ADAMW:\n        if scheduler == EasyDeLSchedulers.LINEAR:\n            tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate_end,\n                steps=steps,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.COSINE:\n            tx, sc = fjformer.optimizers.get_adamw_with_cosine_scheduler(\n                learning_rate=learning_rate,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                steps=steps,\n                weight_decay=weight_decay,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.NONE:\n            tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                learning_rate_start=learning_rate,\n                learning_rate_end=learning_rate,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                steps=steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.WARM_UP_COSINE:\n            tx, sc = fjformer.optimizers.get_adamw_with_warm_up_cosine_scheduler(\n                learning_rate=learning_rate,\n                steps=steps,\n                weight_decay=weight_decay,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                **extra_optimizer_kwargs\n            )\n        elif scheduler == EasyDeLSchedulers.WARM_UP_LINEAR:\n            tx, sc = fjformer.optimizers.get_adamw_with_warmup_linear_scheduler(\n                learning_rate_start=learning_rate,\n                steps=steps,\n                weight_decay=weight_decay,\n                learning_rate_end=learning_rate_end,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                warmup_steps=warmup_steps,\n                **extra_optimizer_kwargs\n            )\n        else:\n            raise ValueError(\n                \"seems like you have choose wrong type or unavailable scheduler\"\n            )\n    else:\n        raise ValueError(\n            f\"seems like you have choose wrong type or unavailable optimizer {optimizer} and scheduler {scheduler}\"\n        )\n    return tx, sc\n</code></pre>"},{"location":"generated-etils-configs/","title":"etils.configs","text":""},{"location":"generated-etils-configs/#src.python.easydel.etils.configs.get_config","title":"<code>get_config(model_type, struct)</code>","text":"<p>The get_config function takes in a model_type and struct, and returns the corresponding config.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>str: Determine which model to use</p> required <code>struct</code> <code>str</code> <p>str: Specify the structure of the model</p> required <p>Returns:</p> Type Description <p>A dictionary of hyperparameters</p> Source code in <code>src/python/easydel/etils/configs.py</code> <pre><code>def get_config(model_type: str, struct: str):\n    \"\"\"The get_config function takes in a model_type and struct, and returns the corresponding config.\n\n    Args:\n        model_type: str: Determine which model to use\n        struct: str: Specify the structure of the model\n\n    Returns:\n        A dictionary of hyperparameters\n    \"\"\"\n    if model_type == \"llama\":\n        return llama_configs[struct]\n    elif model_type == \"llama2\":\n        return llama_2_configs[struct]\n    elif model_type == \"opt\":\n        return opt_configs[struct]\n    elif model_type == \"gptj\":\n        return gptj_configs[struct]\n    elif model_type == \"falcon\":\n        return falcon_configs[struct]\n    elif model_type == \"mpt\":\n        return mpt_configs[struct]\n    else:\n        raise ValueError(f\"Unknown ModelType : {model_type}\")\n</code></pre>"},{"location":"generated-etils-easystate/","title":"etils.easystate","text":""},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState","title":"<code>EasyDeLState</code>","text":"<p>               Bases: <code>PyTreeNode</code></p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>class EasyDeLState(struct.PyTreeNode):\n    step: int\n    module: Optional[\"EasyDeLFlaxPretrainedModel\"] = struct.field(pytree_node=False)  # type:ignore\n    module_config: Optional[\"EasyDeLPretrainedConfig\"] = struct.field(pytree_node=False)  # type:ignore\n    module_config_args: Optional[dict] = struct.field(pytree_node=True)\n    apply_fn: Callable = struct.field(pytree_node=False)\n    params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n    tx: optax.GradientTransformation = struct.field(pytree_node=False)\n    opt_state: Optional[optax.OptState] = struct.field(pytree_node=True)\n    tx_init: Optional[dict] = struct.field(pytree_node=True)\n    hyperparameters: Optional[dict] = struct.field(pytree_node=True)\n\n    def apply_gradients(self, *, grads, **kwargs):\n\n        \"\"\"The apply_gradients function is the core of the optimizer. It takes in a dictionary of gradients,\n        and returns an updated version of itself with new parameters and state. The function also updates\n        the step count.\n\n        Args:\n            self: Refer to the current instance of the class\n            : Unpack the grads dictionary into positional arguments\n            grads: Pass in the gradients of the loss function with\n                respect to each parameter\n            **kwargs: Pass in additional arguments to the function\n\n        Returns:\n            A new State with the updated parameters and params\n        \"\"\"\n        if OVERWRITE_WITH_GRADIENT in grads:\n            grads_with_opt = grads['params']\n            params_with_opt = self.params['params']\n        else:\n            grads_with_opt = grads\n            params_with_opt = self.params\n\n        updates, new_opt_state = self.tx.update(\n            grads_with_opt, self.opt_state, params_with_opt\n        )\n        new_params_with_opt = optax.apply_updates(params_with_opt, updates)\n        if OVERWRITE_WITH_GRADIENT in grads:\n            new_params = {\n                'params': new_params_with_opt,\n                OVERWRITE_WITH_GRADIENT: grads[OVERWRITE_WITH_GRADIENT]\n            }\n        else:\n            new_params = new_params_with_opt\n        return self.replace(\n            step=self.step + 1,\n            params=new_params,\n            opt_state=new_opt_state,\n            **kwargs,\n        )\n\n    @classmethod\n    def create(\n            cls,\n            *,\n            apply_fn: Callable,\n            params: Union[core.FrozenDict[str, Any], Mapping[str, Any]],\n            tx: optax.GradientTransformation,\n            tx_init: Optional[dict] = None,\n            hyperparameters: Optional[dict] = None,\n            module: Optional[\"EasyDeLFlaxPretrainedModel\"] = None,  # type:ignore\n            module_config: Optional[\"EasyDeLPretrainedConfig\"] = None,  # type:ignore\n            module_config_args: Optional[dict] = None,\n            **kwargs\n    ):\n\n        \"\"\"The create function is used to create a new instance of the class.\n\n        Args:\n            cls: Create a new instance of the class\n            : Pass a list of parameters to the function\n            apply_fn: Callable: Apply the model to a batch of data\n            params: core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in\n                the parameters of the model\n            tx: optax.GradientTransformation: Initialize the optimizer\n            tx_init: Optional[dict]: Initialize the optimizer\n            hyperparameters: Optional[dict]: Pass hyperparameters to the\n                state for init\n            module: Optional[EasyDeLFlaxPretrainedModel]: Pass the\n                module to be used int state\n            module_config: Optional[EasyDeLPretrainedConfig]: Pass in\n                the module config\n            module_config_args: Optional[dict]: Store the config args of\n                the model\n            **kwargs: Pass in additional parameters to the\n\n        Returns:\n            A EasyDeLState object\n        \"\"\"\n        if hyperparameters is None:\n            hyperparameters = {}\n        params_with_opt = (\n            params['params'] if OVERWRITE_WITH_GRADIENT in params else params\n        )\n        opt_state = tx.init(params_with_opt)\n        if module_config is not None:\n            module_config = copy.deepcopy(module_config)\n            cls.safe_dict(module_config.__dict__)\n        return cls(\n            step=0,\n            apply_fn=apply_fn,\n            module=module,\n            params=params,\n            tx=tx,\n            opt_state=opt_state,\n            tx_init=cls.safe_dict(tx_init),\n            hyperparameters=hyperparameters,\n            module_config=module_config,\n            module_config_args=None,\n            **kwargs,\n        )\n\n    @classmethod\n    def load(\n            cls,\n            *,\n            apply_fn: Callable,\n            params: Union[core.FrozenDict[str, Any], Mapping[str, Any]],\n            step: int = 0,\n            opt_state: Optional[optax.OptState] = None,\n            tx_init: Optional[dict] = None,\n            hyperparameters: Optional[dict] = None,\n            module: Optional[\"EasyDeLFlaxPretrainedModel\"] = None,  # type:ignore\n            module_config: Optional[\"EasyDeLPretrainedConfig\"] = None,  # type:ignore\n            module_config_args: Optional[dict] = None,\n            **kwargs\n    ):\n\n        \"\"\"The load function is used to load a saved state of the Model and optimizer or Model Only.\n\n        Args:\n            cls: Make the function a class method\n            : Pass in a variable number of arguments\n            step: int: Keep track of the number of steps that have been\n                taken\n            apply_fn: Callable: Apply the optimizer to the model\n            params: core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in\n                the parameters of the model\n            opt_state: Optional[optax.OptState]: optimizer state\n            tx_init: Optional[dict]: Pass the hyperparameters to the\n                optimizer\n            hyperparameters: Optional[dict]: Load hyperparameters from\n                the state dict\n            module: Optional[EasyDeLFlaxPretrainedModel]: Pass in the\n                module\n            module_config: Optional[EasyDeLPretrainedConfig]: Pass the\n                module config\n            module_config_args: Optional[dict]: Pass the config_args to\n                the model\n            **kwargs: Pass in any additional parameters that may be\n                needed for the model\n\n        Returns:\n            A new instance of the class\n        \"\"\"\n        if module_config is not None:\n            module_config = copy.deepcopy(module_config)\n\n        if tx_init is None:\n            tx_init = {}\n        tx_init = copy.deepcopy(tx_init)\n        tx_init = cls.unsafe_dict(tx_init)\n\n        tx_init[\"optimizer\"] = cls.search(\"optimizer\", tx_init, \"adamw\")\n        tx_init[\"scheduler\"] = cls.search(\"scheduler\", tx_init, \"none\")\n        tx_init[\"steps\"] = cls.search(\"steps\", tx_init, 1e6)\n\n        def fix_dict_types(input_dict):\n            fixed_dict = input_dict.copy()\n\n            # Fix extra_optimizer_kwargs\n            if 'extra_optimizer_kwargs' in fixed_dict:\n                fixed_dict['extra_optimizer_kwargs'] = eval(fixed_dict['extra_optimizer_kwargs'])\n\n            # Fix gradient_accumulation_steps\n            if 'gradient_accumulation_steps' in fixed_dict:\n                fixed_dict['gradient_accumulation_steps'] = int(fixed_dict['gradient_accumulation_steps'])\n\n            # Fix steps\n            if 'steps' in fixed_dict:\n                fixed_dict['steps'] = int(fixed_dict['steps'])\n\n            # Fix warmup_steps\n            if 'warmup_steps' in fixed_dict:\n                fixed_dict['warmup_steps'] = int(fixed_dict['warmup_steps'])\n\n            return fixed_dict\n\n        try:\n            tx, sc = get_optimizer_and_scheduler(\n                **tx_init\n            )\n        except TypeError:\n            tx, sc = get_optimizer_and_scheduler(\n                **fix_dict_types(tx_init)\n            )\n        if hyperparameters is None:\n            hyperparameters = {}\n\n        if module_config is not None:\n            hyperparameters = cls.create_hyperparameters(module_config.model_type)\n            cls.safe_dict(module_config.__dict__)\n        return cls(\n            step=step,\n            apply_fn=apply_fn,\n            params=params,\n            tx=tx,\n            opt_state=opt_state,\n            tx_init=cls.safe_dict(tx_init),\n            hyperparameters=hyperparameters,\n            module=module,\n            module_config=module_config,\n            module_config_args=None,\n            **kwargs,\n        )\n\n    @classmethod\n    def load_state(\n            cls,\n            checkpoint_path: Union[str, os.PathLike],\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[Union[str, jax.lax.Precision]] = None,\n            init_optimizer_state: bool = False,\n            state_shard_fns: Optional[Mapping[str, Callable]] = None,\n            verbose: bool = False,\n            input_shape: Tuple = (1, 1),\n            config_kwargs: Optional[dict] = None,\n            sharding_axes_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            sharding_axes_dims: Sequence[int] = (1, -1, 1, 1)\n    ):\n\n        \"\"\"The load_state function is a class method that loads the state of an EasyDeLModel from a checkpoint.\n\n        Args:\n            cls: Create an instance of the class\n            checkpoint_path: str | os.PathLike: Specify the path to the\n                checkpoint file\n            dtype: jnp.dtype: The dtype of the model\n            param_dtype: jnp.dtype: The dtype of the model parameters\n            precision: Optional[Union[str, jax.lax.Precision]]:\n                precision of the model\n            init_optimizer_state: bool: Initialize the optimizer if it's\n                not Initialized yet (if it Initialized the option\n            state_shard_fns: Optional[Mapping[str,Callable]]: Specify\n                the function that will be used\n            verbose: bool: Print out the progress of loading\n            input_shape: Tuple: input_shape to init module\n            config_kwargs: Optional[dict] : config kwargs to be passed\n                to model config\n        will be ignored )\n        to shard the loaded state\n\n        Returns:\n            A state object\n        \"\"\"\n        from ..modules.auto_easydel_model import get_modules_by_type\n        from fjformer.partition_utils import create_mesh\n        mesh = create_mesh(sharding_axes_dims, sharding_axes_names)\n\n        with mesh:\n            checkpoint = fjformer.CheckpointManager.load_checkpoint(\n                path=checkpoint_path,\n                shard_fns=state_shard_fns,\n                verbose=verbose,\n            )\n            hyperparameters = checkpoint.get(\"hyperparameters\")\n            cfg, module, convertor = get_modules_by_type(model_type=cls.get_model_type(hyperparameters))\n            checkpoint.pop(\"module_config\", None)\n            if checkpoint[\"module_config_args\"] is not None:\n                cfg_behave = cls.unsafe_dict(checkpoint.get(\"module_config_args\", {}))\n                cfg_behave.pop(\"id2label\", None)\n                cfg_behave.pop(\"label2id\", None)\n                cfg_behave.pop(\"torch_dtype\", None)\n                for k, v in cfg_behave.items():\n                    if v is None:\n                        cfg_behave.pop(k, None)\n                    elif v == \"None\":\n                        cfg_behave[k] = None\n                    elif isinstance(v, str):\n                        if v.startswith(\"{\") or v.startswith(\"(\") or v.startswith(\"PartitionSpec\"):\n                            cfg_behave[k] = eval(v)\n                module_config = cfg.from_dict(cfg_behave)\n                if config_kwargs is not None:\n                    for k, v in config_kwargs.items():\n                        setattr(module_config, k, v)\n                module_in = module(\n                    config=module_config,\n                    dtype=dtype,\n                    param_dtype=param_dtype,\n                    precision=precision,\n                    input_shape=input_shape,\n                    _do_init=False\n                )\n            else:\n                raise TypeError(\n                    \"Om seems like i couldn't read model correctly ;(\"\n                )\n            state = cls.load(\n                apply_fn=module_in.__call__,\n                module=module_in,\n                module_config=module_config,\n                **checkpoint\n            )\n            state = state.replace(\n                module_config_args=None  # removing because it's not needed anymore\n            )\n            if init_optimizer_state:\n                state = state.init_opt_state()\n        return state\n\n    @classmethod\n    def get_model_type(cls, dictionary):\n        return cls.find_key(\"model_type\", dictionary)\n\n    def save_state(\n            self,\n            filename: Union[str, os.PathLike],\n            save_optimizer: bool = False,\n            checkpoint_dir: Optional[Union[str, os.PathLike]] = None,\n            verbose: bool = False,\n            gather_fns: dict[Callable] = None,\n            float_dtype: Union[str, jax.numpy.dtype] = None,\n    ):\n\n        \"\"\"The save_state function saves the state of a model to disk.\n\n        Args:\n            self: Pass the object itself to the function\n            filename: str | os.PathLike: Specify the name of the file to\n                save\n            save_optimizer: bool: Determine whether to save the\n                optimizer state or not\n            checkpoint_dir: Optional[str | os.PathLike]: Specify the\n                directory where the checkpoint is saved\n            verbose: bool: Print out the path of the saved file\n            gather_fns: dict[Callable]: Specify a dictionary of\n                functions that can be used to gather\n            float_dtype: str | jax.numpy.dtype: Specify the precision of\n                the saved model\n        :param : Save the optimizer state\n\n        Returns:\n            None\n        \"\"\"\n        state = self\n        if not save_optimizer:\n            state = self.replace(\n                opt_state=None\n            )\n        state = state.replace(\n            module_config_args={\n                k: v for k, v in state.module.config.__dict__.items() if\n                isinstance(\n                    v, (int, bool, float)\n                )\n            }\n        )\n        fjformer.CheckpointManager.save_state_to_file(\n            state=state,\n            path=os.path.join(checkpoint_dir, filename) if checkpoint_dir is not None else filename,\n            verbose=verbose,\n            gather_fns=gather_fns,\n            float_dtype=float_dtype,\n        )\n\n    def free_opt_state(self) -&gt; \"EasyDeLState\":\n\n        \"\"\"The free_opt_state function is used to free the memory allocated by a previous call to setopt.\n        It should be called after all the options have been set, and before you perform any of the transfers.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A new state with the opt_state field set to none\n        \"\"\"\n        return self.replace(\n            opt_state=None\n        )\n\n    def init_opt_state(self) -&gt; \"EasyDeLState\":\n\n        \"\"\"The init_opt_state function initializes the optimizer state.\n\n        Args:\n            self: Make the object callable, and params is used to pass\n                in a dictionary of parameters\n\n        Returns:\n            A new instance of the class with opt_state initialized\n        \"\"\"\n        if self.opt_state is None:\n            params_with_opt = (\n                self.params['params'] if OVERWRITE_WITH_GRADIENT in self.params else self.params\n            )\n            opt_state = self.tx.init(params_with_opt)\n\n            return self.replace(\n                opt_state=opt_state\n            )\n        return self\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            pretrained_model_name_or_path: str,\n            filename: Optional[str] = None,\n            optimizer: AVAILABLE_OPTIMIZERS = \"adamw\",\n            scheduler: AVAILABLE_SCHEDULERS = \"none\",\n            tx_init: Optional[dict] = None,\n            device=jax.devices('cpu')[0],\n            dtype: jax.numpy.dtype = jax.numpy.float32,\n            param_dtype: jax.numpy.dtype = jax.numpy.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"tp\", None, None),\n            key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            shard_attention_computation: bool = True,\n            input_shape: Sequence[int] = (1, 1),\n            backend: Optional[str] = None,\n            init_optimizer_state: bool = False,\n            free_optimizer_state: bool = True,\n            verbose: bool = True,\n            state_shard_fns: Optional[Mapping[str, Callable]] = None,\n            config_kwargs: Optional[Mapping[str, Any]] = None,\n            **kwargs\n    ) -&gt; \"EasyDeLState\":\n\n        \"\"\"The from_pretrained function is a helper function to quickly load a pretrained model and its associated configuration.\n        This method takes care of returning the correct model class instance based on the `model_type` property in the\n        config object, or when it's missing, falling back to using pattern matching on the\n         `pretrained_model_name_or_path` string:\n\n        Args:\n            cls: Refer to the class that is being defined\n            pretrained_model_name_or_path: str: Load the pretrained\n                model\n            filename: Optional[str]: Specify the name of the file to\n                download from huggingface hub\n            optimizer: AVAILABLE_OPTIMIZERS: Specify the optimizer used\n                for training\n            scheduler: AVAILABLE_SCHEDULERS: Specify the name of the\n                scheduler to use\n            tx_init: Optional[dict]: Pass the hyperparameters of the\n                optimizer\n            device: Specify the device on which to run the model\n            dtype: jax.numpy.dtype: Specify the dtype of the model\n                parameters\n            param_dtype: jax.numpy.dtype: Specify the data type of the\n                parameters\n            precision: jax.lax.Precision: Control the precision of the\n                calculation\n            sharding_axis_dims: Sequence[int]: Specify the dimension of\n                each axis\n            sharding_axis_names: Sequence[str]: Specify the names of the\n                axes in each shard\n            query_partition_spec: PartitionSpec: Specify the\n                partitioning of the query matrix\n            generation_query_partition_spec: PartitionSpec: Specify the\n                partitioning of the query tensor in\n            value_partition_spec: PartitionSpec: Specify the\n                partitioning of the value tensor\n            bias_partition_spec: PartitionSpec: Specify the partitioning\n                of the bias\n            attention_partition_spec: PartitionSpec: Partition the\n                attention weights\n            shard_attention_computation: bool: Determine whether to use\n                shard_map or not\n            input_shape: Sequence[int]: Specify the shape of the input\n                to be used for training\n            backend: Optional[str]: Specify the backend used for the\n                model\n            init_optimizer_state: bool: Initialize the optimizer state\n            free_optimizer_state: bool: Free the optimizer state from\n                memory\n            verbose: bool: Print the progress of loading the model\n            state_shard_fns: Optional[Mapping[str,Callable]]: Specify\n                the function to use for sharding the state\n            **kwargs: Pass keyword arguments to the function\n            config_kwargs: Optional[Mapping[str, Any]]: Config kwargs to\n                be added to config before creating module\n        generation process:param key_partition_spec: PartitionSpec: Specify the partitioning of the key matrix\n\n        Returns:\n            An `EasyDeLState` object\n        \"\"\"\n        if free_optimizer_state and init_optimizer_state:\n            raise EasyDeLRuntimeError(\n                \"You can't use `free_optimizer_state` and `init_optimizer_state` True at same Time\"\n            )\n\n        if filename is None:\n            from ..modules.auto_easydel_model import AutoEasyDeLModelForCausalLM\n\n            model, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n                pretrained_model_name_or_path,\n                device=device,\n                dtype=dtype,\n                param_dtype=param_dtype,\n                precision=precision,\n                sharding_axis_dims=sharding_axis_dims,\n                sharding_axis_names=sharding_axis_names,\n                query_partition_spec=query_partition_spec,\n                generation_query_partition_spec=generation_query_partition_spec,\n                generation_bias_partition_spec=generation_bias_partition_spec,\n                key_partition_spec=key_partition_spec,\n                value_partition_spec=value_partition_spec,\n                bias_partition_spec=bias_partition_spec,\n                attention_partition_spec=attention_partition_spec,\n                shard_attention_computation=shard_attention_computation,\n                input_shape=input_shape,\n                backend=backend,\n                config_kwargs=config_kwargs,\n                **kwargs\n            )\n            if tx_init is None:\n                tx_init = {}\n\n            tx_init[\"optimizer\"] = optimizer\n            tx_init[\"scheduler\"] = scheduler\n\n            state = cls.load(\n                apply_fn=model.__call__,\n                params=FrozenDict({'params': params}),\n                step=0,\n                opt_state=None,\n                tx_init=tx_init,\n                hyperparameters=None,\n                module=model,\n                module_config=model.config,\n                module_config_args=model.config.to_dict()\n            )\n        else:\n            with jax.default_device(device):\n                from huggingface_hub import hf_hub_download\n                checkpoint_path = hf_hub_download(\n                    repo_id=pretrained_model_name_or_path,\n                    filename=filename,\n                )\n                state = cls.load_state(\n                    checkpoint_path=checkpoint_path,\n                    init_optimizer_state=init_optimizer_state,\n                    verbose=verbose,\n                    state_shard_fns=state_shard_fns,\n                    dtype=dtype,\n                    param_dtype=param_dtype,\n                    precision=precision,\n                    input_shape=input_shape\n                )\n        if init_optimizer_state:\n            with jax.default_device(device):\n                state = state.init_opt_state()\n        if free_optimizer_state:\n            state = state.free_opt_state()\n        return state\n\n    def shard_params(\n            self,\n            fully_sharded_data_parallel: bool = True,\n            shard_fns: Optional[Mapping[str, Callable]] = None,\n            dtype: Union[jax.numpy.dtype, str] = \"bf16\",\n            mesh: Optional[Mesh] = None,\n            rules: Optional[Sequence[Mapping[str, PartitionSpec]]] = None\n    ):\n        dtype = fjformer.get_dtype(dtype)\n        if shard_fns is None and self.module_config is None and rules is None:\n            raise EasyDeLRuntimeError(\n                \"the model doesn't carrying `module_config` you should pass `shard_fns` or `rules`\"\n            )\n        elif shard_fns is None and rules is not None or self.module_config is not None:\n            from fjformer import match_partition_rules, make_shard_and_gather_fns\n            rules = rules or self.module_config.get_partition_rules(fully_sharded_data_parallel)\n            partition_specs = match_partition_rules(\n                rules=rules, params=self.params\n            )\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                partition_specs=partition_specs,\n                dtype_specs=dtype\n            )\n        if mesh is None:\n            mesh = self.module_config.jax_mesh()\n        with mesh:\n            return self.replace(\n                params=jax.tree_util.tree_map(\n                    lambda f, p: f(p), shard_fns, self.params\n                )\n            )\n\n    @staticmethod\n    def create_hyperparameters(model_type: str):\n        \"\"\"it's the only way we can dump xla compiler\"\"\"\n        return {\n            STRING_REP.format(\n                type=\"str\",\n                key=\"model_type\",\n                value=model_type\n            ): DEFAULT_ES_VAL\n        }\n\n    @staticmethod\n    def safe_dict(dictionary: dict):\n        for k in list(dictionary.keys()):\n            val = dictionary.get(k)\n            if not isinstance(val, (int, bool)):\n                val = dictionary.pop(k)\n                string_value_format = STRING_REP.format(\n                    type=type(val).__name__,\n                    key=k,\n                    value=val\n                )\n                dictionary[string_value_format] = DEFAULT_ES_VAL\n        return dictionary\n\n    @staticmethod\n    def unsafe_dict(dictionary: dict):\n        result = {}\n        for k in list(dictionary.keys()):\n            if VALUE_SEP in k and TYPE_SEP in k:\n                v = dictionary[k]\n                key, value = break_format(key=k, value=v)\n                result[key] = value\n            else:\n                result[k] = dictionary[k]\n        return result\n\n    def __str__(self):\n\n        \"\"\"The __str__ function is called when you call str(object) or print(object).\n        The __repr__ function is called when you type the object name in the interpreter.\n        If no __str__ method exists, Python will use __repr__ as a fallback.\n\n        Args:\n            self: Refer to the object itself\n\n        Returns:\n            string\n        \"\"\"\n        params_size = sum(getattr(n, \"size\", 0) for n in jax.tree_util.tree_flatten(self.params)[0])\n        opt_state_size = sum(getattr(n, \"size\", 0) for n in jax.tree_util.tree_flatten(self.opt_state)[0])\n\n        def make_depth(mdl=None):\n            if mdl is not None:\n                try:\n                    return mdl.__str__().replace(\n                        \"\\n\",\n                        \"\\n\\t\"\n                        \"\"\n                    ) if hasattr(mdl, \"__str__\") else None\n                except TypeError:\n                    ...\n            return mdl\n\n        optimizer = self.tx_init.get(\"optimizer\", None)\n        scheduler = self.tx_init.get(\"scheduler\", None)\n\n        if optimizer is None:\n            optimizer = self.find_key(\n                \"optimizer\",\n                self.tx_init\n            )\n        if scheduler is None:\n            scheduler = self.find_key(\n                \"scheduler\",\n                self.tx_init\n            )\n\n        string = (\n            f\"{self.__class__.__name__}(\"\n            f\"\\n\\tstep = {self.step}\"\n            f\"\\n\\tmodule = {make_depth(self.module)}\"\n            f\"\\n\\tmodule_config = {make_depth(self.module_config)}\"\n            f\"\\n\\tapply_fn: Callable = {make_depth(self.apply_fn)}\"\n            f\"\\n\\tparams : {params_size} Parameters\"\n            f\"\\n\\ttx = {optimizer} Optimizer with {scheduler} Scheduler\"\n            f\"\\n\\topt_state : {opt_state_size} Parameters\"\n            f\"\\n\\thyperparameters : {self.hyperparameters}\"\n            f\"\\n)\"\n        )\n        return string\n\n    @classmethod\n    def search(cls, key, dictionary: dict, default: Any = None):\n        req = dictionary.get(key, None)\n        if req is None:\n            req = cls.find_key(key, dictionary)\n        return req or default\n\n    @staticmethod\n    def find_key(key, dictionary: dict) -&gt; Union[str, None]:\n        result = None\n        for k, v in dictionary.items():\n            k_, v_ = break_format(key=k, value=v)\n            if k_ == key:\n                result = v_\n                break\n        return result\n\n    def __repr__(self):\n\n        \"\"\"The __repr__ function is the &amp;quot;official&amp;quot; string representation of an object.\n        It's what you get when you type the object name at the Python prompt, or pass it to str().\n        The goal of __repr__ is to be unambiguous: if eval(repr(x)) == x, then __repr__ should return a string that\n        looks like a valid Python expression that could be used to recreate an object with the same value (\n        given an appropriate environment). If this is not possible, a string formatted using %s\n        formatting is also acceptable.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A string that is a valid python expression\n        \"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is the \"official\" string representation of an object. It's what you get when you type the object name at the Python prompt, or pass it to str(). The goal of repr is to be unambiguous: if eval(repr(x)) == x, then repr should return a string that looks like a valid Python expression that could be used to recreate an object with the same value ( given an appropriate environment). If this is not possible, a string formatted using %s formatting is also acceptable.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A string that is a valid python expression</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"The __repr__ function is the &amp;quot;official&amp;quot; string representation of an object.\n    It's what you get when you type the object name at the Python prompt, or pass it to str().\n    The goal of __repr__ is to be unambiguous: if eval(repr(x)) == x, then __repr__ should return a string that\n    looks like a valid Python expression that could be used to recreate an object with the same value (\n    given an appropriate environment). If this is not possible, a string formatted using %s\n    formatting is also acceptable.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A string that is a valid python expression\n    \"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you call str(object) or print(object). The repr function is called when you type the object name in the interpreter. If no str method exists, Python will use repr as a fallback.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <p>string</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def __str__(self):\n\n    \"\"\"The __str__ function is called when you call str(object) or print(object).\n    The __repr__ function is called when you type the object name in the interpreter.\n    If no __str__ method exists, Python will use __repr__ as a fallback.\n\n    Args:\n        self: Refer to the object itself\n\n    Returns:\n        string\n    \"\"\"\n    params_size = sum(getattr(n, \"size\", 0) for n in jax.tree_util.tree_flatten(self.params)[0])\n    opt_state_size = sum(getattr(n, \"size\", 0) for n in jax.tree_util.tree_flatten(self.opt_state)[0])\n\n    def make_depth(mdl=None):\n        if mdl is not None:\n            try:\n                return mdl.__str__().replace(\n                    \"\\n\",\n                    \"\\n\\t\"\n                    \"\"\n                ) if hasattr(mdl, \"__str__\") else None\n            except TypeError:\n                ...\n        return mdl\n\n    optimizer = self.tx_init.get(\"optimizer\", None)\n    scheduler = self.tx_init.get(\"scheduler\", None)\n\n    if optimizer is None:\n        optimizer = self.find_key(\n            \"optimizer\",\n            self.tx_init\n        )\n    if scheduler is None:\n        scheduler = self.find_key(\n            \"scheduler\",\n            self.tx_init\n        )\n\n    string = (\n        f\"{self.__class__.__name__}(\"\n        f\"\\n\\tstep = {self.step}\"\n        f\"\\n\\tmodule = {make_depth(self.module)}\"\n        f\"\\n\\tmodule_config = {make_depth(self.module_config)}\"\n        f\"\\n\\tapply_fn: Callable = {make_depth(self.apply_fn)}\"\n        f\"\\n\\tparams : {params_size} Parameters\"\n        f\"\\n\\ttx = {optimizer} Optimizer with {scheduler} Scheduler\"\n        f\"\\n\\topt_state : {opt_state_size} Parameters\"\n        f\"\\n\\thyperparameters : {self.hyperparameters}\"\n        f\"\\n)\"\n    )\n    return string\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.apply_gradients","title":"<code>apply_gradients(*, grads, **kwargs)</code>","text":"<p>The apply_gradients function is the core of the optimizer. It takes in a dictionary of gradients, and returns an updated version of itself with new parameters and state. The function also updates the step count.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current instance of the class</p> required <code></code> <p>Unpack the grads dictionary into positional arguments</p> required <code>grads</code> <p>Pass in the gradients of the loss function with respect to each parameter</p> required <code>**kwargs</code> <p>Pass in additional arguments to the function</p> <code>{}</code> <p>Returns:</p> Type Description <p>A new State with the updated parameters and params</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def apply_gradients(self, *, grads, **kwargs):\n\n    \"\"\"The apply_gradients function is the core of the optimizer. It takes in a dictionary of gradients,\n    and returns an updated version of itself with new parameters and state. The function also updates\n    the step count.\n\n    Args:\n        self: Refer to the current instance of the class\n        : Unpack the grads dictionary into positional arguments\n        grads: Pass in the gradients of the loss function with\n            respect to each parameter\n        **kwargs: Pass in additional arguments to the function\n\n    Returns:\n        A new State with the updated parameters and params\n    \"\"\"\n    if OVERWRITE_WITH_GRADIENT in grads:\n        grads_with_opt = grads['params']\n        params_with_opt = self.params['params']\n    else:\n        grads_with_opt = grads\n        params_with_opt = self.params\n\n    updates, new_opt_state = self.tx.update(\n        grads_with_opt, self.opt_state, params_with_opt\n    )\n    new_params_with_opt = optax.apply_updates(params_with_opt, updates)\n    if OVERWRITE_WITH_GRADIENT in grads:\n        new_params = {\n            'params': new_params_with_opt,\n            OVERWRITE_WITH_GRADIENT: grads[OVERWRITE_WITH_GRADIENT]\n        }\n    else:\n        new_params = new_params_with_opt\n    return self.replace(\n        step=self.step + 1,\n        params=new_params,\n        opt_state=new_opt_state,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.create","title":"<code>create(*, apply_fn, params, tx, tx_init=None, hyperparameters=None, module=None, module_config=None, module_config_args=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>The create function is used to create a new instance of the class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create a new instance of the class</p> required <code></code> <p>Pass a list of parameters to the function</p> required <code>apply_fn</code> <code>Callable</code> <p>Callable: Apply the model to a batch of data</p> required <code>params</code> <code>Union[FrozenDict[str, Any], Mapping[str, Any]]</code> <p>core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in the parameters of the model</p> required <code>tx</code> <code>GradientTransformation</code> <p>optax.GradientTransformation: Initialize the optimizer</p> required <code>tx_init</code> <code>Optional[dict]</code> <p>Optional[dict]: Initialize the optimizer</p> <code>None</code> <code>hyperparameters</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass hyperparameters to the state for init</p> <code>None</code> <code>module</code> <code>Optional[EasyDeLFlaxPretrainedModel]</code> <p>Optional[EasyDeLFlaxPretrainedModel]: Pass the module to be used int state</p> <code>None</code> <code>module_config</code> <code>Optional[EasyDeLPretrainedConfig]</code> <p>Optional[EasyDeLPretrainedConfig]: Pass in the module config</p> <code>None</code> <code>module_config_args</code> <code>Optional[dict]</code> <p>Optional[dict]: Store the config args of the model</p> <code>None</code> <code>**kwargs</code> <p>Pass in additional parameters to the</p> <code>{}</code> <p>Returns:</p> Type Description <p>A EasyDeLState object</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>@classmethod\ndef create(\n        cls,\n        *,\n        apply_fn: Callable,\n        params: Union[core.FrozenDict[str, Any], Mapping[str, Any]],\n        tx: optax.GradientTransformation,\n        tx_init: Optional[dict] = None,\n        hyperparameters: Optional[dict] = None,\n        module: Optional[\"EasyDeLFlaxPretrainedModel\"] = None,  # type:ignore\n        module_config: Optional[\"EasyDeLPretrainedConfig\"] = None,  # type:ignore\n        module_config_args: Optional[dict] = None,\n        **kwargs\n):\n\n    \"\"\"The create function is used to create a new instance of the class.\n\n    Args:\n        cls: Create a new instance of the class\n        : Pass a list of parameters to the function\n        apply_fn: Callable: Apply the model to a batch of data\n        params: core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in\n            the parameters of the model\n        tx: optax.GradientTransformation: Initialize the optimizer\n        tx_init: Optional[dict]: Initialize the optimizer\n        hyperparameters: Optional[dict]: Pass hyperparameters to the\n            state for init\n        module: Optional[EasyDeLFlaxPretrainedModel]: Pass the\n            module to be used int state\n        module_config: Optional[EasyDeLPretrainedConfig]: Pass in\n            the module config\n        module_config_args: Optional[dict]: Store the config args of\n            the model\n        **kwargs: Pass in additional parameters to the\n\n    Returns:\n        A EasyDeLState object\n    \"\"\"\n    if hyperparameters is None:\n        hyperparameters = {}\n    params_with_opt = (\n        params['params'] if OVERWRITE_WITH_GRADIENT in params else params\n    )\n    opt_state = tx.init(params_with_opt)\n    if module_config is not None:\n        module_config = copy.deepcopy(module_config)\n        cls.safe_dict(module_config.__dict__)\n    return cls(\n        step=0,\n        apply_fn=apply_fn,\n        module=module,\n        params=params,\n        tx=tx,\n        opt_state=opt_state,\n        tx_init=cls.safe_dict(tx_init),\n        hyperparameters=hyperparameters,\n        module_config=module_config,\n        module_config_args=None,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.create_hyperparameters","title":"<code>create_hyperparameters(model_type)</code>  <code>staticmethod</code>","text":"<p>it's the only way we can dump xla compiler</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>@staticmethod\ndef create_hyperparameters(model_type: str):\n    \"\"\"it's the only way we can dump xla compiler\"\"\"\n    return {\n        STRING_REP.format(\n            type=\"str\",\n            key=\"model_type\",\n            value=model_type\n        ): DEFAULT_ES_VAL\n    }\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.free_opt_state","title":"<code>free_opt_state()</code>","text":"<p>The free_opt_state function is used to free the memory allocated by a previous call to setopt. It should be called after all the options have been set, and before you perform any of the transfers.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>EasyDeLState</code> <p>A new state with the opt_state field set to none</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def free_opt_state(self) -&gt; \"EasyDeLState\":\n\n    \"\"\"The free_opt_state function is used to free the memory allocated by a previous call to setopt.\n    It should be called after all the options have been set, and before you perform any of the transfers.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A new state with the opt_state field set to none\n    \"\"\"\n    return self.replace(\n        opt_state=None\n    )\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, filename=None, optimizer='adamw', scheduler='none', tx_init=None, device=jax.devices('cpu')[0], dtype=jax.numpy.float32, param_dtype=jax.numpy.float32, precision=jax.lax.Precision('fastest'), sharding_axis_dims=(1, -1, 1, 1), sharding_axis_names=('dp', 'fsdp', 'tp', 'sp'), query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), generation_query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'tp', None, None), key_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), value_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), generation_bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), attention_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), shard_attention_computation=True, input_shape=(1, 1), backend=None, init_optimizer_state=False, free_optimizer_state=True, verbose=True, state_shard_fns=None, config_kwargs=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>The from_pretrained function is a helper function to quickly load a pretrained model and its associated configuration. This method takes care of returning the correct model class instance based on the <code>model_type</code> property in the config object, or when it's missing, falling back to using pattern matching on the  <code>pretrained_model_name_or_path</code> string:</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Refer to the class that is being defined</p> required <code>pretrained_model_name_or_path</code> <code>str</code> <p>str: Load the pretrained model</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional[str]: Specify the name of the file to download from huggingface hub</p> <code>None</code> <code>optimizer</code> <code>AVAILABLE_OPTIMIZERS</code> <p>AVAILABLE_OPTIMIZERS: Specify the optimizer used for training</p> <code>'adamw'</code> <code>scheduler</code> <code>AVAILABLE_SCHEDULERS</code> <p>AVAILABLE_SCHEDULERS: Specify the name of the scheduler to use</p> <code>'none'</code> <code>tx_init</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass the hyperparameters of the optimizer</p> <code>None</code> <code>device</code> <p>Specify the device on which to run the model</p> <code>devices('cpu')[0]</code> <code>dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the dtype of the model parameters</p> <code>float32</code> <code>param_dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the data type of the parameters</p> <code>float32</code> <code>precision</code> <code>Optional[Precision]</code> <p>jax.lax.Precision: Control the precision of the calculation</p> <code>Precision('fastest')</code> <code>sharding_axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimension of each axis</p> <code>(1, -1, 1, 1)</code> <code>sharding_axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Specify the names of the axes in each shard</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>generation_query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query tensor in</p> <code>PartitionSpec(('dp', 'fsdp'), 'tp', None, None)</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the bias</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Partition the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>shard_attention_computation</code> <code>bool</code> <p>bool: Determine whether to use shard_map or not</p> <code>True</code> <code>input_shape</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the shape of the input to be used for training</p> <code>(1, 1)</code> <code>backend</code> <code>Optional[str]</code> <p>Optional[str]: Specify the backend used for the model</p> <code>None</code> <code>init_optimizer_state</code> <code>bool</code> <p>bool: Initialize the optimizer state</p> <code>False</code> <code>free_optimizer_state</code> <code>bool</code> <p>bool: Free the optimizer state from memory</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>bool: Print the progress of loading the model</p> <code>True</code> <code>state_shard_fns</code> <code>Optional[Mapping[str, Callable]]</code> <p>Optional[Mapping[str,Callable]]: Specify the function to use for sharding the state</p> <code>None</code> <code>**kwargs</code> <p>Pass keyword arguments to the function</p> <code>{}</code> <code>config_kwargs</code> <code>Optional[Mapping[str, Any]]</code> <p>Optional[Mapping[str, Any]]: Config kwargs to be added to config before creating module</p> <code>None</code> <p>generation process:param key_partition_spec: PartitionSpec: Specify the partitioning of the key matrix</p> <p>Returns:</p> Type Description <code>EasyDeLState</code> <p>An <code>EasyDeLState</code> object</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>@classmethod\ndef from_pretrained(\n        cls,\n        pretrained_model_name_or_path: str,\n        filename: Optional[str] = None,\n        optimizer: AVAILABLE_OPTIMIZERS = \"adamw\",\n        scheduler: AVAILABLE_SCHEDULERS = \"none\",\n        tx_init: Optional[dict] = None,\n        device=jax.devices('cpu')[0],\n        dtype: jax.numpy.dtype = jax.numpy.float32,\n        param_dtype: jax.numpy.dtype = jax.numpy.float32,\n        precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n        sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n        sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"tp\", None, None),\n        key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        shard_attention_computation: bool = True,\n        input_shape: Sequence[int] = (1, 1),\n        backend: Optional[str] = None,\n        init_optimizer_state: bool = False,\n        free_optimizer_state: bool = True,\n        verbose: bool = True,\n        state_shard_fns: Optional[Mapping[str, Callable]] = None,\n        config_kwargs: Optional[Mapping[str, Any]] = None,\n        **kwargs\n) -&gt; \"EasyDeLState\":\n\n    \"\"\"The from_pretrained function is a helper function to quickly load a pretrained model and its associated configuration.\n    This method takes care of returning the correct model class instance based on the `model_type` property in the\n    config object, or when it's missing, falling back to using pattern matching on the\n     `pretrained_model_name_or_path` string:\n\n    Args:\n        cls: Refer to the class that is being defined\n        pretrained_model_name_or_path: str: Load the pretrained\n            model\n        filename: Optional[str]: Specify the name of the file to\n            download from huggingface hub\n        optimizer: AVAILABLE_OPTIMIZERS: Specify the optimizer used\n            for training\n        scheduler: AVAILABLE_SCHEDULERS: Specify the name of the\n            scheduler to use\n        tx_init: Optional[dict]: Pass the hyperparameters of the\n            optimizer\n        device: Specify the device on which to run the model\n        dtype: jax.numpy.dtype: Specify the dtype of the model\n            parameters\n        param_dtype: jax.numpy.dtype: Specify the data type of the\n            parameters\n        precision: jax.lax.Precision: Control the precision of the\n            calculation\n        sharding_axis_dims: Sequence[int]: Specify the dimension of\n            each axis\n        sharding_axis_names: Sequence[str]: Specify the names of the\n            axes in each shard\n        query_partition_spec: PartitionSpec: Specify the\n            partitioning of the query matrix\n        generation_query_partition_spec: PartitionSpec: Specify the\n            partitioning of the query tensor in\n        value_partition_spec: PartitionSpec: Specify the\n            partitioning of the value tensor\n        bias_partition_spec: PartitionSpec: Specify the partitioning\n            of the bias\n        attention_partition_spec: PartitionSpec: Partition the\n            attention weights\n        shard_attention_computation: bool: Determine whether to use\n            shard_map or not\n        input_shape: Sequence[int]: Specify the shape of the input\n            to be used for training\n        backend: Optional[str]: Specify the backend used for the\n            model\n        init_optimizer_state: bool: Initialize the optimizer state\n        free_optimizer_state: bool: Free the optimizer state from\n            memory\n        verbose: bool: Print the progress of loading the model\n        state_shard_fns: Optional[Mapping[str,Callable]]: Specify\n            the function to use for sharding the state\n        **kwargs: Pass keyword arguments to the function\n        config_kwargs: Optional[Mapping[str, Any]]: Config kwargs to\n            be added to config before creating module\n    generation process:param key_partition_spec: PartitionSpec: Specify the partitioning of the key matrix\n\n    Returns:\n        An `EasyDeLState` object\n    \"\"\"\n    if free_optimizer_state and init_optimizer_state:\n        raise EasyDeLRuntimeError(\n            \"You can't use `free_optimizer_state` and `init_optimizer_state` True at same Time\"\n        )\n\n    if filename is None:\n        from ..modules.auto_easydel_model import AutoEasyDeLModelForCausalLM\n\n        model, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            device=device,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            sharding_axis_dims=sharding_axis_dims,\n            sharding_axis_names=sharding_axis_names,\n            query_partition_spec=query_partition_spec,\n            generation_query_partition_spec=generation_query_partition_spec,\n            generation_bias_partition_spec=generation_bias_partition_spec,\n            key_partition_spec=key_partition_spec,\n            value_partition_spec=value_partition_spec,\n            bias_partition_spec=bias_partition_spec,\n            attention_partition_spec=attention_partition_spec,\n            shard_attention_computation=shard_attention_computation,\n            input_shape=input_shape,\n            backend=backend,\n            config_kwargs=config_kwargs,\n            **kwargs\n        )\n        if tx_init is None:\n            tx_init = {}\n\n        tx_init[\"optimizer\"] = optimizer\n        tx_init[\"scheduler\"] = scheduler\n\n        state = cls.load(\n            apply_fn=model.__call__,\n            params=FrozenDict({'params': params}),\n            step=0,\n            opt_state=None,\n            tx_init=tx_init,\n            hyperparameters=None,\n            module=model,\n            module_config=model.config,\n            module_config_args=model.config.to_dict()\n        )\n    else:\n        with jax.default_device(device):\n            from huggingface_hub import hf_hub_download\n            checkpoint_path = hf_hub_download(\n                repo_id=pretrained_model_name_or_path,\n                filename=filename,\n            )\n            state = cls.load_state(\n                checkpoint_path=checkpoint_path,\n                init_optimizer_state=init_optimizer_state,\n                verbose=verbose,\n                state_shard_fns=state_shard_fns,\n                dtype=dtype,\n                param_dtype=param_dtype,\n                precision=precision,\n                input_shape=input_shape\n            )\n    if init_optimizer_state:\n        with jax.default_device(device):\n            state = state.init_opt_state()\n    if free_optimizer_state:\n        state = state.free_opt_state()\n    return state\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.init_opt_state","title":"<code>init_opt_state()</code>","text":"<p>The init_opt_state function initializes the optimizer state.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the object callable, and params is used to pass in a dictionary of parameters</p> required <p>Returns:</p> Type Description <code>EasyDeLState</code> <p>A new instance of the class with opt_state initialized</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def init_opt_state(self) -&gt; \"EasyDeLState\":\n\n    \"\"\"The init_opt_state function initializes the optimizer state.\n\n    Args:\n        self: Make the object callable, and params is used to pass\n            in a dictionary of parameters\n\n    Returns:\n        A new instance of the class with opt_state initialized\n    \"\"\"\n    if self.opt_state is None:\n        params_with_opt = (\n            self.params['params'] if OVERWRITE_WITH_GRADIENT in self.params else self.params\n        )\n        opt_state = self.tx.init(params_with_opt)\n\n        return self.replace(\n            opt_state=opt_state\n        )\n    return self\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.load","title":"<code>load(*, apply_fn, params, step=0, opt_state=None, tx_init=None, hyperparameters=None, module=None, module_config=None, module_config_args=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>The load function is used to load a saved state of the Model and optimizer or Model Only.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Make the function a class method</p> required <code></code> <p>Pass in a variable number of arguments</p> required <code>step</code> <code>int</code> <p>int: Keep track of the number of steps that have been taken</p> <code>0</code> <code>apply_fn</code> <code>Callable</code> <p>Callable: Apply the optimizer to the model</p> required <code>params</code> <code>Union[FrozenDict[str, Any], Mapping[str, Any]]</code> <p>core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in the parameters of the model</p> required <code>opt_state</code> <code>Optional[OptState]</code> <p>Optional[optax.OptState]: optimizer state</p> <code>None</code> <code>tx_init</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass the hyperparameters to the optimizer</p> <code>None</code> <code>hyperparameters</code> <code>Optional[dict]</code> <p>Optional[dict]: Load hyperparameters from the state dict</p> <code>None</code> <code>module</code> <code>Optional[EasyDeLFlaxPretrainedModel]</code> <p>Optional[EasyDeLFlaxPretrainedModel]: Pass in the module</p> <code>None</code> <code>module_config</code> <code>Optional[EasyDeLPretrainedConfig]</code> <p>Optional[EasyDeLPretrainedConfig]: Pass the module config</p> <code>None</code> <code>module_config_args</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass the config_args to the model</p> <code>None</code> <code>**kwargs</code> <p>Pass in any additional parameters that may be needed for the model</p> <code>{}</code> <p>Returns:</p> Type Description <p>A new instance of the class</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>@classmethod\ndef load(\n        cls,\n        *,\n        apply_fn: Callable,\n        params: Union[core.FrozenDict[str, Any], Mapping[str, Any]],\n        step: int = 0,\n        opt_state: Optional[optax.OptState] = None,\n        tx_init: Optional[dict] = None,\n        hyperparameters: Optional[dict] = None,\n        module: Optional[\"EasyDeLFlaxPretrainedModel\"] = None,  # type:ignore\n        module_config: Optional[\"EasyDeLPretrainedConfig\"] = None,  # type:ignore\n        module_config_args: Optional[dict] = None,\n        **kwargs\n):\n\n    \"\"\"The load function is used to load a saved state of the Model and optimizer or Model Only.\n\n    Args:\n        cls: Make the function a class method\n        : Pass in a variable number of arguments\n        step: int: Keep track of the number of steps that have been\n            taken\n        apply_fn: Callable: Apply the optimizer to the model\n        params: core.FrozenDict[str,Any] | Mapping[str,Any]: Pass in\n            the parameters of the model\n        opt_state: Optional[optax.OptState]: optimizer state\n        tx_init: Optional[dict]: Pass the hyperparameters to the\n            optimizer\n        hyperparameters: Optional[dict]: Load hyperparameters from\n            the state dict\n        module: Optional[EasyDeLFlaxPretrainedModel]: Pass in the\n            module\n        module_config: Optional[EasyDeLPretrainedConfig]: Pass the\n            module config\n        module_config_args: Optional[dict]: Pass the config_args to\n            the model\n        **kwargs: Pass in any additional parameters that may be\n            needed for the model\n\n    Returns:\n        A new instance of the class\n    \"\"\"\n    if module_config is not None:\n        module_config = copy.deepcopy(module_config)\n\n    if tx_init is None:\n        tx_init = {}\n    tx_init = copy.deepcopy(tx_init)\n    tx_init = cls.unsafe_dict(tx_init)\n\n    tx_init[\"optimizer\"] = cls.search(\"optimizer\", tx_init, \"adamw\")\n    tx_init[\"scheduler\"] = cls.search(\"scheduler\", tx_init, \"none\")\n    tx_init[\"steps\"] = cls.search(\"steps\", tx_init, 1e6)\n\n    def fix_dict_types(input_dict):\n        fixed_dict = input_dict.copy()\n\n        # Fix extra_optimizer_kwargs\n        if 'extra_optimizer_kwargs' in fixed_dict:\n            fixed_dict['extra_optimizer_kwargs'] = eval(fixed_dict['extra_optimizer_kwargs'])\n\n        # Fix gradient_accumulation_steps\n        if 'gradient_accumulation_steps' in fixed_dict:\n            fixed_dict['gradient_accumulation_steps'] = int(fixed_dict['gradient_accumulation_steps'])\n\n        # Fix steps\n        if 'steps' in fixed_dict:\n            fixed_dict['steps'] = int(fixed_dict['steps'])\n\n        # Fix warmup_steps\n        if 'warmup_steps' in fixed_dict:\n            fixed_dict['warmup_steps'] = int(fixed_dict['warmup_steps'])\n\n        return fixed_dict\n\n    try:\n        tx, sc = get_optimizer_and_scheduler(\n            **tx_init\n        )\n    except TypeError:\n        tx, sc = get_optimizer_and_scheduler(\n            **fix_dict_types(tx_init)\n        )\n    if hyperparameters is None:\n        hyperparameters = {}\n\n    if module_config is not None:\n        hyperparameters = cls.create_hyperparameters(module_config.model_type)\n        cls.safe_dict(module_config.__dict__)\n    return cls(\n        step=step,\n        apply_fn=apply_fn,\n        params=params,\n        tx=tx,\n        opt_state=opt_state,\n        tx_init=cls.safe_dict(tx_init),\n        hyperparameters=hyperparameters,\n        module=module,\n        module_config=module_config,\n        module_config_args=None,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.load_state","title":"<code>load_state(checkpoint_path, dtype=jnp.float32, param_dtype=jnp.float32, precision=None, init_optimizer_state=False, state_shard_fns=None, verbose=False, input_shape=(1, 1), config_kwargs=None, sharding_axes_names=('dp', 'fsdp', 'tp', 'sp'), sharding_axes_dims=(1, -1, 1, 1))</code>  <code>classmethod</code>","text":"<p>The load_state function is a class method that loads the state of an EasyDeLModel from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create an instance of the class</p> required <code>checkpoint_path</code> <code>Union[str, PathLike]</code> <p>str | os.PathLike: Specify the path to the checkpoint file</p> required <code>dtype</code> <code>dtype</code> <p>jnp.dtype: The dtype of the model</p> <code>float32</code> <code>param_dtype</code> <code>dtype</code> <p>jnp.dtype: The dtype of the model parameters</p> <code>float32</code> <code>precision</code> <code>Optional[Union[str, Precision]]</code> <p>Optional[Union[str, jax.lax.Precision]]: precision of the model</p> <code>None</code> <code>init_optimizer_state</code> <code>bool</code> <p>bool: Initialize the optimizer if it's not Initialized yet (if it Initialized the option</p> <code>False</code> <code>state_shard_fns</code> <code>Optional[Mapping[str, Callable]]</code> <p>Optional[Mapping[str,Callable]]: Specify the function that will be used</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>bool: Print out the progress of loading</p> <code>False</code> <code>input_shape</code> <code>Tuple</code> <p>Tuple: input_shape to init module</p> <code>(1, 1)</code> <code>config_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict] : config kwargs to be passed to model config</p> <code>None</code> <p>will be ignored ) to shard the loaded state</p> <p>Returns:</p> Type Description <p>A state object</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>@classmethod\ndef load_state(\n        cls,\n        checkpoint_path: Union[str, os.PathLike],\n        dtype: jnp.dtype = jnp.float32,\n        param_dtype: jnp.dtype = jnp.float32,\n        precision: Optional[Union[str, jax.lax.Precision]] = None,\n        init_optimizer_state: bool = False,\n        state_shard_fns: Optional[Mapping[str, Callable]] = None,\n        verbose: bool = False,\n        input_shape: Tuple = (1, 1),\n        config_kwargs: Optional[dict] = None,\n        sharding_axes_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        sharding_axes_dims: Sequence[int] = (1, -1, 1, 1)\n):\n\n    \"\"\"The load_state function is a class method that loads the state of an EasyDeLModel from a checkpoint.\n\n    Args:\n        cls: Create an instance of the class\n        checkpoint_path: str | os.PathLike: Specify the path to the\n            checkpoint file\n        dtype: jnp.dtype: The dtype of the model\n        param_dtype: jnp.dtype: The dtype of the model parameters\n        precision: Optional[Union[str, jax.lax.Precision]]:\n            precision of the model\n        init_optimizer_state: bool: Initialize the optimizer if it's\n            not Initialized yet (if it Initialized the option\n        state_shard_fns: Optional[Mapping[str,Callable]]: Specify\n            the function that will be used\n        verbose: bool: Print out the progress of loading\n        input_shape: Tuple: input_shape to init module\n        config_kwargs: Optional[dict] : config kwargs to be passed\n            to model config\n    will be ignored )\n    to shard the loaded state\n\n    Returns:\n        A state object\n    \"\"\"\n    from ..modules.auto_easydel_model import get_modules_by_type\n    from fjformer.partition_utils import create_mesh\n    mesh = create_mesh(sharding_axes_dims, sharding_axes_names)\n\n    with mesh:\n        checkpoint = fjformer.CheckpointManager.load_checkpoint(\n            path=checkpoint_path,\n            shard_fns=state_shard_fns,\n            verbose=verbose,\n        )\n        hyperparameters = checkpoint.get(\"hyperparameters\")\n        cfg, module, convertor = get_modules_by_type(model_type=cls.get_model_type(hyperparameters))\n        checkpoint.pop(\"module_config\", None)\n        if checkpoint[\"module_config_args\"] is not None:\n            cfg_behave = cls.unsafe_dict(checkpoint.get(\"module_config_args\", {}))\n            cfg_behave.pop(\"id2label\", None)\n            cfg_behave.pop(\"label2id\", None)\n            cfg_behave.pop(\"torch_dtype\", None)\n            for k, v in cfg_behave.items():\n                if v is None:\n                    cfg_behave.pop(k, None)\n                elif v == \"None\":\n                    cfg_behave[k] = None\n                elif isinstance(v, str):\n                    if v.startswith(\"{\") or v.startswith(\"(\") or v.startswith(\"PartitionSpec\"):\n                        cfg_behave[k] = eval(v)\n            module_config = cfg.from_dict(cfg_behave)\n            if config_kwargs is not None:\n                for k, v in config_kwargs.items():\n                    setattr(module_config, k, v)\n            module_in = module(\n                config=module_config,\n                dtype=dtype,\n                param_dtype=param_dtype,\n                precision=precision,\n                input_shape=input_shape,\n                _do_init=False\n            )\n        else:\n            raise TypeError(\n                \"Om seems like i couldn't read model correctly ;(\"\n            )\n        state = cls.load(\n            apply_fn=module_in.__call__,\n            module=module_in,\n            module_config=module_config,\n            **checkpoint\n        )\n        state = state.replace(\n            module_config_args=None  # removing because it's not needed anymore\n        )\n        if init_optimizer_state:\n            state = state.init_opt_state()\n    return state\n</code></pre>"},{"location":"generated-etils-easystate/#src.python.easydel.etils.easystate.EasyDeLState.save_state","title":"<code>save_state(filename, save_optimizer=False, checkpoint_dir=None, verbose=False, gather_fns=None, float_dtype=None)</code>","text":"<p>The save_state function saves the state of a model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Pass the object itself to the function</p> required <code>filename</code> <code>Union[str, PathLike]</code> <p>str | os.PathLike: Specify the name of the file to save</p> required <code>save_optimizer</code> <code>bool</code> <p>bool: Determine whether to save the optimizer state or not</p> <code>False</code> <code>checkpoint_dir</code> <code>Optional[Union[str, PathLike]]</code> <p>Optional[str | os.PathLike]: Specify the directory where the checkpoint is saved</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>bool: Print out the path of the saved file</p> <code>False</code> <code>gather_fns</code> <code>dict[Callable]</code> <p>dict[Callable]: Specify a dictionary of functions that can be used to gather</p> <code>None</code> <code>float_dtype</code> <code>Union[str, dtype]</code> <p>str | jax.numpy.dtype: Specify the precision of the saved model</p> <code>None</code> <p>:param : Save the optimizer state</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/python/easydel/etils/easystate.py</code> <pre><code>def save_state(\n        self,\n        filename: Union[str, os.PathLike],\n        save_optimizer: bool = False,\n        checkpoint_dir: Optional[Union[str, os.PathLike]] = None,\n        verbose: bool = False,\n        gather_fns: dict[Callable] = None,\n        float_dtype: Union[str, jax.numpy.dtype] = None,\n):\n\n    \"\"\"The save_state function saves the state of a model to disk.\n\n    Args:\n        self: Pass the object itself to the function\n        filename: str | os.PathLike: Specify the name of the file to\n            save\n        save_optimizer: bool: Determine whether to save the\n            optimizer state or not\n        checkpoint_dir: Optional[str | os.PathLike]: Specify the\n            directory where the checkpoint is saved\n        verbose: bool: Print out the path of the saved file\n        gather_fns: dict[Callable]: Specify a dictionary of\n            functions that can be used to gather\n        float_dtype: str | jax.numpy.dtype: Specify the precision of\n            the saved model\n    :param : Save the optimizer state\n\n    Returns:\n        None\n    \"\"\"\n    state = self\n    if not save_optimizer:\n        state = self.replace(\n            opt_state=None\n        )\n    state = state.replace(\n        module_config_args={\n            k: v for k, v in state.module.config.__dict__.items() if\n            isinstance(\n                v, (int, bool, float)\n            )\n        }\n    )\n    fjformer.CheckpointManager.save_state_to_file(\n        state=state,\n        path=os.path.join(checkpoint_dir, filename) if checkpoint_dir is not None else filename,\n        verbose=verbose,\n        gather_fns=gather_fns,\n        float_dtype=float_dtype,\n    )\n</code></pre>"},{"location":"generated-etils-errors/","title":"etils.errors","text":""},{"location":"generated-etils-etils/","title":"etils.etils","text":""},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.EasyDeLGradientCheckPointers","title":"<code>EasyDeLGradientCheckPointers</code>  <code>dataclass</code>","text":"<p>The code snippet is defining a data class called <code>EasyDeLGradientCheckPointers</code> using the <code>@dataclass</code> decorator. A data class is a class that is primarily used to store data, and it automatically generates special methods such as <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code> based on the class attributes.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>@dataclass\nclass EasyDeLGradientCheckPointers:\n    \"\"\"The code snippet is defining a data class called `EasyDeLGradientCheckPointers` using the `@dataclass`\n    decorator. A data class is a class that is primarily used to store data, and it automatically\n    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class\n    attributes.\n    \"\"\"\n    EVERYTHING_SAVEABLE: Literal[\"everything_saveable\"] = \"everything_saveable\"  # Fix Pycharm Debugging Issue\n    NOTHING_SAVEABLE: Literal[\"nothing_saveable\"] = \"nothing_saveable\"  # Fix Pycharm Debugging Issue\n    CHECKPOINT_DOTS: Literal[\"checkpoint_dots\"] = \"checkpoint_dots\"  # Fix Pycharm Debugging Issue\n    CHECKPOINT_DOTS_WITH_NO_BATCH_DMIS: Literal[\"checkpoint_dots_with_no_batch_dims\"] = \\\n        \"checkpoint_dots_with_no_batch_dims\"  # Fix Pycharm Debugging Issue\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.EasyDeLOptimizers","title":"<code>EasyDeLOptimizers</code>  <code>dataclass</code>","text":"<p>The code snippet is defining a data class called <code>EasyDeLOptimizers</code> using the <code>@dataclass</code> decorator. A data class is a class that is primarily used to store data, and it automatically generates special methods such as <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code> based on the class attributes.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>@dataclass\nclass EasyDeLOptimizers:\n    \"\"\"The code snippet is defining a data class called `EasyDeLOptimizers` using the `@dataclass`\n    decorator. A data class is a class that is primarily used to store data, and it automatically\n    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class\n    attributes.\n    \"\"\"\n    ADAFACTOR: Literal[\"adafactor\"] = \"adafactor\"  # Fix Pycharm Debugging Issue\n    LION: Literal[\"lion\"] = \"lion\"  # Fix Pycharm Debugging Issue\n    ADAMW: Literal[\"adamw\"] = 'adamw'  # Fix Pycharm Debugging Issue\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.EasyDeLSchedulers","title":"<code>EasyDeLSchedulers</code>  <code>dataclass</code>","text":"<p>The code snippet is defining a data class called <code>EasyDeLSchedulers</code> using the <code>@dataclass</code> decorator. A data class is a class that is primarily used to store data, and it automatically generates special methods such as <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code> based on the class attributes.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>@dataclass\nclass EasyDeLSchedulers:\n    \"\"\"The code snippet is defining a data class called `EasyDeLSchedulers` using the `@dataclass`\n    decorator. A data class is a class that is primarily used to store data, and it automatically\n    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class\n    attributes.\n    \"\"\"\n    LINEAR: Literal[\"linear\"] = \"linear\"  # Fix Pycharm Debugging Issue\n    COSINE: Literal[\"cosine\"] = \"cosine\"  # Fix Pycharm Debugging Issue\n    NONE: Literal[\"none\"] = \"none\"  # Fix Pycharm Debugging Issue\n    WARM_UP_COSINE: Literal[\"warm_up_cosine\"] = \"warm_up_cosine\"  # Fix Pycharm Debugging Issue\n    WARM_UP_LINEAR: Literal[\"warm_up_linear\"] = \"warm_up_linear\"  # Fix Pycharm Debugging Issue\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.StoreTupleAction","title":"<code>StoreTupleAction</code>","text":"<p>               Bases: <code>Action</code></p> <p>Custom action to store a comma-separated string as a tuple of ints.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>class StoreTupleAction(argparse.Action):\n    \"\"\"Custom action to store a comma-separated string as a tuple of ints.\"\"\"\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        try:\n            setattr(namespace, self.dest, tuple(int(v) for v in values.split(\",\")))\n        except ValueError:\n            raise argparse.ArgumentTypeError(\n                f\"Invalid value for {option_string}: {values} \"\n                f\"(should be comma-separated integers)\"\n            )\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.define_flags_with_default","title":"<code>define_flags_with_default(_required_fields=None, **kwargs)</code>","text":"<p>Defines flags with default values using argparse.</p> <p>Parameters:</p> Name Type Description Default <code>_required_fields</code> <code>List</code> <p>A dictionary with required flag names</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments representing flag names and default values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Namespace, Dict[str, Any]]</code> <p>A tuple containing: - An argparse.Namespace object containing parsed arguments. - A dictionary mapping flag names to default values.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>def define_flags_with_default(\n        _required_fields: List = None,\n        **kwargs\n) -&gt; Tuple[argparse.Namespace, Dict[str, Any]]:\n    \"\"\"Defines flags with default values using argparse.\n\n    Args:\n        _required_fields: A dictionary with required flag names\n        **kwargs: Keyword arguments representing flag names and default values.\n\n    Returns:\n        A tuple containing:\n            - An argparse.Namespace object containing parsed arguments.\n            - A dictionary mapping flag names to default values.\n    \"\"\"\n    _required_fields = _required_fields if _required_fields is not None else []\n    parser = argparse.ArgumentParser()\n\n    default_values = {}\n\n    for name, value in kwargs.items():\n        default_values[name] = value\n\n        # Custom type handling:\n        if isinstance(value, tuple):\n            # For tuples, use a custom action to convert the string to a tuple of ints\n            parser.add_argument(\n                f\"--{name}\",\n                type=str,  # Read as string\n                default=str(value),  # Store default as string\n                help=f\"Value for {name} (comma-separated integers)\",\n                action=StoreTupleAction\n            )\n        else:\n            # For other types, infer type from default value\n            parser.add_argument(\n                f\"--{name}\",\n                type=type(value),\n                default=value,\n                help=f\"Value for {name}\"\n            )\n\n    args = parser.parse_args()\n    for key in _required_fields:\n        if getattr(args, key) == \"\":\n            raise ValueError(f\"Required field {key} for argument parser.\")\n    return args, default_values\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.get_logger","title":"<code>get_logger(name, level=logging.INFO)</code>","text":"<p>Function to create and configure a logger. :param name: str: The name of the logger. :param level: int: The logging level. Defaults to logging.INFO. :return logging.Logger: The configured logger instance.</p> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>def get_logger(name, level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Function to create and configure a logger.\n    :param name: str: The name of the logger.\n    :param level: int: The logging level. Defaults to logging.INFO.\n    :return logging.Logger: The configured logger instance.\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.propagate = False\n\n    # Set the logging level\n    logger.setLevel(level)\n\n    # Create a console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n\n    formatter = logging.Formatter(\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\")\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n    return logger\n</code></pre>"},{"location":"generated-etils-etils/#src.python.easydel.etils.etils.set_loggers_level","title":"<code>set_loggers_level(level=logging.WARNING)</code>","text":"<p>Function to set the logging level of all loggers to the specified level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int: The logging level to set. Defaults to logging.WARNING.</p> <code>WARNING</code> Source code in <code>src/python/easydel/etils/etils.py</code> <pre><code>def set_loggers_level(level: int = logging.WARNING):\n    \"\"\"Function to set the logging level of all loggers to the specified level.\n\n    Args:\n        level: int: The logging level to set. Defaults to\n            logging.WARNING.\n    \"\"\"\n    logging.root.setLevel(level)\n    for handler in logging.root.handlers:\n        handler.setLevel(level)\n</code></pre>"},{"location":"generated-modules-_blockwise_attention/","title":"modules._blockwise_attention","text":"<p>An implementation of Blockwise parallel transformer https://arxiv.org/abs/2305.19370 Also include a reference implementation of memory-efficient transformer https://arxiv.org/abs/2112.05682 from EasyLM https://github.com/young-geng/EasyLM/blob/main/EasyLM/bpt.py</p>"},{"location":"generated-modules-_ring_attention/","title":"modules._ring_attention","text":""},{"location":"generated-modules-_vanilla_attention/","title":"modules._vanilla_attention","text":""},{"location":"generated-modules-arctic-arctic_configuration/","title":"modules.arctic.arctic_configuration","text":""},{"location":"generated-modules-arctic-arctic_configuration/#src.python.easydel.modules.arctic.arctic_configuration.ArcticConfig","title":"<code>ArcticConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/arctic/arctic_configuration.py</code> <pre><code>class ArcticConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"arctic\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=None,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096,\n            initializer_range=0.02,\n            rms_norm_eps=1e-5,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=1e6,\n            sliding_window=None,\n            attention_dropout=0.0,\n            num_experts_per_tok=1,\n            num_local_experts=8,\n            router_aux_loss_coef=0.001,\n            moe_layer_frequency=2,\n            parallel_attn_mlp_res=False,\n            moe_train_capacity_factor=1,\n            moe_eval_capacity_factor=1,\n            enable_expert_tensor_parallelism=False,\n            moe_min_capacity=0,\n            moe_token_dropping=True,\n            quantization=None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.attention_dropout = attention_dropout\n\n        self.num_experts_per_tok = num_experts_per_tok\n        self.num_local_experts = num_local_experts\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.moe_layer_frequency = moe_layer_frequency\n        self.moe_train_capacity_factor = moe_train_capacity_factor\n        self.moe_eval_capacity_factor = moe_eval_capacity_factor\n        self.enable_expert_tensor_parallelism = enable_expert_tensor_parallelism\n        self.moe_min_capacity = moe_min_capacity\n        self.moe_token_dropping = moe_token_dropping\n        self.parallel_attn_mlp_res = parallel_attn_mlp_res\n        self.quantization = quantization\n\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.rope_scaling = rope_scaling\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_sharded_data_parallel: bool: Determine whether to use the fully_sharded_data_parallel partitioning\n         scheme or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n        :param use_scan_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n         variable will turn them off.\n        :param rope_scaling: Dict[str, Union[str, float]]: rope_scaling for rope\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.rope_scaling = rope_scaling\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-arctic-arctic_configuration/#src.python.easydel.modules.arctic.arctic_configuration.ArcticConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, bits=None, rope_scaling=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>:param self: Bind the attributes and methods of a class to an instance of that class :param gradient_checkpointing: str: Determine whether to use gradient checkpointing :param use_scan_mlp: bool: Determine whether to use the scan_mlp function or not :param scan_mlp_chunk_size: int: Chunk the input to the mlp :param bits: Optional[int]: Specify the number of bits to use for quantization  variable will turn them off. :param rope_scaling: Dict[str, Union[str, float]]: rope_scaling for rope :return: A tuple of the following:</p> Source code in <code>src/python/easydel/modules/arctic/arctic_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        **kwargs,\n):\n    \"\"\"\n    The add_jax_args function adds the following arguments to the model:\n\n    :param self: Bind the attributes and methods of a class to an instance of that class\n    :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n    :param use_scan_mlp: bool: Determine whether to use the scan_mlp function or not\n    :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n    :param bits: Optional[int]: Specify the number of bits to use for quantization\n     variable will turn them off.\n    :param rope_scaling: Dict[str, Union[str, float]]: rope_scaling for rope\n    :return: A tuple of the following:\n\n    \"\"\"\n    self.rope_scaling = rope_scaling\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-arctic-arctic_configuration/#src.python.easydel.modules.arctic.arctic_configuration.ArcticConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>:param fully_sharded_data_parallel: bool: Determine whether to use the fully_sharded_data_parallel partitioning  scheme or not :return: A list of tuples</p> Source code in <code>src/python/easydel/modules/arctic/arctic_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"\n    The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    :param fully_sharded_data_parallel: bool: Determine whether to use the fully_sharded_data_parallel partitioning\n     scheme or not\n    :return: A list of tuples\n\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/","title":"modules.arctic.modelling_arctic_flax","text":""},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.ArcticPreTrainedModel","title":"<code>ArcticPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class ArcticPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class: ArcticConfig = ArcticConfig\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    # main_input_name = \"input_ids\"\n\n    def __init__(\n            self,\n            config: ArcticConfig,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: FrozenDict = None\n    ) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n\n        self.config.initialization_of_moe = True\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        self.config.initialization_of_moe = False\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.ArcticPreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n        # attention_mask: Optional[chex.Array] = None\n        jnp.array(attention_mask, dtype=\"i4\"),\n        # position_ids: Optional[chex.Array] = None\n        jnp.array(position_ids, dtype=\"i4\"),\n        None,  # inputs_embeds: Optional[chex.Array] = None\n        output_attentions,  # output_attentions: Optional[bool] = None\n        # output_hidden_states: Optional[bool] = None\n        output_hidden_states,\n        False,  # init_cache: bool = False\n        not train,  # deterministic: bool = True\n        return_dict,  # return_dict: bool = True\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.ArcticPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: FrozenDict = None\n) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n\n    self.config.initialization_of_moe = True\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n        input_shape,\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False\n        )\n    random_params = module_init_outputs[\"params\"]\n\n    self.config.initialization_of_moe = False\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticAttention","title":"<code>FlaxArcticAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class FlaxArcticAttention(BaseJAXAttentionModule):\n    config: ArcticConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            Linear,\n            use_bias=getattr(self.config, \"attention_bias\", False),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.rotary = FlaxArcticRotaryEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length,\n                              self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length,\n                          self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length,\n                              self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        Args:\n            self: Refer to the object itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                model\n            freq_cis: Tuple[chex.Array, chex.Array],: Create the\n                apply_rotary variable\n            attention_mask: chex.Array: Mask the attention weights\n            causal_mask: chex.Array: Mask the attention weights\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights\n\n        Returns:\n            A tuple of (out, attn_output)\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n        outputs = (\n            attn_output, attentions.attention_weights\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice. The call method takes an input tensor (x) and returns an output tensor (y). In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the model</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Create the apply_rotary variable</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of (out, attn_output)</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n    The __call__ method takes an input tensor (x) and returns an output tensor (y).\n    In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n    Args:\n        self: Refer to the object itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            model\n        freq_cis: Tuple[chex.Array, chex.Array],: Create the\n            apply_rotary variable\n        attention_mask: chex.Array: Mask the attention weights\n        causal_mask: chex.Array: Mask the attention weights\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights\n\n    Returns:\n        A tuple of (out, attn_output)\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n    outputs = (\n        attn_output, attentions.attention_weights\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticDecoderLayer","title":"<code>FlaxArcticDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class FlaxArcticDecoderLayer(nn.Module):\n    config: ArcticConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        # hidden_states: chex.Array\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array\n        # causal_mask: chex.Array\n        # position_ids: chex.Array\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = True\n\n        attn_block = FlaxArcticAttention\n        mlp_block = FlaxArcticSparseMoeBlock\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = re_mat(\n                attn_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    1, 3, 4, 6, 7, 8, 9\n                )\n            )\n            mlp_block = re_mat(\n                mlp_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    1,\n                )\n            )\n        self.self_attn = attn_block(\n            config=self.config,\n            layer_index=self.layer_index,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.block_sparse_moe = mlp_block(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = ArcticRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = ArcticRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.parallel_attn_mlp_res = self.config.parallel_attn_mlp_res and self.block_sparse_moe.is_moe_layer\n        if self.parallel_attn_mlp_res:\n            self.residual_layernorm = ArcticRMSNorm(\n                dim=self.config.hidden_size,\n                eps=self.config.rms_norm_eps,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype\n            )\n            self.residual_mlp = ArcticMLP(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                is_residual_mlp=True\n            )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n            by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,\n             used for computing self-attention weights and biases in a more efficient manner than using position\n             embeddings or sinusoidal positional encoding vectors would allow\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n\n        Returns:\n            A tuple of hidden_states and attention_output\n        \"\"\"\n        residual_input = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # hidden_states: chex.Array\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array\n        # causal_mask: chex.Array\n        # position_ids: chex.Array\n        # segment_ids: Optional[chex.Array] = None\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = True\n\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states,\n            freq_cis,\n            attention_mask,\n            causal_mask,\n            position_ids,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions\n        )\n\n        hidden_states = residual_input + hidden_states\n\n        residual_attn = hidden_states\n        if self.parallel_attn_mlp_res:\n\n            hidden_states = self.residual_layernorm(hidden_states)\n            hidden_states = self.residual_mlp(hidden_states)\n            residual_residual = residual_attn + hidden_states\n            # parallel mlp moe part\n            hidden_states = self.post_attention_layernorm(residual_input)\n            hidden_states, gate_loss = self.block_sparse_moe(hidden_states)\n            hidden_states = residual_residual + hidden_states\n        else:\n            hidden_states = self.post_attention_layernorm(hidden_states)\n            hidden_states, gate_loss = self.block_sparse_moe(hidden_states)\n            hidden_states = residual_attn + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        outputs += (gate_loss,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticDecoderLayer.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed     by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,      used for computing self-attention weights and biases in a more efficient manner than using position      embeddings or sinusoidal positional encoding vectors would allow</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states and attention_output</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n        by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,\n         used for computing self-attention weights and biases in a more efficient manner than using position\n         embeddings or sinusoidal positional encoding vectors would allow\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n\n    Returns:\n        A tuple of hidden_states and attention_output\n    \"\"\"\n    residual_input = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n\n    # hidden_states: chex.Array\n    # freq_cis: Tuple[chex.Array, chex.Array],\n    # attention_mask: chex.Array\n    # causal_mask: chex.Array\n    # position_ids: chex.Array\n    # segment_ids: Optional[chex.Array] = None\n    # deterministic: bool = True\n    # init_cache: bool = False\n    # output_attentions: bool = True\n\n    hidden_states, self_attn_weights = self.self_attn(\n        hidden_states,\n        freq_cis,\n        attention_mask,\n        causal_mask,\n        position_ids,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions\n    )\n\n    hidden_states = residual_input + hidden_states\n\n    residual_attn = hidden_states\n    if self.parallel_attn_mlp_res:\n\n        hidden_states = self.residual_layernorm(hidden_states)\n        hidden_states = self.residual_mlp(hidden_states)\n        residual_residual = residual_attn + hidden_states\n        # parallel mlp moe part\n        hidden_states = self.post_attention_layernorm(residual_input)\n        hidden_states, gate_loss = self.block_sparse_moe(hidden_states)\n        hidden_states = residual_residual + hidden_states\n    else:\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states, gate_loss = self.block_sparse_moe(hidden_states)\n        hidden_states = residual_attn + hidden_states\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (self_attn_weights,)\n\n    outputs += (gate_loss,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticDecoderLayerCollection","title":"<code>FlaxArcticDecoderLayerCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class FlaxArcticDecoderLayerCollection(nn.Module):\n    config: ArcticConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.blocks = [\n            FlaxArcticDecoderLayer(\n                layer_index=layer_index,\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(layer_index)\n            )\n\n            for layer_index in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_hidden_states: Optional[bool] = False,\n            output_attentions: Optional[bool] = False,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n             by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector\n            , used for computing self-attention weights and biases in a more efficient manner than using position\n            embeddings or sinusoidal positional encoding vectors would allow for [2].\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states, attention_output,\n            all_hidden_states and all_router_losses\n        \"\"\"\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_router_losses = ()\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                output_attentions=output_attentions,\n                init_cache=init_cache,\n                freq_cis=freq_cis,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n            )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n            all_router_losses += (layer_outputs[-1],)\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (all_self_attns,)\n        if output_hidden_states:\n            outputs += (all_hidden_states,)\n        outputs += (all_router_losses,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticDecoderLayerCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, deterministic=True, init_cache=False, output_hidden_states=False, output_attentions=False)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed      by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector     , used for computing self-attention weights and biases in a more efficient manner than using position     embeddings or sinusoidal positional encoding vectors would allow for [2].</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states, attention_output,</p> <p>all_hidden_states and all_router_losses</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_hidden_states: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n         by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector\n        , used for computing self-attention weights and biases in a more efficient manner than using position\n        embeddings or sinusoidal positional encoding vectors would allow for [2].\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states, attention_output,\n        all_hidden_states and all_router_losses\n    \"\"\"\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    all_router_losses = ()\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            init_cache=init_cache,\n            freq_cis=freq_cis,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n        )\n\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n\n        all_router_losses += (layer_outputs[-1],)\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (all_self_attns,)\n    if output_hidden_states:\n        outputs += (all_hidden_states,)\n    outputs += (all_router_losses,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticForCausalLM","title":"<code>FlaxArcticForCausalLM</code>","text":"<p>               Bases: <code>ArcticPreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class FlaxArcticForCausalLM(ArcticPreTrainedModel):\n    module_class = FlaxArcticForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-arctic-modelling_arctic_flax/#src.python.easydel.modules.arctic.modelling_arctic_flax.FlaxArcticSparseMoeBlock","title":"<code>FlaxArcticSparseMoeBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>This implementation is strictly equivalent to standard MoE with full capacity (no dropped tokens). It's faster since it formulates MoE operations in terms of block-sparse operations to accomodate imbalanced assignments of tokens to experts, whereas standard MoE either (1) drop tokens at the cost of reduced performance or (2) set capacity factor to number of experts and thus waste computation and memory on padding.</p> Source code in <code>src/python/easydel/modules/arctic/modelling_arctic_flax.py</code> <pre><code>class FlaxArcticSparseMoeBlock(nn.Module):\n    \"\"\"This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n    config: ArcticConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[\n        Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.gate = Linear(\n            self.config.num_local_experts,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n        )\n\n        self.experts = FlaxArcticBlocKSparesMLPCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            e: bool = False  # Ignored\n    ) -&gt; Tuple[chex.Array, chex.Array]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n\n        router_logits = self.gate(hidden_states).astype(  # no reshaping is needed\n            jnp.promote_types(self.dtype, jnp.float32)\n        )\n        routing_weights, selected_experts = jax.lax.top_k(\n            router_logits,\n            k=self.config.num_experts_per_tok\n        )\n        routing_weights = jax.nn.softmax(\n            routing_weights.astype(\n                jnp.promote_types(self.dtype, jnp.float32)\n            ), axis=-1\n        )\n\n        return self.experts(\n            selected_experts=selected_experts,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            hidden_dim=hidden_dim,\n            hidden_states=hidden_states,\n            routing_weights=routing_weights\n        ), router_logits\n</code></pre>"},{"location":"generated-modules-attention_module/","title":"modules.attention_module","text":""},{"location":"generated-modules-attention_module/#src.python.easydel.modules.attention_module.AttentionModule","title":"<code>AttentionModule</code>","text":"Source code in <code>src/python/easydel/modules/attention_module.py</code> <pre><code>class AttentionModule:\n    def __init__(\n            self,\n            mesh: Mesh,\n            attn_mechanism: Literal[\n                \"vanilla\",\n                \"flash\",\n                \"splash\",\n                \"ring\",\n                \"cudnn\",\n                \"local_ring\",\n                \"sharded_vanilla\",\n                \"legacy_sharded_vanilla\",\n                \"wise_ring\",\n                \"blockwise\",\n                \"pallas_flash\"\n            ],\n            sm_scale: float,\n            num_attention_heads: int,\n            head_dims: int,\n            block_k: int = ...,\n            block_q: int = ...,\n            block_b: int = ...,\n            block_k_major: int = ...,\n            block_q_major_dkv: int = ...,\n            block_k_major_dkv: int = ...,\n            block_k_dkv: int = ...,\n            block_q_dkv: int = ...,\n            block_k_major_dq: int = ...,\n            block_k_dq: int = ...,\n            block_q_dq: int = ...,\n            query_partition_spec: PartitionSpec = ...,\n            generation_query_partition_spec: PartitionSpec = ...,\n            key_partition_spec: PartitionSpec = ...,\n            value_partition_spec: PartitionSpec = ...,\n            bias_partition_spec: PartitionSpec = ...,\n            generation_bias_partition_spec: PartitionSpec = ...,\n            attention_partition_spec: PartitionSpec = ...,\n            generation_attention_partition_spec: PartitionSpec = ...,\n            scan_ring_attention: bool = ...,\n            scan_attention_layers: bool = ...,\n            attention_dropout: float = 0.0,\n            dtype: jnp.dtype = jnp.float32,\n            precision: lax.Precision = ...,\n            force_float32_tpu: bool = ...,\n            shard_attention_computation: bool = ...,\n            use_sharding_constraint: Optional[bool] = ...,\n            axis_name: str = ...,\n            backward_pass_impl: Literal[\"triton\", \"xla\"] = \"triton\",\n            base_module_class: Optional[EasyDeLPretrainedConfig] = None,\n            _do_check: bool = True\n    ):\n\n        self.block_k: int = ...\n        self.block_q: int = ...\n        self.block_b: int = ...\n        self.block_k_major: int = ...\n        self.block_q_major_dkv: int = ...\n        self.block_k_major_dkv: int = ...\n        self.block_k_dkv: int = ...\n        self.block_q_dkv: int = ...\n        self.block_k_major_dq: int = ...\n        self.block_k_dq: int = ...\n        self.block_q_dq: int = ...\n        self.query_partition_spec: PartitionSpec = ...\n        self.generation_query_partition_spec: PartitionSpec = ...\n        self.key_partition_spec: PartitionSpec = ...\n        self.value_partition_spec: PartitionSpec = ...\n        self.bias_partition_spec: PartitionSpec = ...\n        self.generation_bias_partition_spec: PartitionSpec = ...\n        self.attention_partition_spec: PartitionSpec = ...\n        self.generation_attention_partition_spec: PartitionSpec = ...\n        self.scan_ring_attention: bool = ...\n        self.precision: lax.Precision = ...\n        self.force_float32_tpu: bool = ...\n        self.shard_attention_computation: bool = ...\n        self.use_sharding_constraint: Optional[bool] = ...\n        self.axis_name: str = ...\n\n        set_attrs_smartly_with_prp(self, \"use_sharding_constraint\", False, use_sharding_constraint, base_module_class)\n\n        set_attrs_smartly_with_prp(self, \"block_k_major\", DEFAULT_K_BLOCK, block_k_major, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_b\", 1, block_b, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_q\", DEFAULT_Q_BLOCK, block_q, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_k\", DEFAULT_K_BLOCK, block_k, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_q_major_dkv\", DEFAULT_Q_BLOCK, block_q_major_dkv, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_k_major_dkv\", DEFAULT_K_BLOCK, block_k_major_dkv, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_k_major_dq\", DEFAULT_K_BLOCK, block_k_major_dq, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_k_dkv\", DEFAULT_K_BLOCK, block_k_dkv, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_q_dkv\", DEFAULT_Q_BLOCK, block_q_dkv, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_q_dq\", DEFAULT_Q_BLOCK, block_q_dq, base_module_class)\n        set_attrs_smartly_with_prp(self, \"block_k_dq\", DEFAULT_K_BLOCK, block_k_dq, base_module_class)\n\n        set_attrs_smartly_with_prp(\n            self,\n            \"shard_attention_computation\",\n            True,\n            shard_attention_computation,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"scan_ring_attention\",\n            True,\n            scan_ring_attention,\n            base_module_class\n        )\n\n        set_attrs_smartly_with_prp(\n            self,\n            \"query_partition_spec\",\n            DEFAULT_QPS,\n            query_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"key_partition_spec\",\n            DEFAULT_KPS,\n            key_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"value_partition_spec\",\n            DEFAULT_VPS,\n            value_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"bias_partition_spec\",\n            DEFAULT_BPS,\n            bias_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"attention_partition_spec\",\n            DEFAULT_APS,\n            attention_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"generation_query_partition_spec\",\n            DEFAULT_G_QPS,\n            generation_query_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"generation_bias_partition_spec\",\n            DEFAULT_G_BPS,\n            generation_bias_partition_spec,\n            base_module_class\n        )\n        set_attrs_smartly_with_prp(\n            self,\n            \"generation_attention_partition_spec\",\n            DEFAULT_G_APS,\n            generation_attention_partition_spec,\n            base_module_class\n        )\n\n        set_attrs_smartly_with_prp(self, \"precision\", lax.Precision(\"fastest\"), precision)  # DON'T READ FROM CONFIG\n        set_attrs_smartly_with_prp(self, \"force_float32_tpu\", True, force_float32_tpu)  # DON'T READ FROM CONFIG\n        set_attrs_smartly_with_prp(self, \"axis_name\", \"sp\", axis_name)  # DON'T READ FROM CONFIG\n\n        self.mesh = mesh\n        self.attn_mechanism = attn_mechanism\n        self.platform = jax.lib.xla_bridge.get_backend().platform\n        self.sm_scale = sm_scale\n        self.num_attention_heads = num_attention_heads\n        self.head_dims = head_dims\n\n        self.scan_attention_layers = scan_attention_layers\n        self.attention_dropout = attention_dropout\n        self.dtype = dtype\n        self.backward_pass_impl = backward_pass_impl\n        self._do_check = _do_check\n        if attn_mechanism == \"splash\" and self.platform != \"tpu\":\n            raise OSError(\"splash attention is only supported on TPU.\")\n        if attn_mechanism == \"flash\" and self.platform != \"tpu\":\n            error_msg = \"flash attention is only supported on TPU\"\n            if self.platform == \"gpu\":\n                error_msg += \", for GPUs flash attention you can use `cudnn`.\"\n            raise OSError(error_msg)\n        if attn_mechanism == \"cudnn\" and self.platform != \"gpu\":\n            raise OSError(\"flash attention is only supported on GPU.\")\n\n    def get_block_size_splash_attn(self, q_seq, k_seq):\n        return BlockSizesSplashAttn(\n            block_q=min(self.block_q, q_seq),\n            block_kv_compute=min(self.block_k, k_seq),\n            block_kv=min(self.block_k, k_seq),\n            block_q_dkv=min(self.block_q_dkv, q_seq),\n            block_kv_dkv=min(self.block_k_dkv, k_seq),\n            block_kv_dkv_compute=min(self.block_k_dkv, k_seq),\n            block_q_dq=min(self.block_q_dq, q_seq),\n            block_kv_dq=min(self.block_k_dq, k_seq),\n        )\n\n    def get_block_size_flash_attn(self, q_seq, k_seq):\n        return BlockSizesFlashAttn(\n            block_q=min(self.block_q, q_seq),\n            block_k=min(self.block_k, k_seq),\n            block_q_dkv=min(self.block_q_dkv, q_seq),\n            block_k_dq=min(self.block_k_dkv, k_seq),\n            block_k_dkv=min(self.block_k_dkv, k_seq),\n            block_q_dq=min(self.block_q_dq, q_seq),\n            block_b=min(self.block_b, 1),\n            block_k_major=min(self.block_k_major, k_seq),\n            block_k_major_dq=min(self.block_k_major_dq, k_seq),\n            block_k_major_dkv=min(self.block_k_major_dkv, k_seq),\n            block_q_major_dkv=min(self.block_q_major_dkv, q_seq)\n        )\n\n    def get_partition_specs(self, qs) -&gt; Tuple[\n        PartitionSpec, PartitionSpec, PartitionSpec, PartitionSpec, PartitionSpec, bool\n    ]:\n        is_generating = qs == 1\n        query_sequence_partition = self.generation_query_partition_spec if is_generating else self.query_partition_spec\n        bias_partition_spec = self.generation_bias_partition_spec if is_generating else self.bias_partition_spec\n        attention_partition_spec = self.generation_attention_partition_spec if is_generating else self.attention_partition_spec\n\n        return (\n            query_sequence_partition,\n            self.key_partition_spec,\n            self.value_partition_spec,\n            bias_partition_spec,\n            attention_partition_spec,\n            is_generating\n        )\n\n    def _check_states(\n            self,\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ):\n        batch_size = query_states.shape[0]\n        assert batch_size == key_states.shape[0] == value_states.shape[0], \"Batch Size for q,k,v wont match\"\n        k_v_req_shape = (\n            batch_size,\n            key_value_sequence_length,\n            self.num_attention_heads,\n            self.head_dims\n        )\n        q_shape = (\n            batch_size,\n            query_sequence_length,\n            self.num_attention_heads,\n            self.head_dims\n        )\n\n        assertion_mkv_err = f\"\"\"\n        query_states, key_states, value_states and bias shapes must be like\n        query_states Shape : [batch_size, q_seq_len , {self.num_attention_heads=}, {self.head_dims=}]\n        key_states   Shape : [batch_size, kv_seq_len, {self.num_attention_heads=}, {self.head_dims=}]\n        value_states Shape : [batch_size, kv_seq_len, {self.num_attention_heads=}, {self.head_dims=}]\n        bias         Shape : [batch_size, {self.num_attention_heads=}, q_seq_len , kv_seq_len]\n            \"\"\"\n\n        assert query_states.shape == q_shape, assertion_mkv_err + (\n            f\"\\nMiss Match {query_states.shape} and \"\n            f\"required Shape {q_shape}\"\n        )\n        assert key_states.shape == k_v_req_shape, assertion_mkv_err + (\n            f\"\\nMiss Match {key_states.shape} and \"\n            f\"required Shape {k_v_req_shape}\"\n        )\n        assert value_states.shape == k_v_req_shape, assertion_mkv_err + (\n            f\"\\nMiss Match {value_states.shape} and \"\n            f\"required Shape {k_v_req_shape}\"\n        )\n\n    def __call__(\n            self,\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            causal_mask: Optional[Array] = None,\n            query_sequence_length: Optional[int] = None,\n            key_value_sequence_length: Optional[int] = None,\n            bias: Optional[Array] = None,\n            attention_mask: Optional[Array] = None,\n            segment_ids: Optional[Array] = None,\n            causal: bool = True,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            uses_cache: bool = False\n    ):\n        if query_sequence_length is None:\n            query_sequence_length = query_states.shape[1]\n        if key_value_sequence_length is None:\n            key_value_sequence_length = key_states.shape[1]\n        with self.mesh:\n            if self._do_check:\n                self._check_states(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            if self.attn_mechanism == \"flash\":\n                if segment_ids is not None:\n                    warnings.warn(\n                        \"Flash attention don't support `segment_ids` this argument will be ignored\",\n                        UserWarning\n                    )\n                if self.attention_dropout != 0.0:\n                    warnings.warn(\n                        \"Flash attention don't support `attention_dropout` this argument will be ignored\",\n                        UserWarning\n                    )\n\n                return self.flash_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    causal=causal,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n\n            elif self.attn_mechanism == \"vanilla\":\n\n                return self.vanilla_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    dropout_rng=dropout_rng,\n                    deterministic=deterministic,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"sharded_vanilla\":\n                return self.sharded_vanilla_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    dropout_rng=dropout_rng,\n                    deterministic=deterministic,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"legacy_sharded_vanilla\":\n                return self.legacy_sharded_vanilla_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    dropout_rng=dropout_rng,\n                    deterministic=deterministic,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"ring\":\n                return self.ring_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    dropout_rng=dropout_rng,\n                    deterministic=deterministic,\n                    segment_ids=segment_ids,\n                    attention_mask=attention_mask,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"pallas_flash\":\n                return self.pallas_flash_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    query_sequence_length=query_sequence_length,\n                    bias=bias,\n                )\n            elif self.attn_mechanism == \"splash\":\n                if segment_ids is not None:\n                    warnings.warn(\n                        \"Splash attention don't support `segment_ids` this argument will be ignored\",\n                        UserWarning\n                    )\n                if self.attention_dropout != 0.0:\n                    warnings.warn(\n                        \"Splash attention don't support `attention_dropout` this argument will be ignored\",\n                        UserWarning\n                    )\n                if bias is not None:\n                    warnings.warn(\n                        \"Splash attention don't support `bias` this argument will be ignored\",\n                        UserWarning\n                    )\n\n                return self.splash_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length,\n                    attention_mask=attention_mask\n                )\n            elif self.attn_mechanism == \"blockwise\":\n                if segment_ids is not None:\n                    warnings.warn(\n                        \"BlockWise Attention don't support `segment_ids` this argument will be ignored\",\n                        UserWarning\n                    )\n                return self.blockwise_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    deterministic=deterministic,\n                    dropout_rng=dropout_rng,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"cudnn\":\n                return self.cuddn_flash_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    causal=causal,\n                    deterministic=deterministic,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"local_ring\":\n                if segment_ids is not None:\n                    warnings.warn(\n                        \"LocalRing Attention don't support `segment_ids` this argument will be ignored\",\n                        UserWarning\n                    )\n                if self.attention_dropout != 0.0:\n                    warnings.warn(\n                        \"LocalRing Attention don't support `attention_dropout` this argument will be ignored\",\n                        UserWarning\n                    )\n\n                return self.local_ring_attention(\n                    query_states=query_states,\n                    key_states=key_states,\n                    value_states=value_states,\n                    bias=bias,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            elif self.attn_mechanism == \"wise_ring\":\n                if segment_ids is not None:\n                    warnings.warn(\n                        \"WiseRing Attention don't support `segment_ids` this argument will be ignored\",\n                        UserWarning\n                    )\n                if self.attention_dropout != 0.0:\n                    warnings.warn(\n                        \"WiseRing Attention don't support `attention_dropout` this argument will be ignored\",\n                        UserWarning\n                    )\n\n                return self.wise_ring_attention(\n                    query_states=query_states,\n                    bias=bias,\n                    value_states=value_states,\n                    key_states=key_states,\n                    segment_ids=segment_ids,\n                    query_sequence_length=query_sequence_length,\n                    key_value_sequence_length=key_value_sequence_length\n                )\n            else:\n                raise ValueError(f\"Unknown Attention mechanism of {self.attn_mechanism}\")\n\n    def local_ring_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n            bias: Optional[Array] = None,\n    ):\n        qps, kps, vps, bps, aps, _ = self.get_partition_specs(query_sequence_length)\n        attention_outputs = shard_map(\n            partial(\n                ring_attention_standard,\n                axis_name=self.axis_name,\n                scale=1 / self.sm_scale,\n                float32_logits=True,\n            ),\n            mesh=self.mesh,\n            in_specs=(qps, kps, vps, bps,),\n            out_specs=aps,\n            check_rep=False\n        )(\n            query_states, key_states, value_states, bias\n        )\n        return AttentionOutput(\n            attention_weights=None,\n            attention_outputs=attention_outputs\n        )\n\n    def ring_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n            bias: Optional[Array] = None,\n            attention_mask: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            segment_ids: Optional[Array] = None,\n    ):\n        if segment_ids is None:\n            segment_ids = jnp.zeros((query_states.shape[0], query_sequence_length), dtype=\"i4\")\n        if self.scan_ring_attention and query_states.shape[1] &gt; max(\n                self.block_q,\n                self.block_k\n        ):\n            if self.platform == \"tpu\":\n                ring_attention_fn = ring_flash_attention_tpu\n            else:\n                ring_attention_fn = fjformer.pallas_operations.ring_attention\n            ring_attention_sharded = shard_map(\n                partial(\n                    ring_attention_fn,\n                    axis_name=self.axis_name,\n                    float32_logits=True,\n                    blockwise_kwargs=dict(\n                        deterministic=deterministic,\n                        dropout_rng=dropout_rng,\n                        attn_pdrop=self.attention_dropout,\n                        causal=True,\n                        query_chunk_size=self.block_q,\n                        key_chunk_size=self.block_k,\n                        dtype=self.dtype,\n                        policy=get_gradient_checkpoint_policy(\"nothing_saveable\"),\n                        precision=self.precision,\n                        prevent_cse=not self.scan_attention_layers,\n                    )\n                ),\n                mesh=self.mesh,\n                in_specs=(\n                    self.query_partition_spec,\n                    self.key_partition_spec,\n                    self.value_partition_spec,\n                    self.bias_partition_spec,\n                    PartitionSpec((\"dp\", \"fsdp\"), None),\n                ),\n                out_specs=self.attention_partition_spec,\n                check_rep=False\n            )\n            attn_output = ring_attention_sharded(query_states, key_states, value_states, bias, segment_ids)\n            attn_output = with_sharding_constraint(attn_output, self.attention_partition_spec)\n        else:\n            if self.platform != \"tpu\":\n                warnings.warn(\n                    \"Using Ring attention on CPUs or GPUs are not recommended due to miss computations at the moment. \"\n                    \"please refer to other types of attention mechanism.your are bing fell back on \"\n                    \"`ring_attention_sharded`\"\n                    f\" Usage conditions was\\nscan_ring_attention = {self.scan_ring_attention} [MUST BE TRUE]\"\n                    f\"\\nquery_states.shape[1]({query_states.shape[1]}) &gt; max({self.block_q},{self.block_k})\"\n                    f\"({max(self.block_q, self.block_k)})\"\n                )\n            query_sequence_partition = None if query_states.shape[1] == 1 else \"sp\"\n            ring_attention_sharded = shard_map(\n                partial(\n                    ring_attention_standard,\n                    axis_name=self.axis_name,\n                    scale=self.sm_scale\n                ),\n                mesh=self.mesh,\n                in_specs=(\n                    PartitionSpec((\"dp\", \"fsdp\"), query_sequence_partition, \"tp\", None),\n                    PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                    PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                    PartitionSpec((\"dp\", \"fsdp\"), None, query_sequence_partition, None)\n                ),\n                out_specs=PartitionSpec((\"dp\", \"fsdp\"), query_sequence_partition, \"tp\", None),\n                check_rep=False\n            )\n            attn_output = ring_attention_sharded(\n                query_states, key_states, value_states, attention_mask\n            )\n        return AttentionOutput(\n            attention_weights=None,\n            attention_outputs=attn_output\n        )\n\n    def wise_ring_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n            bias: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            segment_ids: Optional[Array] = None\n    ):\n        if segment_ids is None:\n            segment_ids = jnp.zeros((query_states.shape[0], query_sequence_length), dtype=\"i4\")\n        if self.scan_ring_attention and query_states.shape[1] &gt; max(self.block_q, self.block_k):\n            ring_attention_sharded = shard_map(\n                partial(\n                    wise_ring_attention,\n                    axis_name=self.axis_name,\n                    float32_logits=True,\n                    block_wise_kwargs=dict(\n                        deterministic=deterministic,\n                        dropout_rng=dropout_rng,\n                        attn_pdrop=self.attention_dropout,\n                        causal=True,\n                        query_chunk_size=self.block_q,\n                        key_chunk_size=self.block_k,\n                        dtype=self.dtype,\n                        policy=get_gradient_checkpoint_policy(\"nothing_saveable\"),\n                        precision=self.precision,\n                        prevent_cse=not self.scan_attention_layers,\n                    )\n                ),\n                mesh=self.mesh,\n                in_specs=(\n                    self.query_partition_spec,\n                    self.key_partition_spec,\n                    self.value_partition_spec,\n                    self.bias_partition_spec,\n                    PartitionSpec((\"dp\", \"fsdp\"), \"sp\"),\n                ),\n                out_specs=self.attention_partition_spec,\n                check_rep=False\n            )\n            attn_output = ring_attention_sharded(query_states, key_states, value_states, bias, segment_ids)\n            attn_output = with_sharding_constraint(attn_output, self.attention_partition_spec)\n            return AttentionOutput(\n                attention_weights=None,\n                attention_outputs=attn_output\n            )\n        else:\n            seq_length = query_states.shape[1]\n            chunk = seq_length &gt; max(self.block_q, self.block_k)\n            warnings.warn(\n                f\"generation process detected, switching to local ring attention\"\n                f\" [CHUNK : {chunk}, SCAN : {self.scan_ring_attention}, {self.block_k=}, {self.block_q=}, {seq_length=}]\"\n            )\n            return self.local_ring_attention(\n                query_states=query_states,\n                key_states=key_states,\n                value_states=value_states,\n                bias=bias,\n                query_sequence_length=query_sequence_length,\n                key_value_sequence_length=key_value_sequence_length\n            )\n\n    def vanilla_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            bias: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ) -&gt; AttentionOutput:\n        dtype = jnp.promote_types(self.dtype, jnp.float32)\n        with self.mesh:\n            o, w = vanilla_attention(\n                query_states=query_states,\n                key_states=key_states,\n                value_states=value_states,\n                bias=bias,\n                deterministic=deterministic,\n                dtype=dtype,\n                dropout_rng=dropout_rng,\n                precision=self.precision,\n                attention_dropout=self.attention_dropout,\n                shard_attention_computation=self.shard_attention_computation,\n            )\n            return AttentionOutput(\n                attention_weights=w,\n                attention_outputs=o\n            )\n\n    def blockwise_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            bias: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ) -&gt; AttentionOutput:\n        dtype = jnp.promote_types(self.dtype, jnp.float32)\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n        block_size = self.get_block_size_flash_attn(query_sequence_length, key_value_sequence_length)\n        with self.mesh:\n            query_states = with_sharding_constraint(query_states, qps)\n            key_states = with_sharding_constraint(key_states, self.key_partition_spec)\n            value_states = with_sharding_constraint(value_states, self.value_partition_spec)\n            bias = with_sharding_constraint(bias, bps)\n            o = blockwise_attn(\n                query=query_states,\n                key=key_states,\n                value=value_states,\n                bias=bias,\n                deterministic=deterministic,\n                dtype=dtype,\n                dropout_rng=dropout_rng,\n                precision=self.precision,\n                attn_pdrop=self.attention_dropout,\n                key_chunk_size=block_size.block_k,\n                query_chunk_size=block_size.block_q,\n                prevent_cse=not self.scan_attention_layers,\n                causal=True,\n                float32_logits=True\n            )\n\n            o = with_sharding_constraint(o, aps)\n            return AttentionOutput(\n                attention_weights=None,\n                attention_outputs=o\n            )\n\n    def sharded_vanilla_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            bias: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ) -&gt; AttentionOutput:\n        dtype = jnp.promote_types(self.dtype, jnp.float32)\n\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n\n        with self.mesh:\n            query_states = fjformer.with_sharding_constraint(query_states, qps)\n            key_states = fjformer.with_sharding_constraint(key_states, kps)\n            value_states = fjformer.with_sharding_constraint(value_states, vps)\n\n            query_states, key_states, value_states = promote_dtype(\n                query_states, key_states, value_states,\n                dtype=dtype\n            )\n\n            depth = query_states.shape[-1]\n            query_states = query_states / jnp.sqrt(depth).astype(dtype)\n            attention_weight = jnp.einsum(\"...qhd,...khd-&gt;...hqk\", query_states, key_states, precision=self.precision)\n            if bias is not None:\n                bias = fjformer.with_sharding_constraint(bias, bps)\n                attention_weight = jnp.add(attention_weight, bias)\n\n            attention_weight = jax.nn.softmax(\n                attention_weight.astype(jnp.float32)\n            ).astype(dtype)\n\n            if not deterministic and self.attention_dropout &gt; 0.0:\n                keep_prob = 1.0 - self.attention_dropout\n                dropout_shape = tuple([1] * (key_states.ndim - 2)) + attention_weight.shape[-2:]\n                keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape)  # type: ignore\n\n                multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)\n                attention_weight = attention_weight * multiplier\n\n            attention = jnp.einsum(\n                \"...hqk,...khd-&gt;...qhd\",\n                attention_weight,\n                value_states,\n                precision=self.precision\n            )\n            attention = fjformer.with_sharding_constraint(attention, aps)\n            return AttentionOutput(\n                attention_weights=attention_weight,\n                attention_outputs=attention\n            )\n\n    def legacy_sharded_vanilla_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            bias: Optional[Array] = None,\n            deterministic: bool = False,\n            dropout_rng: Optional[random.PRNGKey] = None,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ) -&gt; AttentionOutput:\n        dtype = jnp.promote_types(self.dtype, jnp.float32)\n\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n\n        with self.mesh:\n            attention_outputs = shard_map(\n                partial(\n                    shard_vanilla_attention,\n                    deterministic=deterministic,\n                    dropout_rng=dropout_rng,\n                    dtype=dtype,\n                    precision=self.precision,\n                    attention_dropout=self.attention_dropout\n                ),\n                in_specs=(qps, kps, vps, PartitionSpec(bps[0], None, None, None) if bias is not None else None),\n                out_specs=aps,\n                check_rep=False,\n                mesh=self.mesh\n            )(query_states, key_states, value_states, bias)\n            attention_outputs = fjformer.with_sharding_constraint(attention_outputs, aps)\n\n            return AttentionOutput(\n                attention_weights=None,\n                attention_outputs=attention_outputs\n            )\n\n    def flash_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n            bias: Optional[Array] = None,\n            causal: bool = False,\n    ) -&gt; AttentionOutput:\n\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n        block_size = self.get_block_size_flash_attn(query_sequence_length, key_value_sequence_length)\n        query_states = query_states.transpose(0, 2, 1, 3)\n        key_states = key_states.transpose(0, 2, 1, 3)\n        value_states = value_states.transpose(0, 2, 1, 3)\n\n        batch_size, num_attention_heads, query_sequence_length, head_dims = query_states.shape\n        if bias is not None:\n            if bias.shape[1] != num_attention_heads:\n                bias = bias.repeat(num_attention_heads, 1, )\n\n        flash_func, float32_logits, _ = get_flash_attention()\n        if float32_logits:\n            query_states, key_states, value_states = map(\n                lambda s: s.astype(jnp.float32),\n                (query_states, key_states, value_states)\n            )\n\n        if self.sm_scale is None:\n            self.sm_scale = 1 / math.sqrt(query_states[-1])\n        attention_o = shard_map(\n            partial(\n                flash_func,\n                causal=causal,\n                sm_scale=self.sm_scale,\n                block_sizes=block_size,\n                debug=False\n            ),\n            in_specs=(qps, kps, vps, bps),\n            out_specs=aps,\n            mesh=self.mesh,\n            check_rep=False,\n        )(\n            query_states,\n            key_states,\n            value_states,\n            bias,\n        )\n\n        attention_o = attention_o.transpose(0, 2, 1, 3)\n        return AttentionOutput(\n            attention_outputs=attention_o,\n            attention_weights=None\n        )\n\n    def splash_attention(\n            self,\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n            attention_mask: Array\n    ) -&gt; AttentionOutput:\n\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n\n        query_states = query_states.transpose(0, 2, 1, 3)\n        key_states = key_states.transpose(0, 2, 1, 3)\n        value_states = value_states.transpose(0, 2, 1, 3)\n\n        query_states, key_states, value_states = map(\n            lambda s: s.astype(jnp.float32),\n            (query_states, key_states, value_states)\n        )\n        if attention_mask is not None:\n            if attention_mask.ndim == 4:\n                attention_mask = attention_mask[:, 0, -1]\n            attention_mask = SegmentIds(attention_mask, attention_mask)\n        else:\n            warnings.warn(\"`attention_mask` is not passed to SplashAttention. (except miss computation problem)\")\n\n        @partial(\n            shard_map,\n            in_specs=(qps, kps, vps, PartitionSpec(qps[0], qps[2])),  # make it easier\n            out_specs=qps,\n            mesh=self.mesh,\n            check_rep=False,\n        )\n        def splash_attention_call(q, k, v, am):\n            block_size = self.get_block_size_splash_attn(query_sequence_length, key_value_sequence_length)\n            masks = [CausalMask(shape=(q.shape[2], k.shape[2])) for _ in range(q.shape[1])]\n            multi_head_mask = MultiHeadMask(masks=masks)\n            splash_kernel = make_splash_mha(\n                mask=multi_head_mask,\n                head_shards=1,\n                q_seq_shards=1,\n                block_sizes=block_size\n            )\n\n            return jax.vmap(splash_kernel)(q, k, v, segment_ids=am)\n\n        attention_o = splash_attention_call(query_states, key_states, value_states, attention_mask)\n\n        attention_o = attention_o.transpose(0, 2, 1, 3)\n        return AttentionOutput(\n            attention_outputs=attention_o,\n            attention_weights=None\n        )\n\n    def pallas_flash_attention(\n            self,\n            *,\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            query_sequence_length: int = None,\n            bias: Optional[Array] = None,\n    ) -&gt; AttentionOutput:\n        if query_sequence_length is None:\n            query_sequence_length = query_states.shape[1]\n        qps, kps, vps, bps, aps, is_gen = self.get_partition_specs(qs=query_sequence_length)\n\n        query_states, key_states, value_states = map(\n            lambda s: s.astype(jnp.float32),\n            (query_states, key_states, value_states)\n        )\n        # query_states = with_sharding_constraint(query_states, qps)\n        # key_states = with_sharding_constraint(key_states, kps)\n        # value_states = with_sharding_constraint(value_states, vps)\n        # bias = with_sharding_constraint(bias, bps)\n        wrapped_fn = partial(\n            flash_attention,\n            sm_scale=self.sm_scale,\n            block_k=self.block_k,\n            block_q=self.block_q,\n            interpret=True if self.platform == \"cpu\" else None,  # auto-decide\n            backward_pass_impl=self.backward_pass_impl,\n            debug=False\n        )\n        attention_outputs = shard_map(\n            f=wrapped_fn,\n            in_specs=(qps, kps, vps, bps),\n            out_specs=aps,\n            mesh=self.mesh,\n            check_rep=False,\n        )(\n            query_states,\n            key_states,\n            value_states,\n            bias,\n        )\n        attention_outputs = with_sharding_constraint(attention_outputs, aps)\n        return AttentionOutput(\n            attention_weights=None,\n            attention_outputs=attention_outputs\n        )\n\n    def cuddn_flash_attention(\n            self,\n            *,  # it's Kwarg Only\n            query_states: Array,\n            key_states: Array,\n            value_states: Array,\n            bias: Optional[Array] = None,\n            causal: bool = False,\n            deterministic: bool = True,\n            query_sequence_length: int,\n            key_value_sequence_length: int,\n    ) -&gt; AttentionOutput:\n        \"\"\"CUDNN Flash Attention with Transformer Engine.\"\"\"\n        try:\n            import transformer_engine.jax.fused_attn as fused_attn\n            from transformer_engine.jax.fused_attn import AttnBiasType, AttnMaskType, QKVLayout\n            from transformer_engine.jax.fused_attn import is_fused_attn_kernel_available\n        except (ModuleNotFoundError, ImportError) as err:\n            raise RuntimeError(\n                \"Please install transformer_engine first. you can install that by running \"\n                f\"`pip install git+https://github.com/NVIDIA/TransformerEngine`\"\n                f\"\\nhere's extra information on error\\n{err}\"\n            )\n        batch, query_sequence_length, num_attention_heads, head_dim = query_states.shape\n\n        qkv_layout = QKVLayout.BS3HD\n        attn_mask_type = AttnMaskType.CAUSAL_MASK\n        attn_bias_type = AttnBiasType.NO_BIAS\n\n        if self.sm_scale is None:\n            self.sm_scale = 1 / math.sqrt(head_dim)\n        has_fused_attn_kernel = is_fused_attn_kernel_available(\n            self.dtype, self.dtype, qkv_layout,\n            attn_bias_type,\n            attn_mask_type,\n            self.attention_dropout,\n            self.num_attention_heads,\n            key_states.shape[2],\n            query_sequence_length,\n            key_value_sequence_length,\n            head_dim\n        )\n\n        if not has_fused_attn_kernel:\n            raise ValueError(\n                \"Flash attention kernel is not supported for current requested arrays\"\n                \" for details check this repo https://github.com/NVIDIA/TransformerEngine/\"\n            )\n\n        return AttentionOutput(\n            attention_weights=None,\n            attention_outputs=fused_attn.self_fused_attn(\n                qkv=jnp.concatenate(\n                    (\n                        jnp.reshape(query_states, (*query_states.shape[:2], 1, *query_states.shape[-2:])),\n                        jnp.reshape(key_states, (*query_states.shape[:2], 1, *query_states.shape[-2:])),\n                        jnp.reshape(value_states, (*query_states.shape[:2], 1, *query_states.shape[-2:]))\n                    ),\n                    axis=2\n                ),\n                bias=bias,\n                mask=jnp.zeros((batch, 1, query_sequence_length, key_value_sequence_length)) if causal else None,\n                seed=None,\n                attn_bias_type=attn_bias_type,\n                attn_mask_type=attn_mask_type,\n                scaling_factor=self.sm_scale,\n                dropout_probability=self.attention_dropout,\n                is_training=deterministic\n            )\n        )\n\n    @staticmethod\n    def test_attentions(\n            batch_size=8,\n            sequence_length=128 * 8,\n            num_attention_heads=32,\n            num_key_value_heads=32,\n            chunk_size=128,\n            axis_dims=(1, -1, 1, 1)\n    ):\n        \"\"\"creates a test for attention module to help you find the best attention mechanism you can use.\"\"\"\n        import flax\n        try:\n            import pandas\n        except (ModuleNotFoundError, ImportError):\n            warnings.warn(\"couldn't import pandas ... please install pandas\")\n            pandas = None\n        from ..modules.mistral import MistralConfig\n        from fjformer import GenerateRNG\n        head_dim = 128\n        rng = GenerateRNG()\n\n        config = MistralConfig(\n            axis_dims=axis_dims,\n            block_q=chunk_size,\n            block_k=chunk_size\n        )\n\n        def value_and_grad_wrapper(fn, **kwargs):\n            @partial(jax.value_and_grad, **kwargs)\n            def inner(*args, **kwargs):\n                return jnp.sum(fn(*args, **kwargs))\n\n            return inner\n\n        def diff(t1, t2):\n            return jnp.max(jnp.abs(t1 - t2))\n\n        @value_and_grad_wrapper\n        def call_dot_product(q, k, v, b, ):\n            attention_pred = flax.linen.dot_product_attention(q, k, v, b, )\n            return attention_pred\n\n        @value_and_grad_wrapper\n        def call_attention_module(q, k, v, b, a, attn_mechanism):\n            attention_pred = AttentionModule(\n                attn_mechanism=attn_mechanism,\n                axis_name=\"sp\",\n                dtype=jnp.float32,\n                mesh=config.jax_mesh(),\n                head_dims=q.shape[-1],\n                sm_scale=1 / math.sqrt(q.shape[-1]),\n                num_attention_heads=q.shape[-2],\n                block_q=config.block_q,\n                block_k=config.block_k,\n                base_module_class=config,\n            )(\n                query_states=q,\n                key_states=k,\n                value_states=v,\n                bias=b,\n                attention_mask=a\n            ).attention_outputs\n            return attention_pred\n\n        def make_inputs():\n            q = jax.random.normal(\n                rng.rng,\n                (batch_size, sequence_length, num_attention_heads, head_dim),\n                dtype=\"float32\"\n            )\n            k = jax.random.normal(\n                rng.rng,\n                (batch_size, sequence_length, num_key_value_heads, head_dim),\n                dtype=\"float32\"\n            )\n            v = jax.random.normal(\n                rng.rng,\n                (batch_size, sequence_length, num_key_value_heads, head_dim),\n                dtype=\"float32\"\n            )\n            c = flax.linen.attention.make_causal_mask(jnp.ones((batch_size, sequence_length)))\n            a = jnp.ones((batch_size, sequence_length))\n            a = a.at[:, sequence_length // 2:].set(0)\n            b = jnp.where(flax.linen.attention.combine_masks(jnp.expand_dims(jnp.expand_dims(a, 1), 1), c), 0, -jnp.inf)\n\n            return q, k, v, b, a\n\n        q, k, v, b, a = make_inputs()\n        excepted_output, excepted_grads = call_dot_product(q, k, v, b)\n        test_attentions = [\n            \"local_ring\",\n            \"blockwise\",\n            \"vanilla\",\n            \"wise_ring\",\n            \"sharded_vanilla\",\n            \"legacy_sharded_vanilla\",\n            \"flash\",\n            \"splash\",\n            \"cudnn\",\n            \"pallas_flash\"\n        ]\n        fns = {\n            k: partial(call_attention_module, attn_mechanism=k) for k in test_attentions\n        }\n        outs_and_grads = {}\n        for nm, fn in fns.items():\n            try:\n                start = time.time()\n                out = jax.block_until_ready(fn(q, k, v, b, a))\n                end = time.time() - start\n                outs_and_grads[nm] = out + (end,)\n            except Exception as e:\n                print(f\"{nm} is Failed :\\n\\n{e}\")\n                outs_and_grads[nm] = (None, None, None)\n        frame_out = {}\n        for key, (out, grad, time_took) in outs_and_grads.items():\n\n            if out is None and grad is None:\n                frame_out[key.upper()] = {\n                    \"OUT DIFF\": \"NA\",\n                    \"GRADIENT DIFF SUM\": \"NA\",\n                    \"TEST PASSED\": \"NA\",\n                    \"COMP TIME\": \"NA\"\n                }\n            else:\n                output_diff = diff(excepted_output, out)\n                g_diff = [diff(*args) for args in zip(excepted_grads, grad)]\n                sum_g = sum(g_diff)\n                # TODO : Fix this\n                # XlaRuntimeError: FAILED_PRECONDITION: The program continuator has halted unexpectedly.\n                # sum_g = jax.device_get(sum_g)\n                # output_diff = jax.device_get(output_diff)\n                frame_out[key.upper()] = {\n                    \"OUT DIFF\": output_diff,\n                    \"GRADIENT DIFF SUM\": sum_g,\n                    \"TEST PASSED\": sum_g &lt; 1 and output_diff &lt; 1e-2,\n                    \"COMP TIME\": time_took\n                }\n        if pandas is not None:\n            result = pandas.DataFrame.from_dict(frame_out)\n            result = result.transpose()\n            return result\n        else:\n            return frame_out\n</code></pre>"},{"location":"generated-modules-attention_module/#src.python.easydel.modules.attention_module.AttentionModule.cuddn_flash_attention","title":"<code>cuddn_flash_attention(*, query_states, key_states, value_states, bias=None, causal=False, deterministic=True, query_sequence_length, key_value_sequence_length)</code>","text":"<p>CUDNN Flash Attention with Transformer Engine.</p> Source code in <code>src/python/easydel/modules/attention_module.py</code> <pre><code>def cuddn_flash_attention(\n        self,\n        *,  # it's Kwarg Only\n        query_states: Array,\n        key_states: Array,\n        value_states: Array,\n        bias: Optional[Array] = None,\n        causal: bool = False,\n        deterministic: bool = True,\n        query_sequence_length: int,\n        key_value_sequence_length: int,\n) -&gt; AttentionOutput:\n    \"\"\"CUDNN Flash Attention with Transformer Engine.\"\"\"\n    try:\n        import transformer_engine.jax.fused_attn as fused_attn\n        from transformer_engine.jax.fused_attn import AttnBiasType, AttnMaskType, QKVLayout\n        from transformer_engine.jax.fused_attn import is_fused_attn_kernel_available\n    except (ModuleNotFoundError, ImportError) as err:\n        raise RuntimeError(\n            \"Please install transformer_engine first. you can install that by running \"\n            f\"`pip install git+https://github.com/NVIDIA/TransformerEngine`\"\n            f\"\\nhere's extra information on error\\n{err}\"\n        )\n    batch, query_sequence_length, num_attention_heads, head_dim = query_states.shape\n\n    qkv_layout = QKVLayout.BS3HD\n    attn_mask_type = AttnMaskType.CAUSAL_MASK\n    attn_bias_type = AttnBiasType.NO_BIAS\n\n    if self.sm_scale is None:\n        self.sm_scale = 1 / math.sqrt(head_dim)\n    has_fused_attn_kernel = is_fused_attn_kernel_available(\n        self.dtype, self.dtype, qkv_layout,\n        attn_bias_type,\n        attn_mask_type,\n        self.attention_dropout,\n        self.num_attention_heads,\n        key_states.shape[2],\n        query_sequence_length,\n        key_value_sequence_length,\n        head_dim\n    )\n\n    if not has_fused_attn_kernel:\n        raise ValueError(\n            \"Flash attention kernel is not supported for current requested arrays\"\n            \" for details check this repo https://github.com/NVIDIA/TransformerEngine/\"\n        )\n\n    return AttentionOutput(\n        attention_weights=None,\n        attention_outputs=fused_attn.self_fused_attn(\n            qkv=jnp.concatenate(\n                (\n                    jnp.reshape(query_states, (*query_states.shape[:2], 1, *query_states.shape[-2:])),\n                    jnp.reshape(key_states, (*query_states.shape[:2], 1, *query_states.shape[-2:])),\n                    jnp.reshape(value_states, (*query_states.shape[:2], 1, *query_states.shape[-2:]))\n                ),\n                axis=2\n            ),\n            bias=bias,\n            mask=jnp.zeros((batch, 1, query_sequence_length, key_value_sequence_length)) if causal else None,\n            seed=None,\n            attn_bias_type=attn_bias_type,\n            attn_mask_type=attn_mask_type,\n            scaling_factor=self.sm_scale,\n            dropout_probability=self.attention_dropout,\n            is_training=deterministic\n        )\n    )\n</code></pre>"},{"location":"generated-modules-attention_module/#src.python.easydel.modules.attention_module.AttentionModule.test_attentions","title":"<code>test_attentions(batch_size=8, sequence_length=128 * 8, num_attention_heads=32, num_key_value_heads=32, chunk_size=128, axis_dims=(1, -1, 1, 1))</code>  <code>staticmethod</code>","text":"<p>creates a test for attention module to help you find the best attention mechanism you can use.</p> Source code in <code>src/python/easydel/modules/attention_module.py</code> <pre><code>@staticmethod\ndef test_attentions(\n        batch_size=8,\n        sequence_length=128 * 8,\n        num_attention_heads=32,\n        num_key_value_heads=32,\n        chunk_size=128,\n        axis_dims=(1, -1, 1, 1)\n):\n    \"\"\"creates a test for attention module to help you find the best attention mechanism you can use.\"\"\"\n    import flax\n    try:\n        import pandas\n    except (ModuleNotFoundError, ImportError):\n        warnings.warn(\"couldn't import pandas ... please install pandas\")\n        pandas = None\n    from ..modules.mistral import MistralConfig\n    from fjformer import GenerateRNG\n    head_dim = 128\n    rng = GenerateRNG()\n\n    config = MistralConfig(\n        axis_dims=axis_dims,\n        block_q=chunk_size,\n        block_k=chunk_size\n    )\n\n    def value_and_grad_wrapper(fn, **kwargs):\n        @partial(jax.value_and_grad, **kwargs)\n        def inner(*args, **kwargs):\n            return jnp.sum(fn(*args, **kwargs))\n\n        return inner\n\n    def diff(t1, t2):\n        return jnp.max(jnp.abs(t1 - t2))\n\n    @value_and_grad_wrapper\n    def call_dot_product(q, k, v, b, ):\n        attention_pred = flax.linen.dot_product_attention(q, k, v, b, )\n        return attention_pred\n\n    @value_and_grad_wrapper\n    def call_attention_module(q, k, v, b, a, attn_mechanism):\n        attention_pred = AttentionModule(\n            attn_mechanism=attn_mechanism,\n            axis_name=\"sp\",\n            dtype=jnp.float32,\n            mesh=config.jax_mesh(),\n            head_dims=q.shape[-1],\n            sm_scale=1 / math.sqrt(q.shape[-1]),\n            num_attention_heads=q.shape[-2],\n            block_q=config.block_q,\n            block_k=config.block_k,\n            base_module_class=config,\n        )(\n            query_states=q,\n            key_states=k,\n            value_states=v,\n            bias=b,\n            attention_mask=a\n        ).attention_outputs\n        return attention_pred\n\n    def make_inputs():\n        q = jax.random.normal(\n            rng.rng,\n            (batch_size, sequence_length, num_attention_heads, head_dim),\n            dtype=\"float32\"\n        )\n        k = jax.random.normal(\n            rng.rng,\n            (batch_size, sequence_length, num_key_value_heads, head_dim),\n            dtype=\"float32\"\n        )\n        v = jax.random.normal(\n            rng.rng,\n            (batch_size, sequence_length, num_key_value_heads, head_dim),\n            dtype=\"float32\"\n        )\n        c = flax.linen.attention.make_causal_mask(jnp.ones((batch_size, sequence_length)))\n        a = jnp.ones((batch_size, sequence_length))\n        a = a.at[:, sequence_length // 2:].set(0)\n        b = jnp.where(flax.linen.attention.combine_masks(jnp.expand_dims(jnp.expand_dims(a, 1), 1), c), 0, -jnp.inf)\n\n        return q, k, v, b, a\n\n    q, k, v, b, a = make_inputs()\n    excepted_output, excepted_grads = call_dot_product(q, k, v, b)\n    test_attentions = [\n        \"local_ring\",\n        \"blockwise\",\n        \"vanilla\",\n        \"wise_ring\",\n        \"sharded_vanilla\",\n        \"legacy_sharded_vanilla\",\n        \"flash\",\n        \"splash\",\n        \"cudnn\",\n        \"pallas_flash\"\n    ]\n    fns = {\n        k: partial(call_attention_module, attn_mechanism=k) for k in test_attentions\n    }\n    outs_and_grads = {}\n    for nm, fn in fns.items():\n        try:\n            start = time.time()\n            out = jax.block_until_ready(fn(q, k, v, b, a))\n            end = time.time() - start\n            outs_and_grads[nm] = out + (end,)\n        except Exception as e:\n            print(f\"{nm} is Failed :\\n\\n{e}\")\n            outs_and_grads[nm] = (None, None, None)\n    frame_out = {}\n    for key, (out, grad, time_took) in outs_and_grads.items():\n\n        if out is None and grad is None:\n            frame_out[key.upper()] = {\n                \"OUT DIFF\": \"NA\",\n                \"GRADIENT DIFF SUM\": \"NA\",\n                \"TEST PASSED\": \"NA\",\n                \"COMP TIME\": \"NA\"\n            }\n        else:\n            output_diff = diff(excepted_output, out)\n            g_diff = [diff(*args) for args in zip(excepted_grads, grad)]\n            sum_g = sum(g_diff)\n            # TODO : Fix this\n            # XlaRuntimeError: FAILED_PRECONDITION: The program continuator has halted unexpectedly.\n            # sum_g = jax.device_get(sum_g)\n            # output_diff = jax.device_get(output_diff)\n            frame_out[key.upper()] = {\n                \"OUT DIFF\": output_diff,\n                \"GRADIENT DIFF SUM\": sum_g,\n                \"TEST PASSED\": sum_g &lt; 1 and output_diff &lt; 1e-2,\n                \"COMP TIME\": time_took\n            }\n    if pandas is not None:\n        result = pandas.DataFrame.from_dict(frame_out)\n        result = result.transpose()\n        return result\n    else:\n        return frame_out\n</code></pre>"},{"location":"generated-modules-attention_module/#src.python.easydel.modules.attention_module.get_flash_attention","title":"<code>get_flash_attention()</code>","text":"<p>return: FlashAttention FN, Upcast Needed to float32,do_shard_map</p> Source code in <code>src/python/easydel/modules/attention_module.py</code> <pre><code>def get_flash_attention() -&gt; Tuple[Callable, bool, bool]:\n    \"\"\"return: FlashAttention FN, Upcast Needed to float32,do_shard_map\"\"\"\n    platform = jax.lib.xla_bridge.get_backend().platform\n    if platform == \"gpu\":\n        warnings.warn(\"for GPU backend use `cudnn` or `pallas_flash`\")\n        float32_logits = False\n        ring_attention_fn = flash_attention\n        do_shard_map = True\n    elif platform == \"tpu\":\n        float32_logits = True\n        ring_attention_fn = tpu_flash_attention\n        do_shard_map = False\n    else:\n        raise ValueError(f\"Unsupported platform {platform}\")\n\n    return ring_attention_fn, float32_logits, do_shard_map\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/","title":"modules.auto_easydel_model","text":""},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoEasyDeLConfig","title":"<code>AutoEasyDeLConfig</code>","text":"Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>class AutoEasyDeLConfig:\n    @classmethod\n    def from_pretrained(\n            cls,\n            pretrained_model_name_or_path: str,\n            sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None),\n            key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            shard_attention_computation: bool = True,\n            backend: Optional[str] = None,\n            **kwargs\n    ) -&gt; EasyDeLPretrainedConfig:\n        \"\"\"The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained\n        model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of\n        the class corresponding to your model, with all weights loaded from disk.\n\n        Args:\n            cls: Create an instance of the class that called this\n                function\n            pretrained_model_name_or_path: str: Identify the model in\n                the huggingface model hub\n            sharding_axis_dims: Sequence[int]: Specify the dimension of\n                each axis in the sharded model\n            sharding_axis_names: Sequence[str]: Specify the order of\n                sharding\n            query_partition_spec: PartitionSpec: Specify the\n                partitioning of the query tensor\n            generation_query_partition_spec: PartitionSpec: Specify the\n                partitioning of the query tensor in\n            key_partition_spec: PartitionSpec: Partition the key matrix\n            value_partition_spec: PartitionSpec: Specify the\n                partitioning of the value tensor\n            bias_partition_spec: PartitionSpec: Specify the Attention\n                Bias partition spec\n            generation_bias_partition_spec: PartitionSpec: Specify the\n                Attention Bias partition spec for generation\n            attention_partition_spec: PartitionSpec: Specify the\n                partitioning of the attention weights\n            shard_attention_computation: bool: whenever to use shard_map\n                for attention\n            backend: Optional[str]: backend to use for model\n            **kwargs: Pass additional arguments to the model and config\n                classes\n        generation process\n\n        Returns:\n            A Model Config\n        \"\"\"\n\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        model_type: str = config.model_type\n\n        cfg, module, trf = get_modules_by_type(model_type)\n        cfg = cfg.from_pretrained(pretrained_model_name_or_path)\n        if hasattr(cfg, 'add_jax_args'):\n            cfg.add_jax_args()\n        cfg.add_basic_configurations(\n            axis_dims=sharding_axis_dims,\n            axis_names=sharding_axis_names,\n            query_partition_spec=query_partition_spec,\n            generation_query_partition_spec=generation_query_partition_spec,\n            generation_bias_partition_spec=generation_bias_partition_spec,\n            key_partition_spec=key_partition_spec,\n            value_partition_spec=value_partition_spec,\n            bias_partition_spec=bias_partition_spec,\n            attention_partition_spec=attention_partition_spec,\n            backend=backend,\n            shard_attention_computation=shard_attention_computation,\n        )\n\n        return cfg\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoEasyDeLConfig.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, sharding_axis_dims=(1, -1, 1, 1), sharding_axis_names=('dp', 'fsdp', 'tp', 'sp'), query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), generation_query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', None, None), key_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), value_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), generation_bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), attention_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), shard_attention_computation=True, backend=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of the class corresponding to your model, with all weights loaded from disk.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create an instance of the class that called this function</p> required <code>pretrained_model_name_or_path</code> <code>str</code> <p>str: Identify the model in the huggingface model hub</p> required <code>sharding_axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimension of each axis in the sharded model</p> <code>(1, -1, 1, 1)</code> <code>sharding_axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Specify the order of sharding</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>generation_query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query tensor in</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', None, None)</code> <code>key_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>generation_bias_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the Attention Bias partition spec for generation</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>shard_attention_computation</code> <code>bool</code> <p>bool: whenever to use shard_map for attention</p> <code>True</code> <code>backend</code> <code>Optional[str]</code> <p>Optional[str]: backend to use for model</p> <code>None</code> <code>**kwargs</code> <p>Pass additional arguments to the model and config classes</p> <code>{}</code> <p>generation process</p> <p>Returns:</p> Type Description <code>EasyDeLPretrainedConfig</code> <p>A Model Config</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n        cls,\n        pretrained_model_name_or_path: str,\n        sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n        sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None),\n        key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        shard_attention_computation: bool = True,\n        backend: Optional[str] = None,\n        **kwargs\n) -&gt; EasyDeLPretrainedConfig:\n    \"\"\"The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained\n    model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of\n    the class corresponding to your model, with all weights loaded from disk.\n\n    Args:\n        cls: Create an instance of the class that called this\n            function\n        pretrained_model_name_or_path: str: Identify the model in\n            the huggingface model hub\n        sharding_axis_dims: Sequence[int]: Specify the dimension of\n            each axis in the sharded model\n        sharding_axis_names: Sequence[str]: Specify the order of\n            sharding\n        query_partition_spec: PartitionSpec: Specify the\n            partitioning of the query tensor\n        generation_query_partition_spec: PartitionSpec: Specify the\n            partitioning of the query tensor in\n        key_partition_spec: PartitionSpec: Partition the key matrix\n        value_partition_spec: PartitionSpec: Specify the\n            partitioning of the value tensor\n        bias_partition_spec: PartitionSpec: Specify the Attention\n            Bias partition spec\n        generation_bias_partition_spec: PartitionSpec: Specify the\n            Attention Bias partition spec for generation\n        attention_partition_spec: PartitionSpec: Specify the\n            partitioning of the attention weights\n        shard_attention_computation: bool: whenever to use shard_map\n            for attention\n        backend: Optional[str]: backend to use for model\n        **kwargs: Pass additional arguments to the model and config\n            classes\n    generation process\n\n    Returns:\n        A Model Config\n    \"\"\"\n\n    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n    model_type: str = config.model_type\n\n    cfg, module, trf = get_modules_by_type(model_type)\n    cfg = cfg.from_pretrained(pretrained_model_name_or_path)\n    if hasattr(cfg, 'add_jax_args'):\n        cfg.add_jax_args()\n    cfg.add_basic_configurations(\n        axis_dims=sharding_axis_dims,\n        axis_names=sharding_axis_names,\n        query_partition_spec=query_partition_spec,\n        generation_query_partition_spec=generation_query_partition_spec,\n        generation_bias_partition_spec=generation_bias_partition_spec,\n        key_partition_spec=key_partition_spec,\n        value_partition_spec=value_partition_spec,\n        bias_partition_spec=bias_partition_spec,\n        attention_partition_spec=attention_partition_spec,\n        backend=backend,\n        shard_attention_computation=shard_attention_computation,\n    )\n\n    return cfg\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoEasyDeLModelForCausalLM","title":"<code>AutoEasyDeLModelForCausalLM</code>","text":"<p>This class provides a convenient way to load and shard pretrained causal language models from the Hugging Face Hub and convert them into EasyDeL compatible models. It utilizes the EasyDeL library for distributed training and inference with JAX.</p> <p>This class inherits from the <code>EasyDeLFlaxPretrainedModel</code> class, providing functionalities for model loading, parameter sharding, and interaction with the EasyDeL framework.</p> <p>Examples:</p> <pre><code>import jax\nfrom easydel import AutoEasyDeLModelForCausalLM\n\n# Load a GPT-2 model on a single CPU\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    device=jax.devices(\"cpu\")[0]\n)\n\n# Load a GPT-2 model sharded across 8 GPUs with data parallelism (DP) and fully sharded data parallelism (FSDP)\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    sharding_axis_dims=(1, 8, 1, 1),\n    sharding_axis_names=(\"dp\", \"fsdp\", \"tp\", \"sp\"),\n    device=jax.devices(\"cpu\")[0] # offload to CPU [OPTIONAL]\n)\n</code></pre> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>class AutoEasyDeLModelForCausalLM:\n    \"\"\"This class provides a convenient way to load and shard pretrained causal language models from the Hugging Face Hub\n    and convert them into EasyDeL compatible models. It utilizes the EasyDeL library for distributed training and inference\n    with JAX.\n\n    This class inherits from the `EasyDeLFlaxPretrainedModel` class, providing functionalities for model loading,\n    parameter sharding, and interaction with the EasyDeL framework.\n\n    Attributes:\n        None\n\n    Examples:\n        ```python\n        import jax\n        from easydel import AutoEasyDeLModelForCausalLM\n\n        # Load a GPT-2 model on a single CPU\n        model, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n            \"gpt2\",\n            device=jax.devices(\"cpu\")[0]\n        )\n\n        # Load a GPT-2 model sharded across 8 GPUs with data parallelism (DP) and fully sharded data parallelism (FSDP)\n        model, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n            \"gpt2\",\n            sharding_axis_dims=(1, 8, 1, 1),\n            sharding_axis_names=(\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            device=jax.devices(\"cpu\")[0] # offload to CPU [OPTIONAL]\n        )\n        ```\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            pretrained_model_name_or_path: str,\n            device=jax.devices('cpu')[0],\n            dtype: jax.numpy.dtype = jax.numpy.float32,\n            param_dtype: jax.numpy.dtype = jax.numpy.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            shard_attention_computation: bool = True,\n            input_shape: Tuple[int, int] = (1, 1),\n            shard_fns: Optional[Mapping[tuple, Callable] | dict] = None,\n            backend: Optional[str] = None,\n            config_kwargs: Optional[Mapping[str, Any]] = None,\n            auto_shard_params: bool = False,\n            partition_rules: Optional[Tuple[Tuple[str, PartitionSpec], ...]] = None,\n            load_in_8bit: bool = False,\n            bit_targeted_params: Optional[List[str]] = None,\n            verbose_params: bool = False,\n            **kwargs\n    ) -&gt; Tuple[EasyDeLFlaxPretrainedModel, dict]:\n        \"\"\"Loads and shards a pretrained causal language model from the Hugging Face Hub and converts it into an\n        EasyDeL compatible model.\n\n        Args:\n            pretrained_model_name_or_path (str): Path or name of the pretrained model in the Hugging Face Hub.\n            device (jax.Array, optional): Device to load the model on. Defaults to the first CPU.\n            dtype (jax.numpy.dtype, optional): Data type of the model. Defaults to jax.numpy.float32.\n            param_dtype (jax.numpy.dtype, optional): Data type of the model parameters. Defaults to jax.numpy.float32.\n            precision (jax.lax.Precision, optional): Precision for computations. Defaults to jax.lax.Precision(\"fastest\").\n            sharding_axis_dims (Sequence[int], optional): Dimensions of each sharding axis. Defaults to (1, -1, 1, 1).\n            sharding_axis_names (Sequence[str], optional): Names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").\n            query_partition_spec (PartitionSpec, optional): Partitioning specification for the query tensor. Defaults to\n                PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n            generation_query_partition_spec (PartitionSpec, optional): Partitioning specification for the query tensor during\n                generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None).\n            key_partition_spec (PartitionSpec, optional): Partitioning specification for the key tensor. Defaults to\n                PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n            value_partition_spec (PartitionSpec, optional): Partitioning specification for the value tensor. Defaults to\n                PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n            bias_partition_spec (PartitionSpec, optional): Partitioning specification for the attention bias. Defaults to\n                PartitionSpec((\"dp\", \"fsdp\"), None, None, None).\n            generation_bias_partition_spec (PartitionSpec, optional): Partitioning specification for the attention bias during\n                generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, None, None).\n            attention_partition_spec (PartitionSpec, optional): Partitioning specification for the attention weights. Defaults to\n                PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n            shard_attention_computation (bool, optional): Whether to shard attention computation. Defaults to True.\n            input_shape (Tuple[int, int], optional): Shape of the input to the model. Defaults to (1, 1).\n            shard_fns (Optional[Mapping[tuple, Callable] | dict], optional): Sharding functions to use for the model. If None,\n                auto-sharding is used if auto_shard_params is True. Defaults to None.\n            backend (Optional[str], optional): Backend to use for the model. Defaults to None.\n            config_kwargs (Optional[Mapping[str, Any]], optional): Configuration keyword arguments to pass to the model config.\n                Defaults to None.\n            auto_shard_params (bool, optional): Whether to automatically shard the model parameters. Defaults to False.\n            partition_rules (Optional[Tuple[Tuple[str, PartitionSpec]]], optional): Custom partition rules for parameter\n                sharding. If not None, shard_fns should also be provided. Defaults to None.\n            load_in_8bit (bool, optional): Whether to load the model parameters in 8-bit precision. Defaults to False.\n            bit_targeted_params (Optional[List[str]], optional): List of parameter names to convert to 8-bit precision. If\n                None and load_in_8bit is True, all kernels and embeddings are converted to 8-bit. Defaults to None.\n            verbose_params (bool): whenever to log number of parameters in converting state.\n            **kwargs: Additional keyword arguments to pass to the model and config classes.\n\n        Returns:\n            Tuple[EasyDeLFlaxPretrainedModel, dict]: A tuple containing the EasyDeL model and the loaded and sharded\n                model parameters.\n        \"\"\"\n\n        logger.debug(f\"Downloading model config from {pretrained_model_name_or_path}\")\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        model_type: str = config.model_type\n\n        cfg, module, trf = get_modules_by_type(model_type)\n\n        logger.debug(f\"Downloading model weights from {pretrained_model_name_or_path}\")\n        model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        if verbose_params:\n            print(f\"PyTorch - HF Model contains {sum(p.numel() for p in model.parameters()) / 1e9} Billion Parameters\")\n        cfg = cfg.from_pretrained(pretrained_model_name_or_path)\n        t_state_dict = model.state_dict()\n        del model\n        gc.collect()\n        logger.debug(f\"adding model basic EasyDeL configurations.\")\n        if hasattr(cfg, 'add_jax_args'):\n            cfg.add_jax_args()\n        cfg.add_basic_configurations(\n            axis_dims=sharding_axis_dims,\n            axis_names=sharding_axis_names,\n            query_partition_spec=query_partition_spec,\n            generation_query_partition_spec=generation_query_partition_spec,\n            generation_bias_partition_spec=generation_bias_partition_spec,\n            key_partition_spec=key_partition_spec,\n            value_partition_spec=value_partition_spec,\n            bias_partition_spec=bias_partition_spec,\n            attention_partition_spec=attention_partition_spec,\n            backend=backend,\n            shard_attention_computation=shard_attention_computation,\n        )\n        if config_kwargs is not None:\n            for k, v in config_kwargs.items():\n                setattr(cfg, k, v)\n\n        logger.debug(\"creating easydel model\")\n        ed_model = module(\n            config=cfg,\n            _do_init=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            input_shape=input_shape\n        )\n\n        needs = [\n            s.replace(\".kernel\", \".weight\").replace(\".scale\", \".weight\").replace(\".embedding\", \".weight\") for s in\n            list(flax.traverse_util.flatten_dict(ed_model.params_shape_tree, sep=\".\").keys())\n        ]\n        state_dict = {}\n        for k in list(t_state_dict.keys()):\n            tensor = t_state_dict.pop(k)\n            if k in needs:\n                state_dict[k] = tensor\n            else:\n                logger.debug(f\"removing {k} from weights as it was not needed by flax model\")\n        del t_state_dict\n        if shard_fns is not None:\n            if auto_shard_params:\n                warnings.warn(\n                    \"`auto_shard_params` will be ignored since you are passing custom sharding functions\"\n                )\n            logger.debug(\"sharding model parameters based on the given shard_fns.\")\n            if not is_flatten(shard_fns):\n                shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n        elif auto_shard_params:\n            shard_fns, _ = AutoShardAndGatherFunctions.from_pretrained(\n                pretrained_model_name_or_path=pretrained_model_name_or_path,\n                dtype_specs=param_dtype,\n                partition_rules=partition_rules,\n                sharding_axis_dims=sharding_axis_dims,\n                sharding_axis_names=sharding_axis_names,\n                query_partition_spec=query_partition_spec,\n                generation_query_partition_spec=generation_query_partition_spec,\n                key_partition_spec=key_partition_spec,\n                value_partition_spec=value_partition_spec,\n                bias_partition_spec=bias_partition_spec,\n                generation_bias_partition_spec=generation_bias_partition_spec,\n                attention_partition_spec=attention_partition_spec,\n                shard_attention_computation=shard_attention_computation,\n                backend=backend,\n                input_shape=input_shape,  # type:ignore\n                config_kwargs=config_kwargs\n            )\n        with cfg.jax_mesh():\n            logger.debug(\"converting huggingface-model to easydel-model.\")\n            params_pattern_selection = None\n            if load_in_8bit:\n                if bit_targeted_params is None:\n                    warnings.warn(\n                        \"since `bit_targeted_params` is set to None, auto loader will convert all of\"\n                        \" kernels(weights) and embeddings to 8bit by default\"\n                    )\n                    bit_targeted_params = [\n                        \"kernel\",\n                        \"embedding\"\n                    ]\n\n                    params_pattern_selection = re.compile(\"({})\".format(\"|\".join(bit_targeted_params)))\n\n            params = trf(\n                state_dict,\n                config=config,\n                device=device,\n                shard_fns=shard_fns,\n                convert_to_8bit=load_in_8bit,\n                params_pattern_selection=params_pattern_selection,\n                remove_state_dict=True\n            )\n        logger.debug(\"deleting huggingface-model\")\n\n        del state_dict\n        gc.collect()\n\n        if is_flatten(params):\n            logger.info(\"converted parameters are flatten making them unflatten \")\n            params = unflatten_dict(params)\n\n        if verbose_params:\n            print(\n                f\"JAX - EasyDeL Model contains {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(params))[0]) / 1e9} Billion Parameters\"\n            )\n        return ed_model, params\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoEasyDeLModelForCausalLM.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, device=jax.devices('cpu')[0], dtype=jax.numpy.float32, param_dtype=jax.numpy.float32, precision=jax.lax.Precision('fastest'), sharding_axis_dims=(1, -1, 1, 1), sharding_axis_names=('dp', 'fsdp', 'tp', 'sp'), query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), generation_query_partition_spec=PartitionSpec(('dp', 'fsdp'), None, 'tp', None), key_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), value_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), generation_bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), attention_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), shard_attention_computation=True, input_shape=(1, 1), shard_fns=None, backend=None, config_kwargs=None, auto_shard_params=False, partition_rules=None, load_in_8bit=False, bit_targeted_params=None, verbose_params=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads and shards a pretrained causal language model from the Hugging Face Hub and converts it into an EasyDeL compatible model.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model_name_or_path</code> <code>str</code> <p>Path or name of the pretrained model in the Hugging Face Hub.</p> required <code>device</code> <code>Array</code> <p>Device to load the model on. Defaults to the first CPU.</p> <code>devices('cpu')[0]</code> <code>dtype</code> <code>dtype</code> <p>Data type of the model. Defaults to jax.numpy.float32.</p> <code>float32</code> <code>param_dtype</code> <code>dtype</code> <p>Data type of the model parameters. Defaults to jax.numpy.float32.</p> <code>float32</code> <code>precision</code> <code>Precision</code> <p>Precision for computations. Defaults to jax.lax.Precision(\"fastest\").</p> <code>Precision('fastest')</code> <code>sharding_axis_dims</code> <code>Sequence[int]</code> <p>Dimensions of each sharding axis. Defaults to (1, -1, 1, 1).</p> <code>(1, -1, 1, 1)</code> <code>sharding_axis_names</code> <code>Sequence[str]</code> <p>Names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the query tensor. Defaults to PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>generation_query_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the query tensor during generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None).</p> <code>PartitionSpec(('dp', 'fsdp'), None, 'tp', None)</code> <code>key_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the key tensor. Defaults to PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the value tensor. Defaults to PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the attention bias. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, None, None).</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>generation_bias_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the attention bias during generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, None, None).</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>Partitioning specification for the attention weights. Defaults to PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>shard_attention_computation</code> <code>bool</code> <p>Whether to shard attention computation. Defaults to True.</p> <code>True</code> <code>input_shape</code> <code>Tuple[int, int]</code> <p>Shape of the input to the model. Defaults to (1, 1).</p> <code>(1, 1)</code> <code>shard_fns</code> <code>Optional[Mapping[tuple, Callable] | dict]</code> <p>Sharding functions to use for the model. If None, auto-sharding is used if auto_shard_params is True. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>Backend to use for the model. Defaults to None.</p> <code>None</code> <code>config_kwargs</code> <code>Optional[Mapping[str, Any]]</code> <p>Configuration keyword arguments to pass to the model config. Defaults to None.</p> <code>None</code> <code>auto_shard_params</code> <code>bool</code> <p>Whether to automatically shard the model parameters. Defaults to False.</p> <code>False</code> <code>partition_rules</code> <code>Optional[Tuple[Tuple[str, PartitionSpec]]]</code> <p>Custom partition rules for parameter sharding. If not None, shard_fns should also be provided. Defaults to None.</p> <code>None</code> <code>load_in_8bit</code> <code>bool</code> <p>Whether to load the model parameters in 8-bit precision. Defaults to False.</p> <code>False</code> <code>bit_targeted_params</code> <code>Optional[List[str]]</code> <p>List of parameter names to convert to 8-bit precision. If None and load_in_8bit is True, all kernels and embeddings are converted to 8-bit. Defaults to None.</p> <code>None</code> <code>verbose_params</code> <code>bool</code> <p>whenever to log number of parameters in converting state.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model and config classes.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[EasyDeLFlaxPretrainedModel, dict]</code> <p>Tuple[EasyDeLFlaxPretrainedModel, dict]: A tuple containing the EasyDeL model and the loaded and sharded model parameters.</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n        cls,\n        pretrained_model_name_or_path: str,\n        device=jax.devices('cpu')[0],\n        dtype: jax.numpy.dtype = jax.numpy.float32,\n        param_dtype: jax.numpy.dtype = jax.numpy.float32,\n        precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n        sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n        sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n        key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        shard_attention_computation: bool = True,\n        input_shape: Tuple[int, int] = (1, 1),\n        shard_fns: Optional[Mapping[tuple, Callable] | dict] = None,\n        backend: Optional[str] = None,\n        config_kwargs: Optional[Mapping[str, Any]] = None,\n        auto_shard_params: bool = False,\n        partition_rules: Optional[Tuple[Tuple[str, PartitionSpec], ...]] = None,\n        load_in_8bit: bool = False,\n        bit_targeted_params: Optional[List[str]] = None,\n        verbose_params: bool = False,\n        **kwargs\n) -&gt; Tuple[EasyDeLFlaxPretrainedModel, dict]:\n    \"\"\"Loads and shards a pretrained causal language model from the Hugging Face Hub and converts it into an\n    EasyDeL compatible model.\n\n    Args:\n        pretrained_model_name_or_path (str): Path or name of the pretrained model in the Hugging Face Hub.\n        device (jax.Array, optional): Device to load the model on. Defaults to the first CPU.\n        dtype (jax.numpy.dtype, optional): Data type of the model. Defaults to jax.numpy.float32.\n        param_dtype (jax.numpy.dtype, optional): Data type of the model parameters. Defaults to jax.numpy.float32.\n        precision (jax.lax.Precision, optional): Precision for computations. Defaults to jax.lax.Precision(\"fastest\").\n        sharding_axis_dims (Sequence[int], optional): Dimensions of each sharding axis. Defaults to (1, -1, 1, 1).\n        sharding_axis_names (Sequence[str], optional): Names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").\n        query_partition_spec (PartitionSpec, optional): Partitioning specification for the query tensor. Defaults to\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n        generation_query_partition_spec (PartitionSpec, optional): Partitioning specification for the query tensor during\n            generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None).\n        key_partition_spec (PartitionSpec, optional): Partitioning specification for the key tensor. Defaults to\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n        value_partition_spec (PartitionSpec, optional): Partitioning specification for the value tensor. Defaults to\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n        bias_partition_spec (PartitionSpec, optional): Partitioning specification for the attention bias. Defaults to\n            PartitionSpec((\"dp\", \"fsdp\"), None, None, None).\n        generation_bias_partition_spec (PartitionSpec, optional): Partitioning specification for the attention bias during\n            generation. Defaults to PartitionSpec((\"dp\", \"fsdp\"), None, None, None).\n        attention_partition_spec (PartitionSpec, optional): Partitioning specification for the attention weights. Defaults to\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None).\n        shard_attention_computation (bool, optional): Whether to shard attention computation. Defaults to True.\n        input_shape (Tuple[int, int], optional): Shape of the input to the model. Defaults to (1, 1).\n        shard_fns (Optional[Mapping[tuple, Callable] | dict], optional): Sharding functions to use for the model. If None,\n            auto-sharding is used if auto_shard_params is True. Defaults to None.\n        backend (Optional[str], optional): Backend to use for the model. Defaults to None.\n        config_kwargs (Optional[Mapping[str, Any]], optional): Configuration keyword arguments to pass to the model config.\n            Defaults to None.\n        auto_shard_params (bool, optional): Whether to automatically shard the model parameters. Defaults to False.\n        partition_rules (Optional[Tuple[Tuple[str, PartitionSpec]]], optional): Custom partition rules for parameter\n            sharding. If not None, shard_fns should also be provided. Defaults to None.\n        load_in_8bit (bool, optional): Whether to load the model parameters in 8-bit precision. Defaults to False.\n        bit_targeted_params (Optional[List[str]], optional): List of parameter names to convert to 8-bit precision. If\n            None and load_in_8bit is True, all kernels and embeddings are converted to 8-bit. Defaults to None.\n        verbose_params (bool): whenever to log number of parameters in converting state.\n        **kwargs: Additional keyword arguments to pass to the model and config classes.\n\n    Returns:\n        Tuple[EasyDeLFlaxPretrainedModel, dict]: A tuple containing the EasyDeL model and the loaded and sharded\n            model parameters.\n    \"\"\"\n\n    logger.debug(f\"Downloading model config from {pretrained_model_name_or_path}\")\n    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n    model_type: str = config.model_type\n\n    cfg, module, trf = get_modules_by_type(model_type)\n\n    logger.debug(f\"Downloading model weights from {pretrained_model_name_or_path}\")\n    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs)\n    if verbose_params:\n        print(f\"PyTorch - HF Model contains {sum(p.numel() for p in model.parameters()) / 1e9} Billion Parameters\")\n    cfg = cfg.from_pretrained(pretrained_model_name_or_path)\n    t_state_dict = model.state_dict()\n    del model\n    gc.collect()\n    logger.debug(f\"adding model basic EasyDeL configurations.\")\n    if hasattr(cfg, 'add_jax_args'):\n        cfg.add_jax_args()\n    cfg.add_basic_configurations(\n        axis_dims=sharding_axis_dims,\n        axis_names=sharding_axis_names,\n        query_partition_spec=query_partition_spec,\n        generation_query_partition_spec=generation_query_partition_spec,\n        generation_bias_partition_spec=generation_bias_partition_spec,\n        key_partition_spec=key_partition_spec,\n        value_partition_spec=value_partition_spec,\n        bias_partition_spec=bias_partition_spec,\n        attention_partition_spec=attention_partition_spec,\n        backend=backend,\n        shard_attention_computation=shard_attention_computation,\n    )\n    if config_kwargs is not None:\n        for k, v in config_kwargs.items():\n            setattr(cfg, k, v)\n\n    logger.debug(\"creating easydel model\")\n    ed_model = module(\n        config=cfg,\n        _do_init=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        precision=precision,\n        input_shape=input_shape\n    )\n\n    needs = [\n        s.replace(\".kernel\", \".weight\").replace(\".scale\", \".weight\").replace(\".embedding\", \".weight\") for s in\n        list(flax.traverse_util.flatten_dict(ed_model.params_shape_tree, sep=\".\").keys())\n    ]\n    state_dict = {}\n    for k in list(t_state_dict.keys()):\n        tensor = t_state_dict.pop(k)\n        if k in needs:\n            state_dict[k] = tensor\n        else:\n            logger.debug(f\"removing {k} from weights as it was not needed by flax model\")\n    del t_state_dict\n    if shard_fns is not None:\n        if auto_shard_params:\n            warnings.warn(\n                \"`auto_shard_params` will be ignored since you are passing custom sharding functions\"\n            )\n        logger.debug(\"sharding model parameters based on the given shard_fns.\")\n        if not is_flatten(shard_fns):\n            shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n    elif auto_shard_params:\n        shard_fns, _ = AutoShardAndGatherFunctions.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            dtype_specs=param_dtype,\n            partition_rules=partition_rules,\n            sharding_axis_dims=sharding_axis_dims,\n            sharding_axis_names=sharding_axis_names,\n            query_partition_spec=query_partition_spec,\n            generation_query_partition_spec=generation_query_partition_spec,\n            key_partition_spec=key_partition_spec,\n            value_partition_spec=value_partition_spec,\n            bias_partition_spec=bias_partition_spec,\n            generation_bias_partition_spec=generation_bias_partition_spec,\n            attention_partition_spec=attention_partition_spec,\n            shard_attention_computation=shard_attention_computation,\n            backend=backend,\n            input_shape=input_shape,  # type:ignore\n            config_kwargs=config_kwargs\n        )\n    with cfg.jax_mesh():\n        logger.debug(\"converting huggingface-model to easydel-model.\")\n        params_pattern_selection = None\n        if load_in_8bit:\n            if bit_targeted_params is None:\n                warnings.warn(\n                    \"since `bit_targeted_params` is set to None, auto loader will convert all of\"\n                    \" kernels(weights) and embeddings to 8bit by default\"\n                )\n                bit_targeted_params = [\n                    \"kernel\",\n                    \"embedding\"\n                ]\n\n                params_pattern_selection = re.compile(\"({})\".format(\"|\".join(bit_targeted_params)))\n\n        params = trf(\n            state_dict,\n            config=config,\n            device=device,\n            shard_fns=shard_fns,\n            convert_to_8bit=load_in_8bit,\n            params_pattern_selection=params_pattern_selection,\n            remove_state_dict=True\n        )\n    logger.debug(\"deleting huggingface-model\")\n\n    del state_dict\n    gc.collect()\n\n    if is_flatten(params):\n        logger.info(\"converted parameters are flatten making them unflatten \")\n        params = unflatten_dict(params)\n\n    if verbose_params:\n        print(\n            f\"JAX - EasyDeL Model contains {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(params))[0]) / 1e9} Billion Parameters\"\n        )\n    return ed_model, params\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoShardAndGatherFunctions","title":"<code>AutoShardAndGatherFunctions</code>","text":"<p>A class to automatically generate shard and gather functions for a given model configuration.</p> <p>This class provides two methods to generate shard and gather functions:</p> <ul> <li><code>from_config</code>: Generates functions based on a provided <code>EasyDeLPretrainedConfig</code> object.</li> <li><code>from_pretrained</code>: Generates functions based on a pretrained model name or path.</li> </ul> <p>Methods:</p> Name Description <code>from_config</code> <p>Generates shard and gather functions based on a provided <code>EasyDeLPretrainedConfig</code> object.</p> <code>from_pretrained</code> <p>Generates functions based on a pretrained model name or path.</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>class AutoShardAndGatherFunctions:\n    \"\"\"\n    A class to automatically generate shard and gather functions for a given model configuration.\n\n    This class provides two methods to generate shard and gather functions:\n\n    - `from_config`: Generates functions based on a provided `EasyDeLPretrainedConfig` object.\n    - `from_pretrained`: Generates functions based on a pretrained model name or path.\n\n    Attributes:\n        None\n\n    Methods:\n        from_config: Generates shard and gather functions based on a provided `EasyDeLPretrainedConfig` object.\n        from_pretrained: Generates functions based on a pretrained model name or path.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n            cls,\n            config: EasyDeLPretrainedConfig,\n            partition_rules: Optional[Tuple[Tuple[str, PartitionSpec]]] = None,\n            flatten: bool = True,\n            dtype_specs=jax.numpy.float16,\n            input_shape: Tuple[int, int] = (1, 1),\n            depth_target: Optional[List[str]] = None\n    ):\n        \"\"\"\n        Generates shard and gather functions based on a provided `EasyDeLPretrainedConfig` object.\n\n        Args:\n            config: An `EasyDeLPretrainedConfig` object containing the model configuration.\n            partition_rules: A tuple of tuples containing partition rule names and `PartitionSpec` objects.\n                If None, uses the default partition rules from the `config`.\n            flatten: Whether to flatten the shard and gather functions. Defaults to True.\n            dtype_specs: The data type to use for the shard and gather functions. Defaults to `jax.numpy.float16`.\n            input_shape: The input shape of the model. Defaults to (1, 1).\n            depth_target: Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.\n\n        Returns:\n            A tuple containing the shard and gather functions.\n        \"\"\"\n        if partition_rules is None:\n            warnings.warn(\"Using config partition rules from `get_partition_rules(fully_sharded_data_parallel=True)`\")\n            partition_rules = config.get_partition_rules(True)\n        _, module, _ = get_modules_by_type(config.model_type)\n        model = module(\n            config=config,\n            _do_init=False,\n            input_shape=input_shape\n        )\n\n        partition_specs = match_partition_rules(\n            partition_rules,\n            model.params_shape_tree\n        )\n        shard_fns, gather_fns = make_shard_and_gather_fns(\n            partition_specs=partition_specs,\n            dtype_specs=dtype_specs\n        )\n        if depth_target is not None:\n            for dp in depth_target[::-1]:\n                gather_fns = {dp: gather_fns}\n                shard_fns = {dp: shard_fns}\n        if flatten and not is_flatten(shard_fns):\n            gather_fns = flax.traverse_util.flatten_dict(gather_fns)\n            shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n        elif not flatten and is_flatten(shard_fns):\n            gather_fns = flax.traverse_util.unflatten_dict(gather_fns)\n            shard_fns = flax.traverse_util.unflatten_dict(shard_fns)\n\n        return shard_fns, gather_fns\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            pretrained_model_name_or_path: str,\n            input_shape: Tuple[int, int] = (1, 1),\n            sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None),\n            key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            shard_attention_computation: bool = True,\n            backend: Optional[str] = None,\n            partition_rules: Optional[Tuple[Tuple[str, PartitionSpec]]] = None,\n            flatten: bool = True,\n            dtype_specs=jax.numpy.float16,\n            config_kwargs: Optional[Mapping[str, Any]] = None,\n            depth_target: Optional[List[str]] = None\n    ) -&gt; Tuple[Mapping[str, Callable], Mapping[str, Callable]]:\n        \"\"\"\n        Generates shard and gather functions based on a pretrained model name or path.\n\n        Args:\n            pretrained_model_name_or_path: The name or path of the pretrained model.\n            input_shape: The input shape of the model. Defaults to (1, 1).\n            sharding_axis_dims: The dimensions of the sharding axes. Defaults to (1, -1, 1, 1).\n            sharding_axis_names: The names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").\n            query_partition_spec: The partition specification for the query matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n            generation_query_partition_spec: The partition specification for the generation query matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None)`.\n            key_partition_spec: The partition specification for the key matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n            value_partition_spec: The partition specification for the value matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n            bias_partition_spec: The partition specification for the bias. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), None, None, None)`.\n            generation_bias_partition_spec: The partition specification for the generation bias. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), None, None, None)`.\n            attention_partition_spec: The partition specification for the attention computation. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n            shard_attention_computation: Whether to shard the attention computation. Defaults to True.\n            backend: The backend to use for sharding. Defaults to None.\n            partition_rules: A tuple of tuples containing partition rule names and `PartitionSpec` objects.\n                If None, uses the default partition rules from the `config`.\n            flatten: Whether to flatten the shard and gather functions. Defaults to True.\n            dtype_specs: The data type to use for the shard and gather functions. Defaults to `jax.numpy.float16`.\n            config_kwargs: Additional keyword arguments to pass to the `AutoEasyDeLConfig` constructor. Defaults to None.\n            depth_target: Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.\n\n        Returns:\n            A tuple containing the shard and gather functions.\n        \"\"\"\n        config = AutoEasyDeLConfig.from_pretrained(\n            pretrained_model_name_or_path,\n            sharding_axis_dims=sharding_axis_dims,\n            sharding_axis_names=sharding_axis_names,\n            query_partition_spec=query_partition_spec,\n            generation_query_partition_spec=generation_query_partition_spec,\n            key_partition_spec=key_partition_spec,\n            value_partition_spec=value_partition_spec,\n            bias_partition_spec=bias_partition_spec,\n            generation_bias_partition_spec=generation_bias_partition_spec,\n            attention_partition_spec=attention_partition_spec,\n            shard_attention_computation=shard_attention_computation,\n            backend=backend,\n        )\n        if config_kwargs is not None:\n            for k, v in config_kwargs.items():\n                setattr(config, k, v)\n        return cls.from_config(\n            config=config,\n            partition_rules=partition_rules,\n            flatten=flatten,\n            dtype_specs=dtype_specs,\n            input_shape=input_shape,\n            depth_target=depth_target\n        )\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoShardAndGatherFunctions.from_config","title":"<code>from_config(config, partition_rules=None, flatten=True, dtype_specs=jax.numpy.float16, input_shape=(1, 1), depth_target=None)</code>  <code>classmethod</code>","text":"<p>Generates shard and gather functions based on a provided <code>EasyDeLPretrainedConfig</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EasyDeLPretrainedConfig</code> <p>An <code>EasyDeLPretrainedConfig</code> object containing the model configuration.</p> required <code>partition_rules</code> <code>Optional[Tuple[Tuple[str, PartitionSpec]]]</code> <p>A tuple of tuples containing partition rule names and <code>PartitionSpec</code> objects. If None, uses the default partition rules from the <code>config</code>.</p> <code>None</code> <code>flatten</code> <code>bool</code> <p>Whether to flatten the shard and gather functions. Defaults to True.</p> <code>True</code> <code>dtype_specs</code> <p>The data type to use for the shard and gather functions. Defaults to <code>jax.numpy.float16</code>.</p> <code>float16</code> <code>input_shape</code> <code>Tuple[int, int]</code> <p>The input shape of the model. Defaults to (1, 1).</p> <code>(1, 1)</code> <code>depth_target</code> <code>Optional[List[str]]</code> <p>Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple containing the shard and gather functions.</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>@classmethod\ndef from_config(\n        cls,\n        config: EasyDeLPretrainedConfig,\n        partition_rules: Optional[Tuple[Tuple[str, PartitionSpec]]] = None,\n        flatten: bool = True,\n        dtype_specs=jax.numpy.float16,\n        input_shape: Tuple[int, int] = (1, 1),\n        depth_target: Optional[List[str]] = None\n):\n    \"\"\"\n    Generates shard and gather functions based on a provided `EasyDeLPretrainedConfig` object.\n\n    Args:\n        config: An `EasyDeLPretrainedConfig` object containing the model configuration.\n        partition_rules: A tuple of tuples containing partition rule names and `PartitionSpec` objects.\n            If None, uses the default partition rules from the `config`.\n        flatten: Whether to flatten the shard and gather functions. Defaults to True.\n        dtype_specs: The data type to use for the shard and gather functions. Defaults to `jax.numpy.float16`.\n        input_shape: The input shape of the model. Defaults to (1, 1).\n        depth_target: Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.\n\n    Returns:\n        A tuple containing the shard and gather functions.\n    \"\"\"\n    if partition_rules is None:\n        warnings.warn(\"Using config partition rules from `get_partition_rules(fully_sharded_data_parallel=True)`\")\n        partition_rules = config.get_partition_rules(True)\n    _, module, _ = get_modules_by_type(config.model_type)\n    model = module(\n        config=config,\n        _do_init=False,\n        input_shape=input_shape\n    )\n\n    partition_specs = match_partition_rules(\n        partition_rules,\n        model.params_shape_tree\n    )\n    shard_fns, gather_fns = make_shard_and_gather_fns(\n        partition_specs=partition_specs,\n        dtype_specs=dtype_specs\n    )\n    if depth_target is not None:\n        for dp in depth_target[::-1]:\n            gather_fns = {dp: gather_fns}\n            shard_fns = {dp: shard_fns}\n    if flatten and not is_flatten(shard_fns):\n        gather_fns = flax.traverse_util.flatten_dict(gather_fns)\n        shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n    elif not flatten and is_flatten(shard_fns):\n        gather_fns = flax.traverse_util.unflatten_dict(gather_fns)\n        shard_fns = flax.traverse_util.unflatten_dict(shard_fns)\n\n    return shard_fns, gather_fns\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.AutoShardAndGatherFunctions.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, input_shape=(1, 1), sharding_axis_dims=(1, -1, 1, 1), sharding_axis_names=('dp', 'fsdp', 'tp', 'sp'), query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), generation_query_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', None, None), key_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), value_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), generation_bias_partition_spec=PartitionSpec(('dp', 'fsdp'), None, None, None), attention_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None), shard_attention_computation=True, backend=None, partition_rules=None, flatten=True, dtype_specs=jax.numpy.float16, config_kwargs=None, depth_target=None)</code>  <code>classmethod</code>","text":"<p>Generates shard and gather functions based on a pretrained model name or path.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model_name_or_path</code> <code>str</code> <p>The name or path of the pretrained model.</p> required <code>input_shape</code> <code>Tuple[int, int]</code> <p>The input shape of the model. Defaults to (1, 1).</p> <code>(1, 1)</code> <code>sharding_axis_dims</code> <code>Sequence[int]</code> <p>The dimensions of the sharding axes. Defaults to (1, -1, 1, 1).</p> <code>(1, -1, 1, 1)</code> <code>sharding_axis_names</code> <code>Sequence[str]</code> <p>The names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the query matrix. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>generation_query_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the generation query matrix. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', None, None)</code> <code>key_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the key matrix. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the value matrix. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the bias. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), None, None, None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>generation_bias_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the generation bias. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), None, None, None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>The partition specification for the attention computation. Defaults to <code>PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)</code>.</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>shard_attention_computation</code> <code>bool</code> <p>Whether to shard the attention computation. Defaults to True.</p> <code>True</code> <code>backend</code> <code>Optional[str]</code> <p>The backend to use for sharding. Defaults to None.</p> <code>None</code> <code>partition_rules</code> <code>Optional[Tuple[Tuple[str, PartitionSpec]]]</code> <p>A tuple of tuples containing partition rule names and <code>PartitionSpec</code> objects. If None, uses the default partition rules from the <code>config</code>.</p> <code>None</code> <code>flatten</code> <code>bool</code> <p>Whether to flatten the shard and gather functions. Defaults to True.</p> <code>True</code> <code>dtype_specs</code> <p>The data type to use for the shard and gather functions. Defaults to <code>jax.numpy.float16</code>.</p> <code>float16</code> <code>config_kwargs</code> <code>Optional[Mapping[str, Any]]</code> <p>Additional keyword arguments to pass to the <code>AutoEasyDeLConfig</code> constructor. Defaults to None.</p> <code>None</code> <code>depth_target</code> <code>Optional[List[str]]</code> <p>Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Mapping[str, Callable], Mapping[str, Callable]]</code> <p>A tuple containing the shard and gather functions.</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n        cls,\n        pretrained_model_name_or_path: str,\n        input_shape: Tuple[int, int] = (1, 1),\n        sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n        sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None),\n        key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        shard_attention_computation: bool = True,\n        backend: Optional[str] = None,\n        partition_rules: Optional[Tuple[Tuple[str, PartitionSpec]]] = None,\n        flatten: bool = True,\n        dtype_specs=jax.numpy.float16,\n        config_kwargs: Optional[Mapping[str, Any]] = None,\n        depth_target: Optional[List[str]] = None\n) -&gt; Tuple[Mapping[str, Callable], Mapping[str, Callable]]:\n    \"\"\"\n    Generates shard and gather functions based on a pretrained model name or path.\n\n    Args:\n        pretrained_model_name_or_path: The name or path of the pretrained model.\n        input_shape: The input shape of the model. Defaults to (1, 1).\n        sharding_axis_dims: The dimensions of the sharding axes. Defaults to (1, -1, 1, 1).\n        sharding_axis_names: The names of the sharding axes. Defaults to (\"dp\", \"fsdp\", \"tp\", \"sp\").\n        query_partition_spec: The partition specification for the query matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n        generation_query_partition_spec: The partition specification for the generation query matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None)`.\n        key_partition_spec: The partition specification for the key matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n        value_partition_spec: The partition specification for the value matrix. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n        bias_partition_spec: The partition specification for the bias. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), None, None, None)`.\n        generation_bias_partition_spec: The partition specification for the generation bias. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), None, None, None)`.\n        attention_partition_spec: The partition specification for the attention computation. Defaults to `PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)`.\n        shard_attention_computation: Whether to shard the attention computation. Defaults to True.\n        backend: The backend to use for sharding. Defaults to None.\n        partition_rules: A tuple of tuples containing partition rule names and `PartitionSpec` objects.\n            If None, uses the default partition rules from the `config`.\n        flatten: Whether to flatten the shard and gather functions. Defaults to True.\n        dtype_specs: The data type to use for the shard and gather functions. Defaults to `jax.numpy.float16`.\n        config_kwargs: Additional keyword arguments to pass to the `AutoEasyDeLConfig` constructor. Defaults to None.\n        depth_target: Pad the sharding to depth, for example make {params:tensor} with depth_target = [\"row\"] to {row:{params:tensor}}. Defaults to None.\n\n    Returns:\n        A tuple containing the shard and gather functions.\n    \"\"\"\n    config = AutoEasyDeLConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        sharding_axis_dims=sharding_axis_dims,\n        sharding_axis_names=sharding_axis_names,\n        query_partition_spec=query_partition_spec,\n        generation_query_partition_spec=generation_query_partition_spec,\n        key_partition_spec=key_partition_spec,\n        value_partition_spec=value_partition_spec,\n        bias_partition_spec=bias_partition_spec,\n        generation_bias_partition_spec=generation_bias_partition_spec,\n        attention_partition_spec=attention_partition_spec,\n        shard_attention_computation=shard_attention_computation,\n        backend=backend,\n    )\n    if config_kwargs is not None:\n        for k, v in config_kwargs.items():\n            setattr(config, k, v)\n    return cls.from_config(\n        config=config,\n        partition_rules=partition_rules,\n        flatten=flatten,\n        dtype_specs=dtype_specs,\n        input_shape=input_shape,\n        depth_target=depth_target\n    )\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.get_modules_by_type","title":"<code>get_modules_by_type(model_type)</code>","text":"The get_modules_by_type function is a helper function that returns the following <ol> <li>The config class for the model type specified (e.g., LlamaConfig, FalconConfig)</li> <li>The Flax Model class for the model type specified (e.g., FlaxLlamaForCausalLM, FlaxFalconForCausalLM)</li> <li>A function to convert a HuggingFace pretrained checkpoint into an easydel checkpoint</li> </ol> <p>:param model_type: str: Determine which model to use :return: A tuple of three elements (BaseConfig,BaseModel,Func To Transform Model from Torch to EasyDeL)</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>def get_modules_by_type(model_type: str) -&gt; Tuple[\n    Type[EasyDeLPretrainedConfig], Type[EasyDeLFlaxPretrainedModel] | Any, partial | Any\n]:\n    \"\"\"\n    The get_modules_by_type function is a helper function that returns the following:\n        1. The config class for the model type specified (e.g., LlamaConfig, FalconConfig)\n        2. The Flax Model class for the model type specified (e.g., FlaxLlamaForCausalLM, FlaxFalconForCausalLM)\n        3. A function to convert a HuggingFace pretrained checkpoint into an easydel checkpoint\n\n    :param model_type: str: Determine which model to use\n    :return: A tuple of three elements (BaseConfig,BaseModel,Func To Transform Model from Torch to EasyDeL)\n\n    \"\"\"\n    if model_type == \"llama\":\n        from .llama import LlamaConfig as _LlamaConfig\n        from .llama import FlaxLlamaForCausalLM as _FlaxLlamaForCausalLM\n        return (\n            _LlamaConfig,\n            _FlaxLlamaForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"gemma\":\n\n        from .gemma import GemmaConfig as _GemmaConfig\n        from .gemma import FlaxGemmaForCausalLM as _FlaxGemmaForCausalLM\n        return (\n            _GemmaConfig,\n            _FlaxGemmaForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"falcon\":\n        from .falcon import FlaxFalconForCausalLM as _FlaxFalconForCausalLM\n        from .falcon import FalconConfig as _FalconConfig\n        return (\n            _FalconConfig,\n            _FlaxFalconForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"word_embeddings\"],\n                layer_norm_names=[\n                    \"input_layernorm\",\n                    \"ln_f\",\n                    \"ln_attn\",\n                    \"ln_mlp\",\n                    \"post_attention_layernorm\"\n                ],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"mpt\":\n        from .mosaic_mpt import FlaxMptForCausalLM as _FlaxMptForCausalLM\n        from .mosaic_mpt import MptConfig as _MptConfig\n        return (\n            _MptConfig,\n            _FlaxMptForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"wte\"],\n                rnn_based_or_rwkv=False,\n                layer_norm_names=[\n                    \"norm_1\", \"norm_2\", \"norm_f\"\n                ]\n            )\n        )\n\n    elif model_type == \"mistral\":\n        from .mistral import FlaxMistralForCausalLM as _FlaxMistralForCausalLM\n        from .mistral import MistralConfig as _MistralConfig\n        return (\n            _MistralConfig,\n            _FlaxMistralForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"gptj\":\n        from .gpt_j import FlaxGPTJForCausalLM as _FlaxGPTJForCausalLM\n        from .gpt_j import GPTJConfig as _GPTJConfig\n        return (\n            _GPTJConfig,\n            _FlaxGPTJForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=\"wte\",\n                layer_norm_names=[\n                    \"ln_1\", \"ln_2\", \"ln_f\",\n                ],\n                rnn_based_or_rwkv=False\n            )\n        )\n\n    elif model_type == \"gpt_neox\":\n        from .gpt_neo_x import FlaxGPTNeoXForCausalLM as _FlaxGPTNeoXForCausalLM\n        from .gpt_neo_x import GPTNeoXConfig as _GPTNeoXConfig\n\n        return (\n            _GPTNeoXConfig,\n            _FlaxGPTNeoXForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=\"wte\",\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"palm\":\n        from .palm import FlaxPalmForCausalLM as _FlaxPalmForCausalLM\n        from .palm import PalmConfig as _PalmConfig\n        return (\n            _PalmConfig,\n            _FlaxPalmForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=\"wte\",\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"lt\":\n        from .lucid_transformer import FlaxLTForCausalLM as _FlaxLTForCausalLM\n        from .lucid_transformer import FlaxLTConfig as _FlaxLTConfig\n\n        return (\n            _FlaxLTConfig,\n            _FlaxLTForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=\"wte\",\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"gpt2\":\n        from .gpt2 import FlaxGPT2LMHeadModel as _FlaxGPT2LMHeadModel\n        from .gpt2 import GPT2Config as _GPT2Config\n\n        return (\n            _GPT2Config,\n            _FlaxGPT2LMHeadModel,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"wte\", \"wpe\"],\n                layer_norm_names=[\n                    \"ln_1\", \"ln_2\", \"ln_f\"\n                ],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"mixtral\":\n        from .mixtral import FlaxMixtralForCausalLM as _FlaxMixtralForCausalLM\n        from .mixtral import MixtralConfig as _MixtralConfig\n        return (\n            _MixtralConfig,\n            _FlaxMixtralForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"phi\":\n        from .phi import FlaxPhiForCausalLM as _FlaxPhiForCausalLM\n        from .phi import PhiConfig as _PhiConfig\n        return (\n            _PhiConfig,\n            _FlaxPhiForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                layer_norm_names=[\n                    \"input_layernorm\",\n                    \"final_layernorm\",\n                    \"q_layernorm\",\n                    \"k_layernorm\"\n                ],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"qwen\":\n        from .qwen1 import Qwen1Config as _Qwen1Config\n        from .qwen1 import FlaxQwen1ForCausalLM as _FlaxQwen1ForCausalLM\n        return (\n            _Qwen1Config,\n            _FlaxQwen1ForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"wte\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n\n    elif model_type == \"qwen2\":\n        from .qwen2 import Qwen2Config as _Qwen2Config\n        from .qwen2 import FlaxQwen2ForCausalLM as _FlaxQwen2ForCausalLM\n        return (\n            _Qwen2Config,\n            _FlaxQwen2ForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"stablelm\":\n        from .stablelm import StableLmConfig as _StableLmConfig\n        from .stablelm import FlaxStableLmForCausalLM as _FlaxStableLmForCausalLM\n\n        return (\n            _StableLmConfig,\n            _FlaxStableLmForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                layer_norm_names=[\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"rwkv\":\n        from .rwkv import RwkvConfig as _RwkvConfig\n        from .rwkv import FlaxRwkvForCausalLM as _FlaxRwkvForCausalLM\n        return (\n            _RwkvConfig,\n            _FlaxRwkvForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embeddings\"],\n                layer_norm_names=[\"ln_out\", \"ln2\", \"ln1\", \"pre_ln\"],\n                rnn_based_or_rwkv=True\n            )\n        )\n    elif model_type == \"mamba\":\n        from .mamba import MambaConfig as _MambaConfig\n        from .mamba import FlaxMambaForCausalLM as _FlaxMambaForCausalLM\n        return (\n            _MambaConfig,\n            _FlaxMambaForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embeddings\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"grok-1\":\n        from .grok_1 import Grok1Config as _Grok1Config\n        from .grok_1 import FlaxGrok1ForCausalLM as _FlaxGrok1ForCausalLM\n        return (\n            _Grok1Config,\n            _FlaxGrok1ForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"qwen2_moe\":\n        from .qwen2_moe import Qwen2MoeConfig as _Qwen2MoeConfig\n        from .qwen2_moe import FlaxQwen2MoeForCausalLM as _FlaxQwen2MoeForCausalLM\n        return (\n            _Qwen2MoeConfig,\n            _FlaxQwen2MoeForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"cohere\":\n        from .cohere import CohereConfig as _CohereConfig\n        from .cohere import FlaxCohereForCausalLM as _FlaxCohereForCausalLM\n        return (\n            _CohereConfig,\n            _FlaxCohereForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"dbrx\":\n        from .dbrx import DbrxConfig as _DbrxConfig\n        from .dbrx import FlaxDbrxForCausalLM as _FlaxDbrxForCausalLM\n        return (\n            _DbrxConfig,\n            _FlaxDbrxForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"wte\"],\n                rnn_based_or_rwkv=False,\n                layer_norm_names=[\"norm_1\", \"norm_2\", \"norm_f\"]\n            )\n        )\n    elif model_type == \"phi3\":\n        from .phi3 import Phi3Config as _Phi3Config\n        from .phi3 import FlaxPhi3ForCausalLM as _FlaxPhi3ForCausalLM\n        return (\n            _Phi3Config,\n            _FlaxPhi3ForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n\n    elif model_type == \"arctic\":\n        from .arctic import ArcticConfig as _ArcticConfig\n        from .arctic import FlaxArcticForCausalLM as _FlaxArcticForCausalLM\n        return (\n            _ArcticConfig,\n            _FlaxArcticForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"openelm\":\n        from .openelm import OpenELMConfig as _OpenELMConfig\n        from .openelm import FlaxOpenELMForCausalLM as _FlaxOpenELMForCausalLM\n        return (\n            _OpenELMConfig,\n            _FlaxOpenELMForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"token_embeddings\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    elif model_type == \"deepseek_v2\":\n        from .deepseek_v2 import DeepseekV2Config as _DeepseekV2Config\n        from .deepseek_v2 import FlaxDeepseekV2ForCausalLM as _FlaxDeepseekV2ForCausalLM\n\n        return (\n            _DeepseekV2Config,\n            _FlaxDeepseekV2ForCausalLM,\n            functools.partial(\n                huggingface_to_easydel,\n                embedding_layer_names=[\"embed_tokens\"],\n                rnn_based_or_rwkv=False\n            )\n        )\n    raise EasyDeLRuntimeError(f'Model Type ({model_type}) is not supported or is not found')\n</code></pre>"},{"location":"generated-modules-auto_easydel_model/#src.python.easydel.modules.auto_easydel_model.is_flatten","title":"<code>is_flatten(pytree)</code>","text":"<p>The is_flatten function checks if the pytree is flattened.     If it is, then the first key in the dictionary will be a tuple of (mpl, mpl_id).     Otherwise, it will be an integer representing mpl_id.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>dict</code> <p>dict: Pass the pytree to the function</p> required <p>Returns:</p> Type Description <p>True if the pytree is a flattened tree, and false otherwise</p> Source code in <code>src/python/easydel/modules/auto_easydel_model.py</code> <pre><code>def is_flatten(pytree: dict):\n    \"\"\"The is_flatten function checks if the pytree is flattened.\n        If it is, then the first key in the dictionary will be a tuple of (mpl, mpl_id).\n        Otherwise, it will be an integer representing mpl_id.\n\n    Args:\n        pytree: dict: Pass the pytree to the function\n\n    Returns:\n        True if the pytree is a flattened tree, and false otherwise\n    \"\"\"\n    mpl = [k for k in pytree.keys()][0]\n    return True if isinstance(mpl, tuple) else False\n</code></pre>"},{"location":"generated-modules-cohere-cohere_configuration/","title":"modules.cohere.cohere_configuration","text":""},{"location":"generated-modules-cohere-cohere_configuration/#src.python.easydel.modules.cohere.cohere_configuration.CohereConfig","title":"<code>CohereConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/cohere/cohere_configuration.py</code> <pre><code>class CohereConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"cohere\"\n\n    def __init__(\n            self,\n            vocab_size=256000,\n            hidden_size=8192,\n            intermediate_size=22528,\n            logit_scale=0.0625,\n            num_hidden_layers=40,\n            num_attention_heads=64,\n            num_key_value_heads=None,\n            hidden_act=\"silu\",\n            max_position_embeddings=8192,\n            initializer_range=0.02,\n            layer_norm_eps=1e-5,\n            use_cache=True,\n            pad_token_id=0,\n            bos_token_id=5,\n            eos_token_id=255001,\n            tie_word_embeddings=True,\n            rope_theta=10000.0,\n            attention_bias=False,\n            attention_dropout=0.0,\n            use_qk_norm: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.logit_scale = logit_scale\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.use_qk_norm = use_qk_norm\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"linear_1/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"post_attn_norm/kernel\", PartitionSpec(None)),\n            (\"pre_attn_norm/kernel\", PartitionSpec(None)),\n            (\"pre_moe_norm/kernel\", PartitionSpec(None)),\n            (\"post_moe_norm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"linear_1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"post_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"pre_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"pre_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"post_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"model/norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            tie_word_embeddings: bool: Tie the word embeddings to the\n                decoder\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n        \"\"\"\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout'\n</code></pre>"},{"location":"generated-modules-cohere-cohere_configuration/#src.python.easydel.modules.cohere.cohere_configuration.CohereConfig.add_jax_args","title":"<code>add_jax_args(tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', bits=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> Source code in <code>src/python/easydel/modules/cohere/cohere_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        tie_word_embeddings: bool: Tie the word embeddings to the\n            decoder\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n    \"\"\"\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-cohere-cohere_configuration/#src.python.easydel.modules.cohere.cohere_configuration.CohereConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/cohere/cohere_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"linear_1/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"post_attn_norm/kernel\", PartitionSpec(None)),\n        (\"pre_attn_norm/kernel\", PartitionSpec(None)),\n        (\"pre_moe_norm/kernel\", PartitionSpec(None)),\n        (\"post_moe_norm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"linear_1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"post_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"pre_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"pre_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"post_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"model/norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/","title":"modules.cohere.modelling_cohere_flax","text":""},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereAttention","title":"<code>FlaxCohereAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereAttention(BaseJAXAttentionModule):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n\n        if config.use_qk_norm:\n            self.q_norm = RMSNorm(\n                dim=(self.head_dim, self.config.num_attention_heads),\n                eps=config.layer_norm_eps,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                do_t=True\n            )\n            self.k_norm = RMSNorm(\n                dim=(self.head_dim, self.config.num_key_value_heads,),\n                eps=config.layer_norm_eps,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                do_t=True\n            )\n        self.q_proj = nn.Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = nn.Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = nn.Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = nn.Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxCohereEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        (\n            query_states,\n            key_states,\n            value_states\n        ) = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        if self.config.use_qk_norm:\n            query_states = self.q_norm(query_states)\n            key_states = self.k_norm(key_states)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    (\n        query_states,\n        key_states,\n        value_states\n    ) = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    if self.config.use_qk_norm:\n        query_states = self.q_norm(query_states)\n        key_states = self.k_norm(key_states)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereBlock","title":"<code>FlaxCohereBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereBlock(nn.Module):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxCohereAttention\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = re_mat(\n                FlaxCohereAttention, static_argnums=(1, 3, 4, 6, 7, 8),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxCohereMLP\n\n        if self.config.gradient_checkpointing != \"\":\n            mlp_block = re_mat(\n                FlaxCohereMLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.layer_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        Args:\n            self: Refer to the class instance itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency information\n            attention_mask: chex.Array: Mask out the attention weights\n                for padding tokens\n            position_ids: chex.Array: Determine the position of each\n                token in the sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Control whether the dropout is applied\n                or not\n            init_cache: bool: Initialize the cache in the attention\n                layer\n            output_attentions: bool: Return the attention weights\n            fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n\n        Returns:\n            A tuple of two items\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        attn_outputs = self.self_attn(\n            hidden_states,\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n\n        feed_forward_input = hidden_states\n\n        if self.config.use_scan_mlp:\n            feed_forward_hidden_states = block_wise_ffn(\n                self.mlp,\n                feed_forward_input,\n                self.config.scan_mlp_chunk_size,\n                deterministic,\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = attn_output + feed_forward_hidden_states + residual\n\n        return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereBlock.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <p>:param : Control the dropout in the self attention layer</p> <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask: Optional[jnp.ndarray] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    Args:\n        self: Refer to the class instance itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency information\n        attention_mask: chex.Array: Mask out the attention weights\n            for padding tokens\n        position_ids: chex.Array: Determine the position of each\n            token in the sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Control whether the dropout is applied\n            or not\n        init_cache: bool: Initialize the cache in the attention\n            layer\n        output_attentions: bool: Return the attention weights\n        fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n\n    Returns:\n        A tuple of two items\n    \"\"\"\n    residual = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n    attn_outputs = self.self_attn(\n        hidden_states,\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n\n    feed_forward_input = hidden_states\n\n    if self.config.use_scan_mlp:\n        feed_forward_hidden_states = block_wise_ffn(\n            self.mlp,\n            feed_forward_input,\n            self.config.scan_mlp_chunk_size,\n            deterministic,\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n    hidden_states = attn_output + feed_forward_hidden_states + residual\n\n    return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereBlockCollection","title":"<code>FlaxCohereBlockCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereBlockCollection(nn.Module):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxCohereBlock(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model\n         in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module,\n        and return all outputs that are computed by this module.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the input tensor to the\n                encoder\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency of each token\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Determine whether the model is in\n                training or evaluation mode\n            init_cache: bool: Initialize the cache for each layer\n            output_attentions: bool: Determine whether to output the\n                attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states of each layer\n            return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n\n        Returns:\n            A tuple of 3 values\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereBlockCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model  in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <p>:param : Determine whether to use the forgetful causal mask</p> <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model\n     in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module,\n    and return all outputs that are computed by this module.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the input tensor to the\n            encoder\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency of each token\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Determine whether the model is in\n            training or evaluation mode\n        init_cache: bool: Initialize the cache for each layer\n        output_attentions: bool: Determine whether to output the\n            attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states of each layer\n        return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n\n    Returns:\n        A tuple of 3 values\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += (layer_outputs[1],)\n\n    outputs = (hidden_states, all_hidden_states, all_attentions)\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereForCausalLM","title":"<code>FlaxCohereForCausalLM</code>","text":"<p>               Bases: <code>FlaxCoherePreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereForCausalLM(FlaxCoherePreTrainedModel):\n    module_class = FlaxCohereForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>:param self: Access variables that belong to the class :param input_ids: Pass in the input tokens :param max_length: Set the length of the sequence to be generated :param attention_mask: Optional[chex.Array]: Mask the attention weights :return: A dictionary of the past_key_values, attention_mask and position ids</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"\n    The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    :param self: Access variables that belong to the class\n    :param input_ids: Pass in the input tokens\n    :param max_length: Set the length of the sequence to be generated\n    :param attention_mask: Optional[chex.Array]: Mask the attention weights\n    :return: A dictionary of the past_key_values, attention_mask and position ids\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereForCausalLMModule","title":"<code>FlaxCohereForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereForCausalLMModule(nn.Module):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxCohereModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = nn.Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.logit_scale = self.config.logit_scale\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass the input token ids to the model\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the input sequence\n            deterministic: bool: Control whether the model is trained or\n                not\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states\n            return_dict: bool: Return a dictionary of the outputs or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the word that we want to predict\n            None]]: Pass in the extra embedding\n\n        Returns:\n            The logits and the hidden states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = (lm_logits * self.logit_scale).astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass the input token ids to the model\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the input sequence\n        deterministic: bool: Control whether the model is trained or\n            not\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states\n        return_dict: bool: Return a dictionary of the outputs or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the word that we want to predict\n        None]]: Pass in the extra embedding\n\n    Returns:\n        The logits and the hidden states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = (lm_logits * self.logit_scale).astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereMLP","title":"<code>FlaxCohereMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereMLP(nn.Module):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.gate_proj = nn.Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = nn.Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = nn.Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        return x\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n    return x\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereModule","title":"<code>FlaxCohereModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCohereModule(nn.Module):\n    config: CohereConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.layers = FlaxCohereBlockCollection(self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                                                precision=self.precision)\n        self.norm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.layer_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        config = self.config\n        self.causal_mask = flax.linen.make_causal_mask(\n            jnp.ones(\n                (1, getattr(self.config, \"c_max_position_embeddings\", self.config.max_position_embeddings)),\n                dtype=\"bool\"\n            ), dtype=\"bool\"\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if getattr(config, \"rope_scaling\", None) is not None:\n            scaling_type = config.rope_scaling[\"type\"]\n            scaling_factor = config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=(\n                getattr(self.config, \"freq_max_position_embeddings\", self.config.max_position_embeddings)\n            ),\n            dim=config.hidden_size // config.num_attention_heads,\n            base=config.rope_theta,\n            **initial_rope_kwargs\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input token ids\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in a sequence\n            deterministic: bool: Control whether dropout is applied or\n                not\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attentions or not\n            output_hidden_states: bool: Determine whether to return\n                hidden states\n            return_dict: bool: Return a dictionary of the output or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the\n            None]]: Pass in the extra embedding\n\n        Returns:\n            A tuple of:\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length, _ = inputs_embeds.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n        inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n\n        outputs = self.layers(\n            hidden_states=inputs_embeds,\n            freq_cis=self.freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCohereModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids and returns the output of the model. The call function also has optional arguments that can be used to control the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when calling a Flax model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input token ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether dropout is applied or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attentions or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the output or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>A tuple of:</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n    and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n    the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n    calling a Flax model.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input token ids\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in a sequence\n        deterministic: bool: Control whether dropout is applied or\n            not\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attentions or not\n        output_hidden_states: bool: Determine whether to return\n            hidden states\n        return_dict: bool: Return a dictionary of the output or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the\n        None]]: Pass in the extra embedding\n\n    Returns:\n        A tuple of:\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n    batch_size, sequence_length, _ = inputs_embeds.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n    inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n\n    outputs = self.layers(\n        hidden_states=inputs_embeds,\n        freq_cis=self.freq_cis,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        causal_mask=self.causal_mask,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(v for v in outputs if v is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCoherePreTrainedModel","title":"<code>FlaxCoherePreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>class FlaxCoherePreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = CohereConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: CohereConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: CohereConfig: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the input\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of layers in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape,\n                         seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input\n            position_ids: chex.Array: Create the positional embeddings\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass in the past key values from a\n                previous call to __call__\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all layers\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCoherePreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, extra_embedding=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input\n        position_ids: chex.Array: Create the positional embeddings\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass in the past key values from a\n            previous call to __call__\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all layers\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    if self.config.bits is not None:\n        rngs['params'] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCoherePreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>CohereConfig</code> <p>CohereConfig: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of layers in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def __init__(\n        self,\n        config: CohereConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: CohereConfig: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the input\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of layers in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape,\n                     seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCoherePreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-cohere-modelling_cohere_flax/#src.python.easydel.modules.cohere.modelling_cohere_flax.FlaxCoherePreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/cohere/modelling_cohere_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-dbrx-dbrx_configuration/","title":"modules.dbrx.dbrx_configuration","text":"<p>Dbrx configuration.</p>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/","title":"modules.dbrx.modelling_dbrx_flax","text":""},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.DbrxPreTrainedModel","title":"<code>DbrxPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>class DbrxPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class: DbrxConfig = DbrxConfig\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    def __init__(\n            self,\n            config: DbrxConfig,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: FrozenDict = None\n    ) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n\n        self.config.initialization_of_moe = True\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        self.config.initialization_of_moe = False\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            # output_router_logits: Optional[bool] = None\n            output_router_logits,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.DbrxPreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n        # attention_mask: Optional[chex.Array] = None\n        jnp.array(attention_mask, dtype=\"i4\"),\n        # position_ids: Optional[chex.Array] = None\n        jnp.array(position_ids, dtype=\"i4\"),\n        None,  # inputs_embeds: Optional[chex.Array] = None\n        output_attentions,  # output_attentions: Optional[bool] = None\n        # output_hidden_states: Optional[bool] = None\n        output_hidden_states,\n        # output_router_logits: Optional[bool] = None\n        output_router_logits,\n        False,  # init_cache: bool = False\n        not train,  # deterministic: bool = True\n        return_dict,  # return_dict: bool = True\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.DbrxPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: FrozenDict = None\n) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n\n    self.config.initialization_of_moe = True\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n        input_shape,\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False\n        )\n    random_params = module_init_outputs[\"params\"]\n\n    self.config.initialization_of_moe = False\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.FlaxDbrxAttention","title":"<code>FlaxDbrxAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>class FlaxDbrxAttention(BaseJAXAttentionModule):\n    config: DbrxConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.num_attention_heads = self.config.n_heads\n        self.num_key_value_heads = self.config.attn_config.kv_n_heads\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.d_model // self.config.n_heads\n        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.num_attention_heads == self.config.attn_config.kv_n_heads\n        self.Wqkv = Linear(\n            self.hidden_size + 2 * self.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.out_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxDbrxEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.num_attention_heads,\n            attention_dropout=self.config.attn_config.attn_pdrop,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n        self.resid_dropout = flax.linen.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        qkv_states = self.Wqkv(hidden_states)\n        if self.config.attn_config.clip_qkv is not None:\n            qkv_states = qkv_states.clip(\n                min=-self.config.attn_config.clip_qkv,\n                max=self.config.attn_config.clip_qkv\n            )\n\n        query_size = self.hidden_size\n        key_size = self.num_key_value_heads * self.head_dim\n\n        query_states, key_value_states = jnp.split(qkv_states, [query_size], axis=2)\n        key_states, value_states = jnp.split(key_value_states, [key_size], axis=2)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.num_attention_heads} KVH : {self.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attn_config.attn_pdrop &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.out_proj(attn_output)\n\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output, attentions.attention_weights\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.FlaxDbrxAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    qkv_states = self.Wqkv(hidden_states)\n    if self.config.attn_config.clip_qkv is not None:\n        qkv_states = qkv_states.clip(\n            min=-self.config.attn_config.clip_qkv,\n            max=self.config.attn_config.clip_qkv\n        )\n\n    query_size = self.hidden_size\n    key_size = self.num_key_value_heads * self.head_dim\n\n    query_states, key_value_states = jnp.split(qkv_states, [query_size], axis=2)\n    key_states, value_states = jnp.split(key_value_states, [key_size], axis=2)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.num_attention_heads} KVH : {self.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attn_config.attn_pdrop &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.out_proj(attn_output)\n\n    attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n    return attn_output, attentions.attention_weights\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.FlaxDbrxAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.FlaxDbrxForCausalLM","title":"<code>FlaxDbrxForCausalLM</code>","text":"<p>               Bases: <code>DbrxPreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>class FlaxDbrxForCausalLM(DbrxPreTrainedModel):\n    module_class = FlaxDbrxForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-dbrx-modelling_dbrx_flax/#src.python.easydel.modules.dbrx.modelling_dbrx_flax.FlaxDbrxForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>:param self: Access variables that belong to the class :param input_ids: Pass in the input tokens :param max_length: Set the length of the sequence to be generated :param attention_mask: Optional[chex.Array]: Mask the attention weights :return: A dictionary of the past_key_values, attention_mask and position ids</p> Source code in <code>src/python/easydel/modules/dbrx/modelling_dbrx_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"\n    The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    :param self: Access variables that belong to the class\n    :param input_ids: Pass in the input tokens\n    :param max_length: Set the length of the sequence to be generated\n    :param attention_mask: Optional[chex.Array]: Mask the attention weights\n    :return: A dictionary of the past_key_values, attention_mask and position ids\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-deepseek_v2-deepseek_configuration/","title":"modules.deepseek_v2.deepseek_configuration","text":""},{"location":"generated-modules-deepseek_v2-deepseek_configuration/#src.python.easydel.modules.deepseek_v2.deepseek_configuration.DeepseekV2Config","title":"<code>DeepseekV2Config</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/deepseek_v2/deepseek_configuration.py</code> <pre><code>class DeepseekV2Config(EasyDeLPretrainedConfig):\n    model_type: str = \"deepseek_v2\"\n\n    def __init__(\n            self,\n            vocab_size=102400,\n            hidden_size=4096,\n            intermediate_size=11008,\n            moe_intermediate_size=1407,\n            num_hidden_layers=30,\n            num_attention_heads=32,\n            num_key_value_heads=32,\n            n_shared_experts=None,\n            n_routed_experts=None,\n            ep_size=1,\n            routed_scaling_factor=1.0,\n            kv_lora_rank=512,\n            q_lora_rank=1536,\n            qk_rope_head_dim=64,\n            v_head_dim=128,\n            qk_nope_head_dim=128,\n            topk_method='gready',\n            n_group=None,\n            topk_group=None,\n            num_experts_per_tok=None,\n            moe_layer_freq=1,\n            first_k_dense_replace=0,\n            norm_topk_prob=False,\n            scoring_func='softmax',\n            aux_loss_alpha=0.001,\n            seq_aux=True,\n            hidden_act=\"silu\",\n            max_position_embeddings=2048,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=100000,\n            eos_token_id=100001,\n            pretraining_tp=1,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            attention_bias=False,\n            attention_dropout=0.0,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            **kwargs,\n    ):\n        warnings.warn(\n            \"`DeepseekV2` is still in beta mode.\",\n            UserWarning\n        )\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.moe_intermediate_size = moe_intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.n_shared_experts = n_shared_experts\n        self.n_routed_experts = n_routed_experts\n        self.ep_size = ep_size\n        self.routed_scaling_factor = routed_scaling_factor\n        self.kv_lora_rank = kv_lora_rank\n        self.q_lora_rank = q_lora_rank\n        self.qk_rope_head_dim = qk_rope_head_dim\n        self.v_head_dim = v_head_dim\n        self.qk_nope_head_dim = qk_nope_head_dim\n        self.topk_method = topk_method\n        self.n_group = n_group\n        self.topk_group = topk_group\n        self.num_experts_per_tok = num_experts_per_tok\n        self.moe_layer_freq = moe_layer_freq\n        self.first_k_dense_replace = first_k_dense_replace\n        self.norm_topk_prob = norm_topk_prob\n        self.scoring_func = scoring_func\n        self.aux_loss_alpha = aux_loss_alpha\n        self.seq_aux = seq_aux\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.gradient_checkpointing = gradient_checkpointing\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to use\n                the fully_sharded_data_parallel partitioning scheme or\n                not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the model:\n\n        Args:\n            self: Bind the attributes and methods of a class to an\n                instance of that class\n            gradient_checkpointing: str: Determine whether to use\n                gradient checkpointing\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or not\n            scan_mlp_chunk_size: int: Chunk the input to the mlp\n            number_rep_kv: int: Control the number of times that the key\n                and value vectors are repeated\n            bits: Optional[int]: Specify the number of bits to use for\n                quantization\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            attention_bias: bool: when ever to use attention_bias\n            initialization_of_moe: bool: initialization of moe needs to\n                disable some dynamic part's this boolean variable will\n                turn them off.\n            rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n                rope\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        self.attention_dropout = attention_dropout\n        self.attention_bias = attention_bias\n        self.rope_scaling = rope_scaling\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.initialization_of_moe = initialization_of_moe\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-deepseek_v2-deepseek_configuration/#src.python.easydel.modules.deepseek_v2.deepseek_configuration.DeepseekV2Config.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, bits=None, rope_scaling=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the attributes and methods of a class to an instance of that class</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Determine whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Chunk the input to the mlp</p> <code>1024</code> <code>number_rep_kv</code> <p>int: Control the number of times that the key and value vectors are repeated</p> required <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits to use for quantization</p> <code>None</code> <code>attention_dropout</code> <p>float: Set the dropout rate for the attention layer</p> required <code>attention_bias</code> <p>bool: when ever to use attention_bias</p> required <code>initialization_of_moe</code> <p>bool: initialization of moe needs to disable some dynamic part's this boolean variable will turn them off.</p> required <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str, Union[str, float]]: rope_scaling for rope</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/deepseek_v2/deepseek_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the model:\n\n    Args:\n        self: Bind the attributes and methods of a class to an\n            instance of that class\n        gradient_checkpointing: str: Determine whether to use\n            gradient checkpointing\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or not\n        scan_mlp_chunk_size: int: Chunk the input to the mlp\n        number_rep_kv: int: Control the number of times that the key\n            and value vectors are repeated\n        bits: Optional[int]: Specify the number of bits to use for\n            quantization\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        attention_bias: bool: when ever to use attention_bias\n        initialization_of_moe: bool: initialization of moe needs to\n            disable some dynamic part's this boolean variable will\n            turn them off.\n        rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n            rope\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    self.attention_dropout = attention_dropout\n    self.attention_bias = attention_bias\n    self.rope_scaling = rope_scaling\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n    self.initialization_of_moe = initialization_of_moe\n</code></pre>"},{"location":"generated-modules-deepseek_v2-deepseek_configuration/#src.python.easydel.modules.deepseek_v2.deepseek_configuration.DeepseekV2Config.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to use the fully_sharded_data_parallel partitioning scheme or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/deepseek_v2/deepseek_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to use\n            the fully_sharded_data_parallel partitioning scheme or\n            not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/","title":"modules.deepseek_v2.modeling_deepseek_flax","text":""},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.DeepseekV2PreTrainedModel","title":"<code>DeepseekV2PreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>class DeepseekV2PreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class: DeepseekV2Config = DeepseekV2Config\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    def __init__(\n            self,\n            config: DeepseekV2Config,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: FrozenDict = None\n    ) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n\n        self.config.initialization_of_moe = True\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        self.config.initialization_of_moe = False\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            # output_router_logits: Optional[bool] = None\n            output_router_logits,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.DeepseekV2PreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n        # attention_mask: Optional[chex.Array] = None\n        jnp.array(attention_mask, dtype=\"i4\"),\n        # position_ids: Optional[chex.Array] = None\n        jnp.array(position_ids, dtype=\"i4\"),\n        None,  # inputs_embeds: Optional[chex.Array] = None\n        output_attentions,  # output_attentions: Optional[bool] = None\n        # output_hidden_states: Optional[bool] = None\n        output_hidden_states,\n        # output_router_logits: Optional[bool] = None\n        output_router_logits,\n        False,  # init_cache: bool = False\n        not train,  # deterministic: bool = True\n        return_dict,  # return_dict: bool = True\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.DeepseekV2PreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: FrozenDict = None\n) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n\n    self.config.initialization_of_moe = True\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n        input_shape,\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False\n        )\n    random_params = module_init_outputs[\"params\"]\n\n    self.config.initialization_of_moe = False\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2ForCausalLM","title":"<code>FlaxDeepseekV2ForCausalLM</code>","text":"<p>               Bases: <code>DeepseekV2PreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>class FlaxDeepseekV2ForCausalLM(DeepseekV2PreTrainedModel):\n    module_class = FlaxDeepseekV2ForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2ForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2ForCausalLMModule","title":"<code>FlaxDeepseekV2ForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>class FlaxDeepseekV2ForCausalLMModule(nn.Module):\n    config: DeepseekV2Config\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.model = FlaxDeepseekV2Module(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.lm_head = nn.Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            use_bias=False,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n        and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n        as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n        the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n        output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Determine whether to use dropout in the\n                model\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of the outputs or\n                just the logits\n        :param : Determine whether to return the logits or not\n\n        Returns:\n            A tuple of (lm_logits, hidden_states, attentions)\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            inputs_embeds=inputs_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2ForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax module. It defines how the model will be called, and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask as inputs (these are defined in init). We also have some optional arguments that can be passed to the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings), output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout in the model</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or just the logits</p> <code>True</code> <p>:param : Determine whether to return the logits or not</p> <p>Returns:</p> Type Description <p>A tuple of (lm_logits, hidden_states, attentions)</p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n    and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n    as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n    the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n    output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Determine whether to use dropout in the\n            model\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of the outputs or\n            just the logits\n    :param : Determine whether to return the logits or not\n\n    Returns:\n        A tuple of (lm_logits, hidden_states, attentions)\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        deterministic=deterministic,\n        inputs_embeds=inputs_embeds,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.transformer.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    # lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2Module","title":"<code>FlaxDeepseekV2Module</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>class FlaxDeepseekV2Module(nn.Module):\n    config: DeepseekV2Config\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxDeepseekV2DecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = DeepseekV2RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        initial_rope_kwargs = {}\n        method = None\n        if self.config.rope_scaling is not None:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            method = scaling_type\n            if scaling_type != \"yarn\":\n                initial_rope_kwargs = dict(scaling_factor=self.config.rope_scaling[\"factor\"])\n            else:\n                initial_rope_kwargs = {\n                    key: self.config.rope_scaling[key]\n                    for key in [\n                        \"original_max_position_embeddings\",\n                        \"beta_fast\",\n                        \"beta_slow\",\n                        \"mscale\",\n                        \"mscale_all_dim\",\n                    ]\n                    if key in self.config.rope_scaling\n                }\n                initial_rope_kwargs[\"scaling_factor\"] = self.config.rope_scaling[\"factor\"]\n        self.freq_cis = init_deepseek_rotary_embedding(\n            dim=self.config.hidden_size // self.config.num_attention_heads,\n            max_position_embeddings=(\n                getattr(\n                    self.config,\n                    \"freq_max_position_embeddings\",\n                    self.config.max_position_embeddings\n                )\n            ),\n            base=self.config.rope_theta,\n            method=method,  # type:ignore\n            kwargs=initial_rope_kwargs\n        )\n        self.causal_mask = flax.linen.make_causal_mask(\n            jnp.ones(\n                (\n                    1,\n                    getattr(\n                        self.config,\n                        \"c_max_position_embeddings\",\n                        self.config.max_position_embeddings\n                    )\n                ),\n                dtype=\"bool\"\n            ),\n            dtype=\"bool\"\n        )\n\n    def __call__(\n            self,\n            input_ids: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ) -&gt; typing.Union[Tuple[chex.Array, ...], FlaxBaseModelOutput]:\n        \"\"\"The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input ids\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain tokens\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            deterministic: bool: Determine whether to use dropout or not\n            inputs_embeds: chex.Array: Pass in the embedding of the\n                input_ids\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            output_hidden_states: bool: Return all hidden states or just\n                the last one\n            return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n\n        Returns:\n            A tuple of the hidden states, all hidden states, and\n            attentions\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_states=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-deepseek_v2-modeling_deepseek_flax/#src.python.easydel.modules.deepseek_v2.modeling_deepseek_flax.FlaxDeepseekV2Module.__call__","title":"<code>__call__(input_ids=None, attention_mask=None, position_ids=None, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids as inputs to the model. The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Optional[Array]</code> <p>chex.Array: Pass in the input ids</p> <code>None</code> <code>attention_mask</code> <code>Optional[Array]</code> <p>chex.Array: Mask out the attention weights for certain tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>chex.Array: Determine the position of each token in a sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embedding of the input_ids</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return all hidden states or just the last one</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <p>:param : Determine whether the model is in training mode or not</p> <p>Returns:</p> Type Description <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>A tuple of the hidden states, all hidden states, and</p> <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>attentions</p> Source code in <code>src/python/easydel/modules/deepseek_v2/modeling_deepseek_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: Optional[chex.Array] = None,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n) -&gt; typing.Union[Tuple[chex.Array, ...], FlaxBaseModelOutput]:\n    \"\"\"The __call__ function is the main function of a Flax model.\n    It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n    The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input ids\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain tokens\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        deterministic: bool: Determine whether to use dropout or not\n        inputs_embeds: chex.Array: Pass in the embedding of the\n            input_ids\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        output_hidden_states: bool: Return all hidden states or just\n            the last one\n        return_dict: bool: Return a dictionary of the outputs or not\n    :param : Determine whether the model is in training mode or not\n\n    Returns:\n        A tuple of the hidden states, all hidden states, and\n        attentions\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n    if attention_mask.ndim == 2:\n        b, s = attention_mask.shape\n        attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n    outputs = self.layers(\n        hidden_states=inputs_embeds,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        freq_cis=self.freq_cis,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        deterministic=deterministic,\n        causal_mask=self.causal_mask\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(value for value in outputs if value is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/","title":"modules.easydel_modelling_utils","text":""},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel","title":"<code>EasyDeLFlaxPretrainedModel</code>","text":"<p>               Bases: <code>FlaxPreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>class EasyDeLFlaxPretrainedModel(FlaxPreTrainedModel):\n    def __init__(\n            self,\n            config: Optional[PretrainedConfig] = None,\n            module: Optional[flax.linen.Module] = None,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,  # Ignored\n            precision: Optional[Union[jax.lax.Precision, str]] = None,  # Ignored\n            _do_init: bool = True,\n    ):\n        assert config is not None, \"`config` must be provided.`\"\n        assert module is not None, \"`module` must be provided.`\"\n        super().__init__(\n            config=config,\n            module=module,\n            input_shape=input_shape,\n            seed=seed,\n            dtype=dtype,\n            _do_init=_do_init\n        )\n\n    def get_input_embeddings(self):\n        \"\"\"The get_input_embeddings function returns the embedding layer of the model.\n\n        Args:\n            self: Refer to the current object\n\n        Returns:\n            The embedding layer of the model\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_input_embeddings(self, value):\n        \"\"\"The set_input_embeddings function is used to set the embedding module of the model.\n\n        Args:\n            self: Represent the instance of the class\n            value: Set the embeddings of the model\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_output_embeddings(self):\n        \"\"\"The get_output_embeddings function returns the output embeddings of a model.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            The output embeddings of the model\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_output_embeddings(self, new_embeddings):\n        \"\"\"The set_output_embeddings function is used to set the output embeddings of a model.\n        This function can be used to change the output embedding layer of a pretrained model in order to finetune it\n        to some downstream task. Changing this layer has an effect only if the model has already been fine-tuned on some\n        task (e.g., for classification). If you are training your own language models, you should call this function before\n        you start training.\n\n        Args:\n            self: Represent the instance of the class\n            new_embeddings: Set the embeddings of the output layer\n\n        Returns:\n            A new embedding layer\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_decoder(self, decoder):\n        \"\"\"The set_decoder function is used to set the decoder for a given encoder.\n\n        Args:\n            self: Refer to the object itself\n            decoder: Set the decoder for a given encoder\n\n        Returns:\n            A decoder\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_decoder(self):\n        \"\"\"The get_decoder function is used to create a decoder object.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A decoder object\n        \"\"\"\n        raise NotImplementedError()\n\n    def init_cache(self, batch_size: int, max_length: int):\n        raise NotImplementedError(\"init_cache is not Implemented Yet!\")\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            vision_mask: Optional[chex.Array] = None,\n            **kwargs\n    ):\n        raise NotImplementedError(\"Not Implemented Yet\")\n\n    def __repr__(self):\n\n        \"\"\"The __repr__ function is used to generate a string representation of an object.\n        This function should return a string that can be parsed by the Python interpreter\n        to recreate the object. The __repr__ function is called when you use print() on an\n        object, or when you type its name in the REPL.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            A string representation of the object\n        \"\"\"\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                try:\n                    repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n                except TypeError:\n                    pass\n        return string + \")\"\n\n    def __str__(self):\n\n        \"\"\"The __str__ function is called when you use the print function or when str() is used.\n        It should return a string representation of the object.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            The object's string representation\n        \"\"\"\n        return self.__repr__()\n\n    @property\n    def config(self) -&gt; EasyDeLPretrainedConfig:\n        return self._config  # type:ignore\n\n    def to_easydel_state(\n            self,\n            params: flax.core.FrozenDict,\n    ):\n        \"\"\"\n        Convert the Model to EasyDeLState\n        \"\"\"\n        return EasyDeLState.load(\n            apply_fn=self.__call__,\n            params=params,\n            opt_state=None,\n            module_config=self.config,\n        )\n\n    def to_pytorch(\n            self,\n            params: flax.core.FrozenDict,\n            base_hf_auto_class=AutoModelForCausalLM,\n            easystate_to_huggingface_model_kwargs: Optional[dict] = None\n    ):\n        \"\"\"\n        Return the Huggingface / Pytorch implementation of the model with same weights  (if model is available in HF)\n        \"\"\"\n\n        from ..transform.easydel_transform import easystate_to_huggingface_model\n        state = self.to_easydel_state(params=params)\n        if easystate_to_huggingface_model_kwargs is None:\n            easystate_to_huggingface_model_kwargs = {}\n\n        model_config = state.module_config\n        if model_config is None:\n            model_config = state.module.config_class\n        # model_type = model_config.model_type\n        model_class = base_hf_auto_class._model_mapping[type(model_config)]  # noqa\n        hf_model = easystate_to_huggingface_model(\n            state=state,\n            base_huggingface_module=model_class,\n            config=model_config,\n\n            **easystate_to_huggingface_model_kwargs\n        )\n        return hf_model\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is used to generate a string representation of an object. This function should return a string that can be parsed by the Python interpreter to recreate the object. The repr function is called when you use print() on an object, or when you type its name in the REPL.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>A string representation of the object</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"The __repr__ function is used to generate a string representation of an object.\n    This function should return a string that can be parsed by the Python interpreter\n    to recreate the object. The __repr__ function is called when you use print() on an\n    object, or when you type its name in the REPL.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        A string representation of the object\n    \"\"\"\n    string = f\"{self.__class__.__name__}(\\n\"\n    for k, v in self.__dict__.items():\n        if not k.startswith(\"_\"):\n            try:\n                repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n            except TypeError:\n                pass\n    return string + \")\"\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you use the print function or when str() is used. It should return a string representation of the object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>The object's string representation</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def __str__(self):\n\n    \"\"\"The __str__ function is called when you use the print function or when str() is used.\n    It should return a string representation of the object.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        The object's string representation\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.get_decoder","title":"<code>get_decoder()</code>","text":"<p>The get_decoder function is used to create a decoder object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A decoder object</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_decoder(self):\n    \"\"\"The get_decoder function is used to create a decoder object.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A decoder object\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.get_input_embeddings","title":"<code>get_input_embeddings()</code>","text":"<p>The get_input_embeddings function returns the embedding layer of the model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <p>Returns:</p> Type Description <p>The embedding layer of the model</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_input_embeddings(self):\n    \"\"\"The get_input_embeddings function returns the embedding layer of the model.\n\n    Args:\n        self: Refer to the current object\n\n    Returns:\n        The embedding layer of the model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.get_output_embeddings","title":"<code>get_output_embeddings()</code>","text":"<p>The get_output_embeddings function returns the output embeddings of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The output embeddings of the model</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_output_embeddings(self):\n    \"\"\"The get_output_embeddings function returns the output embeddings of a model.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        The output embeddings of the model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = jax.lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.set_decoder","title":"<code>set_decoder(decoder)</code>","text":"<p>The set_decoder function is used to set the decoder for a given encoder.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>decoder</code> <p>Set the decoder for a given encoder</p> required <p>Returns:</p> Type Description <p>A decoder</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def set_decoder(self, decoder):\n    \"\"\"The set_decoder function is used to set the decoder for a given encoder.\n\n    Args:\n        self: Refer to the object itself\n        decoder: Set the decoder for a given encoder\n\n    Returns:\n        A decoder\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.set_input_embeddings","title":"<code>set_input_embeddings(value)</code>","text":"<p>The set_input_embeddings function is used to set the embedding module of the model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>value</code> <p>Set the embeddings of the model</p> required Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def set_input_embeddings(self, value):\n    \"\"\"The set_input_embeddings function is used to set the embedding module of the model.\n\n    Args:\n        self: Represent the instance of the class\n        value: Set the embeddings of the model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.set_output_embeddings","title":"<code>set_output_embeddings(new_embeddings)</code>","text":"<p>The set_output_embeddings function is used to set the output embeddings of a model. This function can be used to change the output embedding layer of a pretrained model in order to finetune it to some downstream task. Changing this layer has an effect only if the model has already been fine-tuned on some task (e.g., for classification). If you are training your own language models, you should call this function before you start training.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>new_embeddings</code> <p>Set the embeddings of the output layer</p> required <p>Returns:</p> Type Description <p>A new embedding layer</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def set_output_embeddings(self, new_embeddings):\n    \"\"\"The set_output_embeddings function is used to set the output embeddings of a model.\n    This function can be used to change the output embedding layer of a pretrained model in order to finetune it\n    to some downstream task. Changing this layer has an effect only if the model has already been fine-tuned on some\n    task (e.g., for classification). If you are training your own language models, you should call this function before\n    you start training.\n\n    Args:\n        self: Represent the instance of the class\n        new_embeddings: Set the embeddings of the output layer\n\n    Returns:\n        A new embedding layer\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.to_easydel_state","title":"<code>to_easydel_state(params)</code>","text":"<p>Convert the Model to EasyDeLState</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def to_easydel_state(\n        self,\n        params: flax.core.FrozenDict,\n):\n    \"\"\"\n    Convert the Model to EasyDeLState\n    \"\"\"\n    return EasyDeLState.load(\n        apply_fn=self.__call__,\n        params=params,\n        opt_state=None,\n        module_config=self.config,\n    )\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel.to_pytorch","title":"<code>to_pytorch(params, base_hf_auto_class=AutoModelForCausalLM, easystate_to_huggingface_model_kwargs=None)</code>","text":"<p>Return the Huggingface / Pytorch implementation of the model with same weights  (if model is available in HF)</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def to_pytorch(\n        self,\n        params: flax.core.FrozenDict,\n        base_hf_auto_class=AutoModelForCausalLM,\n        easystate_to_huggingface_model_kwargs: Optional[dict] = None\n):\n    \"\"\"\n    Return the Huggingface / Pytorch implementation of the model with same weights  (if model is available in HF)\n    \"\"\"\n\n    from ..transform.easydel_transform import easystate_to_huggingface_model\n    state = self.to_easydel_state(params=params)\n    if easystate_to_huggingface_model_kwargs is None:\n        easystate_to_huggingface_model_kwargs = {}\n\n    model_config = state.module_config\n    if model_config is None:\n        model_config = state.module.config_class\n    # model_type = model_config.model_type\n    model_class = base_hf_auto_class._model_mapping[type(model_config)]  # noqa\n    hf_model = easystate_to_huggingface_model(\n        state=state,\n        base_huggingface_module=model_class,\n        config=model_config,\n\n        **easystate_to_huggingface_model_kwargs\n    )\n    return hf_model\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig","title":"<code>EasyDeLPretrainedConfig</code>","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>It initializes all the attributes of an object, and it's called when you create a new instance of that class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the number of dimensions for each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Set the names of the axes</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>attn_mechanism</code> <code>AVAILABLE_ATTENTION_MECHANISMS</code> <p>Literal[\"vanilla\", \"flash\", \"splash\", \"ring\"]: attention mechanism to use</p> <code>'sharded_vanilla'</code> <code>block_k</code> <code>int</code> <p>int: block size of key_states</p> <code>128</code> <code>block_q</code> <code>int</code> <p>int: block size of query_states</p> <code>128</code> <code>block_b</code> <code>int</code> <p>int: block size of bias</p> <code>1</code> <code>block_q_major_dkv</code> <code>int | None</code> <p>int: block size of block_q_major_dkv</p> <code>None</code> <code>block_k_major_dkv</code> <code>int | None</code> <p>int: block size of block_k_major_dkv</p> <code>None</code> <code>block_k_dkv</code> <code>int | None</code> <p>int: block size of block_k_dkv</p> <code>None</code> <code>block_q_dkv</code> <code>int | None</code> <p>int: block size of block_q_dkv</p> <code>None</code> <code>block_k_major_dq</code> <code>int | None</code> <p>int: block size of block_k_major_dq</p> <code>None</code> <code>block_k_dq</code> <code>int | None</code> <p>int: block size of block_k_dq</p> <code>None</code> <code>block_q_dq</code> <code>int | None</code> <p>int: block size of block_q_dq</p> <code>None</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>key_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec(('dp', 'fsdp'), None, None, None)</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp', 'tp', None)</code> <code>shard_attention_computation</code> <code>bool</code> <p>bool: whenever to shard qkv b for attention</p> <code>True</code> <code>use_sharding_constraint</code> <code>bool</code> <p>bool: whether to use sharding constraint for the arrays</p> <code>False</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use scan_mlp or not</p> <code>True</code> <code>backend</code> <code>Optional[None]</code> <p>Optional[None]: Specify the backend to use</p> <code>default_backend()</code> <code>flash_attention_backward_pass_impl</code> <code>Literal['triton', 'xla']</code> <p>Literal[\"triton\", \"xla\"]: Specify the backward pass kernel for flash attention</p> <code>'triton'</code> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>class EasyDeLPretrainedConfig(PretrainedConfig):\n    \"\"\"It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n\n    Args:\n        self: Refer to the instance of the class\n        axis_dims: Sequence[int]: Specify the number of dimensions for\n            each axis\n        axis_names: Sequence[str]: Set the names of the axes\n        attn_mechanism: Literal[\"vanilla\", \"flash\", \"splash\", \"ring\"]:\n            attention mechanism to use\n        block_k: int: block size of key_states\n        block_q: int: block size of query_states\n        block_b: int: block size of bias\n        block_q_major_dkv: int: block size of block_q_major_dkv\n        block_k_major_dkv: int: block size of block_k_major_dkv\n        block_k_dkv: int: block size of block_k_dkv\n        block_q_dkv: int: block size of block_q_dkv\n        block_k_major_dq: int: block size of block_k_major_dq\n        block_k_dq: int: block size of block_k_dq\n        block_q_dq: int: block size of block_q_dq\n        query_partition_spec: PartitionSpec: Specify the partitioning of\n            the query tensor\n        key_partition_spec: PartitionSpec: Partition the key matrix\n        value_partition_spec: PartitionSpec: Specify the partitioning of\n            the value tensor\n        bias_partition_spec: PartitionSpec: Specify the Attention Bias\n            partition spec\n        attention_partition_spec: PartitionSpec: Specify the\n            partitioning of the attention weights\n        shard_attention_computation: bool: whenever to shard qkv b for\n            attention\n        use_sharding_constraint: bool: whether to use sharding\n            constraint for the arrays\n        use_scan_mlp: bool: Determine whether to use scan_mlp or not\n        backend: Optional[None]: Specify the backend to use\n        flash_attention_backward_pass_impl: Literal[\"triton\", \"xla\"]:\n            Specify the backward pass kernel for flash attention\n    \"\"\"\n\n    def __init__(\n            self,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            attn_mechanism: AVAILABLE_ATTENTION_MECHANISMS = \"sharded_vanilla\",\n            block_k: int = 128,\n            block_q: int = 128,\n            block_b: int = 1,\n            block_k_major: int = 128,\n            block_q_major_dkv: int | None = None,\n            block_k_major_dkv: int | None = None,\n            block_k_dkv: int | None = None,\n            block_q_dkv: int | None = None,\n            block_k_major_dq: int | None = None,\n            block_k_dq: int | None = None,\n            block_q_dq: int | None = None,\n            query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_query_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            key_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            generation_attention_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            shard_attention_computation: bool = True,\n            use_sharded_kv_caching: bool = True,\n            use_sharding_constraint: bool = False,\n            backend: Optional[None] = jax.default_backend(),\n            easy_method: Literal[\"train\", \"serve\", \"convert\"] = EasyMethod.TRAIN,\n            bits: Optional[int] = None,\n            scan_ring_attention: bool = True,\n            scan_attention_layers: bool = False,\n            use_scan_mlp: bool = True,\n            scan_mlp_chunk_size: int = 1024,\n            attention_axis_name: str = \"sp\",\n            quantize_kv_cache: bool = False,\n            flash_attention_backward_pass_impl: Literal[\"triton\", \"xla\"] = \"triton\",\n            **kwargs\n    ):\n        self.query_partition_spec = query_partition_spec\n        self.generation_query_partition_spec = generation_query_partition_spec\n        self.key_partition_spec = key_partition_spec\n        self.value_partition_spec = value_partition_spec\n        self.bias_partition_spec = bias_partition_spec\n        self.generation_bias_partition_spec = generation_bias_partition_spec\n        self.attention_partition_spec = attention_partition_spec\n        self.generation_attention_partition_spec = generation_attention_partition_spec\n        self.shard_attention_computation = shard_attention_computation\n        self.axis_dims = axis_dims\n        self.axis_names = axis_names\n        self.backend = backend if backend is not None else \"\"\n        self.easy_method = easy_method\n        self.attn_mechanism = attn_mechanism\n        self.block_b = block_b\n        self.block_k = block_k\n        self.block_q = block_q\n        self.block_k_major = block_k_major\n        self.block_q_major_dkv = block_q_major_dkv or block_q\n        self.block_k_major_dkv = block_k_major_dkv or block_k\n        self.block_k_dkv = block_k_dkv or block_k\n        self.block_q_dkv = block_q_dkv or block_q\n        self.block_k_major_dq = block_k_major_dq or block_k\n        self.block_k_dq = block_k_dq or block_k\n        self.block_q_dq = block_q_dq or block_q\n        self.bits = bits\n        self.scan_attention_layers = scan_attention_layers\n        self.scan_ring_attention = scan_ring_attention\n        self.use_sharded_kv_caching = use_sharded_kv_caching\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.use_sharding_constraint = use_sharding_constraint\n        self.attention_axis_name = attention_axis_name\n        self.quantize_kv_cache = quantize_kv_cache\n        self.flash_attention_backward_pass_impl = flash_attention_backward_pass_impl\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def create_mesh(\n            axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"), backend=\"\"\n    ):\n        \"\"\"The create_mesh function creates a mesh object that can be used to shard arrays.\n\n        Args:\n            axis_dims: Sequence[int]: Specify the dimensions of the mesh\n            axis_names: Sequence[str]: Name the axes of the mesh\n            backend: Specify the backend to use\n\n        Returns:\n            A mesh object\n        \"\"\"\n        array_devices = jax.numpy.ones(\n            (len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n        if isinstance(axis_dims, str):\n            axis_dims = eval(axis_dims)\n            warnings.warn(\n                \"axis_dims argument is not a Sequence of int and it's an string. \"\n                \"(backbone Warning in EasyDeLModuleConfig)\\n\"\n                f\"\\tchanged to {axis_dims}\"\n            )\n        if isinstance(axis_names, str):\n            axis_names = eval(axis_names)\n            warnings.warn(\n                \"axis_names argument is not a Sequence of strings and it's an string class. \"\n                \"(backbone Warning in EasyDeLModuleConfig)\\n\"\n                f\"\\tchanged to {axis_names}\"\n            )\n        resh = array_devices.reshape(axis_dims).shape\n\n        return Mesh(\n            create_device_mesh(resh), axis_names\n        )\n\n    def jax_mesh(self) -&gt; Mesh:\n        \"\"\"The jax_mesh function is a helper function that creates a Mesh object from the\n        axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.\n        The backend attribute is also used if it exists.\n\n        Args:\n            self: Refer to the object itself\n\n        Returns:\n            A jaxMesh\n        \"\"\"\n        return self.create_mesh(\n            axis_dims=[v for k, v in self.axis_dims.items()] if isinstance(\n                self.axis_dims,\n                dict\n            ) else self.axis_dims,\n            axis_names=[v for k, v in self.axis_names.items()] if isinstance(\n                self.axis_names,\n                dict\n            ) else self.axis_names,\n            backend=(self.backend if self.backend is not None else \"\") if hasattr(\n                self, 'backend') else \"\"\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n\n        \"\"\"The get_partition_rules function is used to specify how the parameters of a model are partitioned across devices.\n\n        Args:\n            self: Access the attributes of the class\n            fully_sharded_data_parallel: bool: Determine whether the\n                model is fully sharded or not\n\n        Returns:\n            A tuple of tuples\n        \"\"\"\n        if not fully_sharded_data_parallel:\n            raise NotImplementedError()\n        else:\n            return (\n                ('.*', PartitionSpec((\"fsdp\", \"sp\"), ),),\n            )\n\n    def get_axis_dims(self) -&gt; Sequence[int]:\n        \"\"\"The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            The dimensions of the axes\n        \"\"\"\n        return self.axis_dims\n\n    def get_axis_names(self) -&gt; Sequence[str]:\n        \"\"\"The get_axis_names function returns a list of the names of the axes.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A list of the names of all axes\n        \"\"\"\n        return self.axis_names\n\n    def get_backend(self) -&gt; str:\n        \"\"\"The get_backend function returns the backend that is currently being used.\n        If no backend has been set, it will return the default JAX backend.\n\n        Args:\n            self: Bind the method to an object\n\n        Returns:\n            The backend platform\n        \"\"\"\n        return self.backend if not self.backend == \"\" else jax.lib.xla_bridge.get_backend().platform\n\n    def add_basic_configurations(\n            self,\n            axis_dims: Sequence[int] = ...,\n            axis_names: Sequence[str] = ...,\n            attn_mechanism: AVAILABLE_ATTENTION_MECHANISMS = ...,\n            block_k: int = ...,\n            block_q: int = ...,\n            block_b: int = ...,\n            block_k_major: int = ...,\n            block_q_major_dkv: int | None = ...,\n            block_k_major_dkv: int | None = ...,\n            block_k_dkv: int | None = ...,\n            block_q_dkv: int | None = ...,\n            block_k_major_dq: int | None = ...,\n            block_k_dq: int | None = ...,\n            block_q_dq: int | None = ...,\n            query_partition_spec: PartitionSpec = ...,\n            generation_query_partition_spec: PartitionSpec = ...,\n            key_partition_spec: PartitionSpec = ...,\n            value_partition_spec: PartitionSpec = ...,\n            bias_partition_spec: PartitionSpec = ...,\n            attention_partition_spec: PartitionSpec = ...,\n            generation_bias_partition_spec: PartitionSpec = ...,\n            generation_attention_partition_spec: PartitionSpec = ...,\n            shard_attention_computation: bool = ...,\n            use_sharded_kv_caching: bool = ...,\n            backend: Optional[None] = ...,\n            easy_method: Literal[\"train\", \"serve\", \"convert\"] = ...,\n            bits: Optional[int] = ...,\n            scan_ring_attention: bool = ...,\n            scan_attention_layers: bool = ...,\n            use_sharding_constraint: bool = ...,\n            use_scan_mlp: bool = ...,\n            scan_mlp_chunk_size: int = ...,\n            attention_axis_name: str = ...,\n            quantize_kv_cache: bool = ...,\n            flash_attention_backward_pass_impl: Literal[\"triton\", \"xla\"] = ...\n    ):\n        \"\"\"It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n\n        Args:\n            self: Refer to the instance of the class\n            axis_dims: Sequence[int]: Specify the number of dimensions\n                for each axis\n            axis_names: Sequence[str]: Set the names of the axes\n            attn_mechanism: Literal[\"vanilla\", \"flash\", \"splash\"]:\n                attention mechanism to use\n            block_k: int: block size of key_states\n            block_q: int: block size of query_states\n            block_b: int: block size of bias\n            block_k_major: int: block size if key major\n            block_q_major_dkv: int: block size of block_q_major_dkv\n            block_k_major_dkv: int: block size of block_k_major_dkv\n            block_k_dkv: int: block size of block_k_dkv\n            block_q_dkv: int: block size of block_q_dkv\n            block_k_major_dq: int: block size of block_k_major_dq\n            block_k_dq: int: block size of block_k_dq\n            block_q_dq: int: block size of block_q_dq\n            query_partition_spec: PartitionSpec: Specify the\n                partitioning of the query tensor\n            key_partition_spec: PartitionSpec: Partition the key matrix\n            value_partition_spec: PartitionSpec: Specify the\n                partitioning of the value tensor\n            bias_partition_spec: PartitionSpec: Specify the Attention\n                Bias partition spec\n            attention_partition_spec: PartitionSpec: Specify the\n                partitioning of the attention weights\n            generation_attention_partition_spec: : PartitionSpec:\n                Specify the partitioning of the attention weights\n            generation_bias_partition_spec: : PartitionSpec: Specify the\n                partitioning of the Attention Bias partition spec in\n                generation process\n            generation_query_partition_spec: : PartitionSpec: Specify\n                the partitioning of the query tensor\n            shard_attention_computation: bool: whenever to use shard_map\n                for attention\n            use_sharded_kv_caching: bool: whenever to use shard_map and\n                sharding for key and value\n            backend: Optional[None]: Specify the backend to use\n            easy_method: Literal[\"train\", \"serve\", \"convert\"]: easydel\n                Quantization Method to be applied for\n            bits: Optional[int]: Model bits for quantization\n            use_sharding_constraint: bool: whether to use sharding\n                constraint for the arrays\n            scan_ring_attention: bool: Whether to use can for ring\n                attention\n            scan_attention_layers: bool: Whether to use can for\n                attention layers\n            use_scan_mlp: bool: Determine whether to use scan_mlp or not\n            scan_mlp_chunk_size: int: Size of chunks in scan MLP.\n            attention_axis_name: str: Name of the attention axis name\n            quantize_kv_cache: bool: Whether to quantize Key/Value in\n                attention for generation process.\n            flash_attention_backward_pass_impl: Literal[\"triton\",\n                \"xla\"]: Specify the backward pass kernel for flash\n                attention\n        in generation process\n        in generation process\n        \"\"\"\n        set_attrs_smartly(self, \"axis_dims\", (1, -1, 1, 1), axis_dims)\n        set_attrs_smartly(self, \"axis_names\", (\"dp\", \"fsdp\", \"tp\", \"sp\"), axis_names)\n\n        set_attrs_smartly(self, \"block_q\", 1024, block_q)\n        set_attrs_smartly(self, \"block_k\", 1024, block_k)\n        set_attrs_smartly(self, \"block_b\", 1024, block_b)\n\n        set_attrs_smartly(\n            self,\n            \"query_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            query_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"generation_query_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            generation_query_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"generation_bias_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            generation_bias_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"key_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            key_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"value_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            value_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"bias_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            bias_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"attention_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            attention_partition_spec\n        )\n        set_attrs_smartly(\n            self,\n            \"generation_attention_partition_spec\",\n            PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            generation_attention_partition_spec\n        )\n        set_attrs_smartly(self, \"use_sharding_constraint\", False, use_sharding_constraint)\n        set_attrs_smartly(self, \"backend\", jax.default_backend(), backend)\n        set_attrs_smartly(self, \"shard_attention_computation\", True, shard_attention_computation)\n        set_attrs_smartly(self, \"use_sharded_kv_caching\", True, use_sharded_kv_caching)\n        set_attrs_smartly(self, \"attn_mechanism\", \"sharded_vanilla\", attn_mechanism)\n\n        set_attrs_smartly(self, \"block_k_dkv\", block_k_dkv or self.block_k, block_k_dkv)\n        set_attrs_smartly(self, \"block_q_dkv\", block_q_dkv or self.block_q, block_q_dkv)\n\n        set_attrs_smartly(self, \"block_q_major_dkv\", block_q_major_dkv or self.block_q, block_q_major_dkv)\n        set_attrs_smartly(self, \"block_k_major_dkv\", block_k_major_dkv or self.block_k, block_k_major_dkv)\n\n        set_attrs_smartly(self, \"block_k_major\", block_k_major or self.block_k, block_k_major)\n        set_attrs_smartly(self, \"block_k_major_dq\", block_k_major_dq or self.block_k, block_k_major_dq)\n\n        set_attrs_smartly(self, \"block_k_dq\", block_k_dq or self.block_k, block_k_dq)\n        set_attrs_smartly(self, \"block_q_dq\", block_q_dq or self.block_q, block_q_dq)\n\n        set_attrs_smartly(self, \"easy_method\", EasyMethod.TRAIN, easy_method)\n        set_attrs_smartly(self, \"bits\", None, bits)\n        set_attrs_smartly(self, \"scan_attention_layers\", True, scan_attention_layers)\n        set_attrs_smartly(self, \"scan_ring_attention\", True, scan_ring_attention)\n        set_attrs_smartly(self, \"use_scan_mlp\", True, use_scan_mlp)\n        set_attrs_smartly(self, \"scan_mlp_chunk_size\", 1024, scan_mlp_chunk_size)\n        set_attrs_smartly(self, \"attention_axis_name\", \"sp\", attention_axis_name)\n        set_attrs_smartly(self, \"quantize_kv_cache\", False, quantize_kv_cache)\n        set_attrs_smartly(self, \"flash_attention_backward_pass_impl\", \"triton\", flash_attention_backward_pass_impl)\n\n    def __repr__(self):\n\n        \"\"\"The __repr__ function is used to generate a string representation of an object.\n        This function should return a string that can be parsed by the Python interpreter\n        to recreate the object. The __repr__ function is called when you use print() on an\n        object, or when you type its name in the REPL.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            A string representation of the object\n        \"\"\"\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                try:\n                    repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n                except TypeError:\n                    pass\n        return string + \")\"\n\n    def add_jax_args(self, **kwargs):\n        for k, v in kwargs.items():\n            set_attrs_smartly(self, \"k\", v, v)\n\n    def __str__(self):\n\n        \"\"\"The __str__ function is called when you use the print function or when str() is used.\n        It should return a string representation of the object.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            The object's string representation\n        \"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is used to generate a string representation of an object. This function should return a string that can be parsed by the Python interpreter to recreate the object. The repr function is called when you use print() on an object, or when you type its name in the REPL.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>A string representation of the object</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"The __repr__ function is used to generate a string representation of an object.\n    This function should return a string that can be parsed by the Python interpreter\n    to recreate the object. The __repr__ function is called when you use print() on an\n    object, or when you type its name in the REPL.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        A string representation of the object\n    \"\"\"\n    string = f\"{self.__class__.__name__}(\\n\"\n    for k, v in self.__dict__.items():\n        if not k.startswith(\"_\"):\n            try:\n                repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n            except TypeError:\n                pass\n    return string + \")\"\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you use the print function or when str() is used. It should return a string representation of the object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>The object's string representation</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def __str__(self):\n\n    \"\"\"The __str__ function is called when you use the print function or when str() is used.\n    It should return a string representation of the object.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        The object's string representation\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.add_basic_configurations","title":"<code>add_basic_configurations(axis_dims=..., axis_names=..., attn_mechanism=..., block_k=..., block_q=..., block_b=..., block_k_major=..., block_q_major_dkv=..., block_k_major_dkv=..., block_k_dkv=..., block_q_dkv=..., block_k_major_dq=..., block_k_dq=..., block_q_dq=..., query_partition_spec=..., generation_query_partition_spec=..., key_partition_spec=..., value_partition_spec=..., bias_partition_spec=..., attention_partition_spec=..., generation_bias_partition_spec=..., generation_attention_partition_spec=..., shard_attention_computation=..., use_sharded_kv_caching=..., backend=..., easy_method=..., bits=..., scan_ring_attention=..., scan_attention_layers=..., use_sharding_constraint=..., use_scan_mlp=..., scan_mlp_chunk_size=..., attention_axis_name=..., quantize_kv_cache=..., flash_attention_backward_pass_impl=...)</code>","text":"<p>It initializes all the attributes of an object, and it's called when you create a new instance of that class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the number of dimensions for each axis</p> <code>...</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Set the names of the axes</p> <code>...</code> <code>attn_mechanism</code> <code>AVAILABLE_ATTENTION_MECHANISMS</code> <p>Literal[\"vanilla\", \"flash\", \"splash\"]: attention mechanism to use</p> <code>...</code> <code>block_k</code> <code>int</code> <p>int: block size of key_states</p> <code>...</code> <code>block_q</code> <code>int</code> <p>int: block size of query_states</p> <code>...</code> <code>block_b</code> <code>int</code> <p>int: block size of bias</p> <code>...</code> <code>block_k_major</code> <code>int</code> <p>int: block size if key major</p> <code>...</code> <code>block_q_major_dkv</code> <code>int | None</code> <p>int: block size of block_q_major_dkv</p> <code>...</code> <code>block_k_major_dkv</code> <code>int | None</code> <p>int: block size of block_k_major_dkv</p> <code>...</code> <code>block_k_dkv</code> <code>int | None</code> <p>int: block size of block_k_dkv</p> <code>...</code> <code>block_q_dkv</code> <code>int | None</code> <p>int: block size of block_q_dkv</p> <code>...</code> <code>block_k_major_dq</code> <code>int | None</code> <p>int: block size of block_k_major_dq</p> <code>...</code> <code>block_k_dq</code> <code>int | None</code> <p>int: block size of block_k_dq</p> <code>...</code> <code>block_q_dq</code> <code>int | None</code> <p>int: block size of block_q_dq</p> <code>...</code> <code>query_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the query tensor</p> <code>...</code> <code>key_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Partition the key matrix</p> <code>...</code> <code>value_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the value tensor</p> <code>...</code> <code>bias_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the Attention Bias partition spec</p> <code>...</code> <code>attention_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Specify the partitioning of the attention weights</p> <code>...</code> <code>generation_attention_partition_spec</code> <code>PartitionSpec</code> <p>: PartitionSpec: Specify the partitioning of the attention weights</p> <code>...</code> <code>generation_bias_partition_spec</code> <code>PartitionSpec</code> <p>: PartitionSpec: Specify the partitioning of the Attention Bias partition spec in generation process</p> <code>...</code> <code>generation_query_partition_spec</code> <code>PartitionSpec</code> <p>: PartitionSpec: Specify the partitioning of the query tensor</p> <code>...</code> <code>shard_attention_computation</code> <code>bool</code> <p>bool: whenever to use shard_map for attention</p> <code>...</code> <code>use_sharded_kv_caching</code> <code>bool</code> <p>bool: whenever to use shard_map and sharding for key and value</p> <code>...</code> <code>backend</code> <code>Optional[None]</code> <p>Optional[None]: Specify the backend to use</p> <code>...</code> <code>easy_method</code> <code>Literal['train', 'serve', 'convert']</code> <p>Literal[\"train\", \"serve\", \"convert\"]: easydel Quantization Method to be applied for</p> <code>...</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Model bits for quantization</p> <code>...</code> <code>use_sharding_constraint</code> <code>bool</code> <p>bool: whether to use sharding constraint for the arrays</p> <code>...</code> <code>scan_ring_attention</code> <code>bool</code> <p>bool: Whether to use can for ring attention</p> <code>...</code> <code>scan_attention_layers</code> <code>bool</code> <p>bool: Whether to use can for attention layers</p> <code>...</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use scan_mlp or not</p> <code>...</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Size of chunks in scan MLP.</p> <code>...</code> <code>attention_axis_name</code> <code>str</code> <p>str: Name of the attention axis name</p> <code>...</code> <code>quantize_kv_cache</code> <code>bool</code> <p>bool: Whether to quantize Key/Value in attention for generation process.</p> <code>...</code> <code>flash_attention_backward_pass_impl</code> <code>Literal['triton', 'xla']</code> <p>Literal[\"triton\", \"xla\"]: Specify the backward pass kernel for flash attention</p> <code>...</code> <p>in generation process in generation process</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def add_basic_configurations(\n        self,\n        axis_dims: Sequence[int] = ...,\n        axis_names: Sequence[str] = ...,\n        attn_mechanism: AVAILABLE_ATTENTION_MECHANISMS = ...,\n        block_k: int = ...,\n        block_q: int = ...,\n        block_b: int = ...,\n        block_k_major: int = ...,\n        block_q_major_dkv: int | None = ...,\n        block_k_major_dkv: int | None = ...,\n        block_k_dkv: int | None = ...,\n        block_q_dkv: int | None = ...,\n        block_k_major_dq: int | None = ...,\n        block_k_dq: int | None = ...,\n        block_q_dq: int | None = ...,\n        query_partition_spec: PartitionSpec = ...,\n        generation_query_partition_spec: PartitionSpec = ...,\n        key_partition_spec: PartitionSpec = ...,\n        value_partition_spec: PartitionSpec = ...,\n        bias_partition_spec: PartitionSpec = ...,\n        attention_partition_spec: PartitionSpec = ...,\n        generation_bias_partition_spec: PartitionSpec = ...,\n        generation_attention_partition_spec: PartitionSpec = ...,\n        shard_attention_computation: bool = ...,\n        use_sharded_kv_caching: bool = ...,\n        backend: Optional[None] = ...,\n        easy_method: Literal[\"train\", \"serve\", \"convert\"] = ...,\n        bits: Optional[int] = ...,\n        scan_ring_attention: bool = ...,\n        scan_attention_layers: bool = ...,\n        use_sharding_constraint: bool = ...,\n        use_scan_mlp: bool = ...,\n        scan_mlp_chunk_size: int = ...,\n        attention_axis_name: str = ...,\n        quantize_kv_cache: bool = ...,\n        flash_attention_backward_pass_impl: Literal[\"triton\", \"xla\"] = ...\n):\n    \"\"\"It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n\n    Args:\n        self: Refer to the instance of the class\n        axis_dims: Sequence[int]: Specify the number of dimensions\n            for each axis\n        axis_names: Sequence[str]: Set the names of the axes\n        attn_mechanism: Literal[\"vanilla\", \"flash\", \"splash\"]:\n            attention mechanism to use\n        block_k: int: block size of key_states\n        block_q: int: block size of query_states\n        block_b: int: block size of bias\n        block_k_major: int: block size if key major\n        block_q_major_dkv: int: block size of block_q_major_dkv\n        block_k_major_dkv: int: block size of block_k_major_dkv\n        block_k_dkv: int: block size of block_k_dkv\n        block_q_dkv: int: block size of block_q_dkv\n        block_k_major_dq: int: block size of block_k_major_dq\n        block_k_dq: int: block size of block_k_dq\n        block_q_dq: int: block size of block_q_dq\n        query_partition_spec: PartitionSpec: Specify the\n            partitioning of the query tensor\n        key_partition_spec: PartitionSpec: Partition the key matrix\n        value_partition_spec: PartitionSpec: Specify the\n            partitioning of the value tensor\n        bias_partition_spec: PartitionSpec: Specify the Attention\n            Bias partition spec\n        attention_partition_spec: PartitionSpec: Specify the\n            partitioning of the attention weights\n        generation_attention_partition_spec: : PartitionSpec:\n            Specify the partitioning of the attention weights\n        generation_bias_partition_spec: : PartitionSpec: Specify the\n            partitioning of the Attention Bias partition spec in\n            generation process\n        generation_query_partition_spec: : PartitionSpec: Specify\n            the partitioning of the query tensor\n        shard_attention_computation: bool: whenever to use shard_map\n            for attention\n        use_sharded_kv_caching: bool: whenever to use shard_map and\n            sharding for key and value\n        backend: Optional[None]: Specify the backend to use\n        easy_method: Literal[\"train\", \"serve\", \"convert\"]: easydel\n            Quantization Method to be applied for\n        bits: Optional[int]: Model bits for quantization\n        use_sharding_constraint: bool: whether to use sharding\n            constraint for the arrays\n        scan_ring_attention: bool: Whether to use can for ring\n            attention\n        scan_attention_layers: bool: Whether to use can for\n            attention layers\n        use_scan_mlp: bool: Determine whether to use scan_mlp or not\n        scan_mlp_chunk_size: int: Size of chunks in scan MLP.\n        attention_axis_name: str: Name of the attention axis name\n        quantize_kv_cache: bool: Whether to quantize Key/Value in\n            attention for generation process.\n        flash_attention_backward_pass_impl: Literal[\"triton\",\n            \"xla\"]: Specify the backward pass kernel for flash\n            attention\n    in generation process\n    in generation process\n    \"\"\"\n    set_attrs_smartly(self, \"axis_dims\", (1, -1, 1, 1), axis_dims)\n    set_attrs_smartly(self, \"axis_names\", (\"dp\", \"fsdp\", \"tp\", \"sp\"), axis_names)\n\n    set_attrs_smartly(self, \"block_q\", 1024, block_q)\n    set_attrs_smartly(self, \"block_k\", 1024, block_k)\n    set_attrs_smartly(self, \"block_b\", 1024, block_b)\n\n    set_attrs_smartly(\n        self,\n        \"query_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        query_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"generation_query_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n        generation_query_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"generation_bias_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        generation_bias_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"key_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        key_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"value_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        value_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"bias_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n        bias_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"attention_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n        attention_partition_spec\n    )\n    set_attrs_smartly(\n        self,\n        \"generation_attention_partition_spec\",\n        PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n        generation_attention_partition_spec\n    )\n    set_attrs_smartly(self, \"use_sharding_constraint\", False, use_sharding_constraint)\n    set_attrs_smartly(self, \"backend\", jax.default_backend(), backend)\n    set_attrs_smartly(self, \"shard_attention_computation\", True, shard_attention_computation)\n    set_attrs_smartly(self, \"use_sharded_kv_caching\", True, use_sharded_kv_caching)\n    set_attrs_smartly(self, \"attn_mechanism\", \"sharded_vanilla\", attn_mechanism)\n\n    set_attrs_smartly(self, \"block_k_dkv\", block_k_dkv or self.block_k, block_k_dkv)\n    set_attrs_smartly(self, \"block_q_dkv\", block_q_dkv or self.block_q, block_q_dkv)\n\n    set_attrs_smartly(self, \"block_q_major_dkv\", block_q_major_dkv or self.block_q, block_q_major_dkv)\n    set_attrs_smartly(self, \"block_k_major_dkv\", block_k_major_dkv or self.block_k, block_k_major_dkv)\n\n    set_attrs_smartly(self, \"block_k_major\", block_k_major or self.block_k, block_k_major)\n    set_attrs_smartly(self, \"block_k_major_dq\", block_k_major_dq or self.block_k, block_k_major_dq)\n\n    set_attrs_smartly(self, \"block_k_dq\", block_k_dq or self.block_k, block_k_dq)\n    set_attrs_smartly(self, \"block_q_dq\", block_q_dq or self.block_q, block_q_dq)\n\n    set_attrs_smartly(self, \"easy_method\", EasyMethod.TRAIN, easy_method)\n    set_attrs_smartly(self, \"bits\", None, bits)\n    set_attrs_smartly(self, \"scan_attention_layers\", True, scan_attention_layers)\n    set_attrs_smartly(self, \"scan_ring_attention\", True, scan_ring_attention)\n    set_attrs_smartly(self, \"use_scan_mlp\", True, use_scan_mlp)\n    set_attrs_smartly(self, \"scan_mlp_chunk_size\", 1024, scan_mlp_chunk_size)\n    set_attrs_smartly(self, \"attention_axis_name\", \"sp\", attention_axis_name)\n    set_attrs_smartly(self, \"quantize_kv_cache\", False, quantize_kv_cache)\n    set_attrs_smartly(self, \"flash_attention_backward_pass_impl\", \"triton\", flash_attention_backward_pass_impl)\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.create_mesh","title":"<code>create_mesh(axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'sp'), backend='')</code>  <code>staticmethod</code>","text":"<p>The create_mesh function creates a mesh object that can be used to shard arrays.</p> <p>Parameters:</p> Name Type Description Default <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the mesh</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>backend</code> <p>Specify the backend to use</p> <code>''</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>@staticmethod\ndef create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"), backend=\"\"\n):\n    \"\"\"The create_mesh function creates a mesh object that can be used to shard arrays.\n\n    Args:\n        axis_dims: Sequence[int]: Specify the dimensions of the mesh\n        axis_names: Sequence[str]: Name the axes of the mesh\n        backend: Specify the backend to use\n\n    Returns:\n        A mesh object\n    \"\"\"\n    array_devices = jax.numpy.ones(\n        (len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n    if isinstance(axis_dims, str):\n        axis_dims = eval(axis_dims)\n        warnings.warn(\n            \"axis_dims argument is not a Sequence of int and it's an string. \"\n            \"(backbone Warning in EasyDeLModuleConfig)\\n\"\n            f\"\\tchanged to {axis_dims}\"\n        )\n    if isinstance(axis_names, str):\n        axis_names = eval(axis_names)\n        warnings.warn(\n            \"axis_names argument is not a Sequence of strings and it's an string class. \"\n            \"(backbone Warning in EasyDeLModuleConfig)\\n\"\n            f\"\\tchanged to {axis_names}\"\n        )\n    resh = array_devices.reshape(axis_dims).shape\n\n    return Mesh(\n        create_device_mesh(resh), axis_names\n    )\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.get_axis_dims","title":"<code>get_axis_dims()</code>","text":"<p>The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>Sequence[int]</code> <p>The dimensions of the axes</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_axis_dims(self) -&gt; Sequence[int]:\n    \"\"\"The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        The dimensions of the axes\n    \"\"\"\n    return self.axis_dims\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.get_axis_names","title":"<code>get_axis_names()</code>","text":"<p>The get_axis_names function returns a list of the names of the axes.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>A list of the names of all axes</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_axis_names(self) -&gt; Sequence[str]:\n    \"\"\"The get_axis_names function returns a list of the names of the axes.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A list of the names of all axes\n    \"\"\"\n    return self.axis_names\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.get_backend","title":"<code>get_backend()</code>","text":"<p>The get_backend function returns the backend that is currently being used. If no backend has been set, it will return the default JAX backend.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the method to an object</p> required <p>Returns:</p> Type Description <code>str</code> <p>The backend platform</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_backend(self) -&gt; str:\n    \"\"\"The get_backend function returns the backend that is currently being used.\n    If no backend has been set, it will return the default JAX backend.\n\n    Args:\n        self: Bind the method to an object\n\n    Returns:\n        The backend platform\n    \"\"\"\n    return self.backend if not self.backend == \"\" else jax.lib.xla_bridge.get_backend().platform\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to specify how the parameters of a model are partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes of the class</p> required <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether the model is fully sharded or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of tuples</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n\n    \"\"\"The get_partition_rules function is used to specify how the parameters of a model are partitioned across devices.\n\n    Args:\n        self: Access the attributes of the class\n        fully_sharded_data_parallel: bool: Determine whether the\n            model is fully sharded or not\n\n    Returns:\n        A tuple of tuples\n    \"\"\"\n    if not fully_sharded_data_parallel:\n        raise NotImplementedError()\n    else:\n        return (\n            ('.*', PartitionSpec((\"fsdp\", \"sp\"), ),),\n        )\n</code></pre>"},{"location":"generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLPretrainedConfig.jax_mesh","title":"<code>jax_mesh()</code>","text":"<p>The jax_mesh function is a helper function that creates a Mesh object from the axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively. The backend attribute is also used if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <code>Mesh</code> <p>A jaxMesh</p> Source code in <code>src/python/easydel/modules/easydel_modelling_utils.py</code> <pre><code>def jax_mesh(self) -&gt; Mesh:\n    \"\"\"The jax_mesh function is a helper function that creates a Mesh object from the\n    axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.\n    The backend attribute is also used if it exists.\n\n    Args:\n        self: Refer to the object itself\n\n    Returns:\n        A jaxMesh\n    \"\"\"\n    return self.create_mesh(\n        axis_dims=[v for k, v in self.axis_dims.items()] if isinstance(\n            self.axis_dims,\n            dict\n        ) else self.axis_dims,\n        axis_names=[v for k, v in self.axis_names.items()] if isinstance(\n            self.axis_names,\n            dict\n        ) else self.axis_names,\n        backend=(self.backend if self.backend is not None else \"\") if hasattr(\n            self, 'backend') else \"\"\n    )\n</code></pre>"},{"location":"generated-modules-falcon-falcon_configuration/","title":"modules.falcon.falcon_configuration","text":""},{"location":"generated-modules-falcon-modelling_falcon_flax/","title":"modules.falcon.modelling_falcon_flax","text":""},{"location":"generated-modules-falcon-modelling_falcon_flax/#src.python.easydel.modules.falcon.modelling_falcon_flax.FlaxFalconPretrainedModel","title":"<code>FlaxFalconPretrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>class FlaxFalconPretrainedModel(EasyDeLFlaxPretrainedModel):\n    module_class: nn.Module = None\n    config_class = FalconConfig\n\n    def __init__(self, config,\n                 _do_init=False,\n                 dtype: jnp.dtype = jnp.float32,\n                 param_dtype: jnp.dtype = jnp.float32,\n                 input_shape: Tuple = (1, 1),\n                 precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n                 ):\n        module = self.module_class(config=config, dtype=dtype, param_dtype=param_dtype, precision=precision)\n        super().__init__(_do_init=_do_init, module=module, config=config, dtype=dtype, input_shape=input_shape)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                return_dict=False\n            )\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            past_key_values: Optional[nn.Module] = None,\n            output_attentions: bool = False,\n            train: bool = True,\n            return_dict: Optional[bool] = True,\n            params: FrozenDict = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        input_ids = jnp.asarray(input_ids, dtype=jnp.int32)\n        inputs = {'params': params or self.params} if add_params_field else params or self.params\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(input_ids.shape[1])[None, :],\n                                            (input_ids.shape[0], input_ids.shape[1]))\n        rngs = {}\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n        if attention_mask is None:\n            attention_mask = jnp.ones((input_ids.shape[0], input_ids.shape[1]))\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            output_attentions,\n            not train,\n            False,\n            return_dict,\n            mutable=mutable,\n            rngs=rngs\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n        return outputs\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    @staticmethod\n    def update_inputs_for_generation(model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-falcon-modelling_falcon_flax/#src.python.easydel.modules.falcon.modelling_falcon_flax.FlaxFalconPretrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            return_dict=False\n        )\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-falcon-modelling_falcon_flax/#src.python.easydel.modules.falcon.modelling_falcon_flax.built_bloom_alibi","title":"<code>built_bloom_alibi(attention_mask, num_attention_heads)</code>","text":"<p>The built_bloom_alibi function is used to create a bloom alibi for the attention mask. The bloom alibi is used in the Bloom Attention layer to ensure that each token has a unique attention vector, even if it's masked out. This ensures that all tokens have an equal chance of being selected as the most important token in the sequence, which helps with training stability and performance.</p> <p>Parameters:</p> Name Type Description Default <code>attention_mask</code> <p>Mask out the padding tokens in the input sequence</p> required <code>num_attention_heads</code> <p>Determine the number of attention heads in the model</p> required <p>Returns:</p> Type Description <p>A tensor of shape (batch_size, num_attention_heads, 1,</p> <p>sequence_length)</p> Source code in <code>src/python/easydel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def built_bloom_alibi(attention_mask, num_attention_heads):\n    \"\"\"The built_bloom_alibi function is used to create a bloom alibi for the attention mask.\n    The bloom alibi is used in the Bloom Attention layer to ensure that each token has a unique\n    attention vector, even if it's masked out. This ensures that all tokens have an equal chance of being selected as\n    the most important token in the sequence, which helps with training stability and performance.\n\n    Args:\n        attention_mask: Mask out the padding tokens in the input\n            sequence\n        num_attention_heads: Determine the number of attention heads in\n            the model\n\n    Returns:\n        A tensor of shape (batch_size, num_attention_heads, 1,\n        sequence_length)\n    \"\"\"\n    batch_size, sequence_length = attention_mask.shape\n    cp2 = 2 ** math.floor(math.log2(num_attention_heads))\n    base = jnp.asarray(\n        2 ** (- (2 ** -(math.log2(cp2) - 3))), dtype=jnp.float32\n    )\n    powers = jnp.arange(1, 1 + cp2, dtype=jnp.float32)\n    slops = jnp.power(base, powers)\n    if cp2 != num_attention_heads:\n        extra_base = jnp.asarray(\n            2 ** (-(2 ** -(math.log2(2 * cp2) - 3))), dtype=jnp.float32\n        )\n        num_rem_heads = min(cp2, num_attention_heads - cp2)\n        extra_power = jnp.arange(1, 1 + 2 * num_rem_heads, 2, dtype=jnp.dtype)\n        slops = jnp.concatenate([slops, jnp.power(extra_base, extra_power)], axis=0)\n    arange_tensor = (((jnp.cumsum(attention_mask, axis=-1)) - 1) * attention_mask)[:, jnp.newaxis, :]\n    alibi = slops[..., jnp.newaxis].astype(jnp.bfloat16) * arange_tensor\n    return alibi.reshape(batch_size, num_attention_heads, 1, sequence_length)\n</code></pre>"},{"location":"generated-modules-falcon-modelling_falcon_flax/#src.python.easydel.modules.falcon.modelling_falcon_flax.dropout_add","title":"<code>dropout_add(linen_drop, x, residual, deterministic)</code>","text":"<p>The dropout_add function is a helper function that adds the residual to the output of the dropout layer. This is necessary because we want to use deterministic=True when we are evaluating our model, but we still need to add in the residual. The reason for this is that during training, we have two paths through our network: one with dropout and one without. The path without dropout (residual) allows us to backpropagate gradients through both paths at once.</p> <p>Parameters:</p> Name Type Description Default <code>linen_drop</code> <code>Dropout</code> <p>flax.linen.Dropout: Specify the dropout layer</p> required <code>x</code> <code>Array</code> <p>chex.Array: Pass in the input to the dropout layer</p> required <code>residual</code> <code>Array</code> <p>chex.Array: Add the residual to the output of dropout_add</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the dropout layer is active or not</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A tensor that is the sum of the residual and a dropout layer</p> Source code in <code>src/python/easydel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def dropout_add(linen_drop: flax.linen.Dropout, x: chex.Array, residual: chex.Array, deterministic: bool) -&gt; chex.Array:\n    \"\"\"The dropout_add function is a helper function that adds the residual to the output of\n    the dropout layer. This is necessary because we want to use deterministic=True when\n    we are evaluating our model, but we still need to add in the residual. The reason for this\n    is that during training, we have two paths through our network: one with dropout and one without.\n    The path without dropout (residual) allows us to backpropagate gradients through both paths at once.\n\n    Args:\n        linen_drop: flax.linen.Dropout: Specify the dropout layer\n        x: chex.Array: Pass in the input to the dropout layer\n        residual: chex.Array: Add the residual to the output of\n            dropout_add\n        deterministic: bool: Determine whether the dropout layer is\n            active or not\n\n    Returns:\n        A tensor that is the sum of the residual and a dropout layer\n    \"\"\"\n    out = linen_drop(inputs=x, deterministic=deterministic)\n    out = residual + out\n    return out\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/","title":"modules.flax_modelling_utils","text":""},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.BaseJAXAttentionModule","title":"<code>BaseJAXAttentionModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>class BaseJAXAttentionModule(nn.Module):\n    config: \"EasyDeLPretrainedConfig\"  # type: ignore\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query_states, attention_mask):\n        \"\"\"The _concatenate_to_cache function is used to concatenate the key and value vectors\n        of a query_states with those of previous queries. This allows for the attention mechanism to\n        look at all previous queries when computing its output. The function takes in three\n        arguments: key, value, and query_states. It also uses two variables that are stored in the cache:\n        cached_key and cached_value.\n\n        Args:\n            self: Access the variables stored in the cache\n            key: Store the keys of the encoder-decoder attention\n            value: Initialize the cached_value variable\n            query_states: Determine the number of cache vectors to\n                update\n            attention_mask: Mask out the padded vectors in the cache\n\n        Returns:\n            The key, value and attention_mask\n        \"\"\"\n        quantize_kv_cache = self.config.quantize_kv_cache\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        if quantize_kv_cache:\n            cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, jnp.int8)\n            cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, jnp.int8)\n            cached_key_scale = self.variable(\"cache\", \"cached_key_scale\", jnp.zeros, key.shape[0:-1], jnp.float32)\n            cached_value_scale = self.variable(\"cache\", \"cached_value_scale\", jnp.zeros, value.shape[0:-1], jnp.float32)\n            cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n        else:\n            cached_key_scale = None\n            cached_value_scale = None\n            cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n            cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n            cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            cur_index = cache_index.value\n            if query_states.shape[1] == 1 and self.config.use_sharded_kv_caching:\n                mesh = self.config.jax_mesh()\n\n                def fn(\n                        _cached_key,\n                        _cached_value,\n                        _key,\n                        _value,\n                        _cur_index\n                ):\n                    assert _key.shape[1] == 1 and _value.shape[1] == 1, (_key.shape, _value.shape)\n                    sp_size = max_length // mesh.shape[\"sp\"]\n                    axis_index = jax.lax.axis_index(\"sp\")\n                    _cur_index = _cur_index - axis_index * sp_size\n                    _key, _value = jax.lax.cond(\n                        jnp.logical_and(_cur_index &gt;= 0, _cur_index &lt; sp_size),\n                        lambda: (\n                            _cached_key.at[:, _cur_index].set(_key[:, -1]),\n                            _cached_value.at[:, _cur_index].set(_value[:, -1]),\n                        ),\n                        lambda: (_cached_key, _cached_value),\n                    )\n                    return _key, _value\n\n                fn = shard_map(\n                    fn, mesh=mesh,\n                    in_specs=(\n                        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                        PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n                        PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n                        PartitionSpec()\n                    ),\n                    out_specs=(\n                        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                        PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n                    ),\n                    check_rep=False\n                )\n                key, value = fn(cached_key.value, cached_value.value, key, value, cur_index)\n            else:\n                *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n                cur_index = cache_index.value\n                indices = (0,) * len(batch_dims) + (cur_index, 0, 0)  # type:ignore\n                if quantize_kv_cache:\n                    key_val = fjformer.linen.linen.de_quantize(\n                        cached_key.value,\n                        cached_key_scale.value,\n                        key.dtype,\n                        .0\n                    )\n                    value_val = fjformer.linen.linen.de_quantize(\n                        cached_value.value,\n                        cached_value_scale.value,\n                        value.dtype,\n                        .0\n                    )\n                else:\n                    key_val = cached_key.value\n                    value_val = cached_value.value\n\n                key = lax.dynamic_update_slice(key_val, key, indices)\n                value = lax.dynamic_update_slice(value_val, value, indices)\n                num_updated_cache_vectors = query_states.shape[1]\n                pad_mask = jnp.broadcast_to(\n                    jnp.arange(max_length) &lt; cur_index + num_updated_cache_vectors,\n                    tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n                )\n                attention_mask = combine_masks(pad_mask, attention_mask)\n            if quantize_kv_cache:\n                kq, ks = fjformer.linen.linen.quantize(key)\n                vq, vs = fjformer.linen.linen.quantize(value)\n\n                cached_key.value = kq\n                cached_key_scale.value = ks\n\n                cached_value.value = vq\n                cached_value_scale.value = vs\n\n            else:\n                cached_key.value = key\n                cached_value.value = value\n\n            num_updated_cache_vectors = query_states.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n        return key, value, attention_mask\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.add_start_docstrings","title":"<code>add_start_docstrings(*docstr)</code>","text":"<p>The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function. The add_start_docstrings function takes in an arbitrary number of strings and returns a decorator. The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.</p> <p>Parameters:</p> Name Type Description Default <code>*docstr</code> <p>Pass in a variable number of arguments to the function</p> <code>()</code> <p>Returns:</p> Type Description <p>A decorator that adds the docstrings to the function</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def add_start_docstrings(*docstr):\n    \"\"\"The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function.\n    The add_start_docstrings function takes in an arbitrary number of strings and returns a decorator.\n    The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to\n    the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.\n\n    Args:\n        *docstr: Pass in a variable number of arguments to the function\n\n    Returns:\n        A decorator that adds the docstrings to the function\n    \"\"\"\n\n    def docstring_decorator(fn):\n        fn.__doc__ = \"\".join(docstr) + \\\n                     (fn.__doc__ if fn.__doc__ is not None else \"\")\n        return fn\n\n    return docstring_decorator\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(tensor, sin_, cos_)</code>","text":"<p>The apply_rotary_pos_emb function applies a rotary positional embedding to the input tensor. b,h,s,d or pytorch style</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>Store the tensor that is passed into the function</p> required <code>sin_</code> <p>Rotate the tensor by pi/2</p> required <code>cos_</code> <p>Apply the cosine function to the tensor</p> required <p>Returns:</p> Type Description <p>A tensor with the same shape as the input tensor</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def apply_rotary_pos_emb(tensor, sin_, cos_):\n    \"\"\"The apply_rotary_pos_emb function applies a rotary positional embedding to the input tensor.\n    b,h,s,d or pytorch style\n\n    Args:\n        tensor: Store the tensor that is passed into the function\n        sin_: Rotate the tensor by pi/2\n        cos_: Apply the cosine function to the tensor\n\n    Returns:\n        A tensor with the same shape as the input tensor\n    \"\"\"\n    b, h, s, d = tensor.shape\n    return (tensor * cos_[:, :, :s, :]) + (rotate_half(tensor) * sin_[:, :, :s, :])\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.canonicalize_dtype","title":"<code>canonicalize_dtype(*args, dtype=None, inexact=True)</code>","text":"<p>Canonicalize an optional dtype to the definitive dtype.</p> <p>If the <code>dtype</code> is None this function will infer the dtype. If it is not None it will be returned unmodified or an exceptions is raised if the dtype is invalid. from the input arguments using <code>jnp.result_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>JAX array compatible values. None values are ignored.</p> <code>()</code> <code>dtype</code> <code>Optional[ArrayDType]</code> <p>Optional dtype override. If specified the arguments are cast to the specified dtype instead and dtype inference is disabled.</p> <code>None</code> <code>inexact</code> <code>bool</code> <p>When True, the output dtype must be a subdtype</p> <code>True</code> <p>Returns:   The dtype that *args should be cast to.</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def canonicalize_dtype(\n        *args, dtype: Optional[chex.ArrayDType] = None, inexact: bool = True\n) -&gt; chex.ArrayDType:\n    \"\"\"Canonicalize an optional dtype to the definitive dtype.\n\n    If the ``dtype`` is None this function will infer the dtype. If it is not\n    None it will be returned unmodified or an exceptions is raised if the dtype\n    is invalid.\n    from the input arguments using ``jnp.result_type``.\n\n    Args:\n      *args: JAX array compatible values. None values\n        are ignored.\n      dtype: Optional dtype override. If specified the arguments are cast to\n        the specified dtype instead and dtype inference is disabled.\n      inexact: When True, the output dtype must be a subdtype\n      of `jnp.inexact`. Inexact dtypes are real or complex floating points. This\n      is useful when you want to apply operations that don't work directly on\n      integers like taking a mean for example.\n    Returns:\n      The dtype that *args should be cast to.\n    \"\"\"\n    if dtype is None:\n        args_filtered = [jax.numpy.asarray(x) for x in args if x is not None]\n        dtype = jax.numpy.result_type(*args_filtered)\n        if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n            dtype = jax.numpy.promote_types(jax.numpy.float32, dtype)\n    if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n        raise ValueError(f'Dtype must be inexact: {dtype}')\n    return dtype\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.create_mesh","title":"<code>create_mesh(axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'sp'), backend='')</code>","text":"<p>The create_mesh function creates a mesh object that can be used to shard arrays.</p> <p>Parameters:</p> Name Type Description Default <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the mesh</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>backend</code> <p>Specify the backend to use</p> <code>''</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"), backend=\"\"\n):\n    \"\"\"The create_mesh function creates a mesh object that can be used to shard arrays.\n\n    Args:\n        axis_dims: Sequence[int]: Specify the dimensions of the mesh\n        axis_names: Sequence[str]: Name the axes of the mesh\n        backend: Specify the backend to use\n\n    Returns:\n        A mesh object\n    \"\"\"\n    array_devices = jax.numpy.ones(\n        (len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n    resh = array_devices.reshape(axis_dims).shape\n\n    return jax.sharding.Mesh(\n        create_device_mesh(resh), axis_names\n    )\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.get_dot_general_by_bits","title":"<code>get_dot_general_by_bits(bits=None, mode=EasyMethod.TRAIN)</code>","text":"<p>The get_general_dot function is a helper function that returns a q_flax.QDotGeneral object with the specified number of bits for forward and backward passes. If no bits are specified, the function returns None.</p> <p>Parameters:</p> Name Type Description Default <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits for quantization</p> <code>None</code> <code>mode</code> <code>Literal['train', 'serve', 'convert']</code> <p>EasyMethod: Specify the use of model to init the QDot Method for (e.q TRAIN,SERVE,...)</p> <code>TRAIN</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict that contain dot_general_cls</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def get_dot_general_by_bits(\n        bits: Optional[int] = None,\n        mode: Literal[\"train\", \"serve\", \"convert\"] = EasyMethod.TRAIN\n) -&gt; dict:\n    \"\"\"The get_general_dot function is a helper function that returns a q_flax.QDotGeneral object\n    with the specified number of bits for forward and backward passes. If no bits are specified,\n    the function returns None.\n\n    Args:\n        bits: Optional[int]: Specify the number of bits for quantization\n        mode: EasyMethod: Specify the use of model to init the QDot\n            Method for (e.q TRAIN,SERVE,...)\n\n    Returns:\n        A dict that contain dot_general_cls\n    \"\"\"\n    if mode == EasyMethod.TRAIN:\n        rhs_quant_mode = q_flax.QuantMode.TRAIN\n    elif mode == EasyMethod.EVAL or mode == EasyMethod.SERVE:\n        rhs_quant_mode = q_flax.QuantMode.SERVE\n    elif mode == EasyMethod.CONVERT:\n        rhs_quant_mode = q_flax.QuantMode.CONVERT\n    else:\n        raise ValueError(\"Unknown Quant Method for EasyMethod\")\n    if bits is not None:\n        return {\n            \"dot_general_cls\": functools.partial(\n                q_flax.QDotGeneral,\n                q_config.fully_quantized(\n                    fwd_bits=bits,\n                    bwd_bits=bits\n                ),\n                rhs_quant_mode=rhs_quant_mode\n            )\n        }\n    return {}  # empty just in case of not getting any error\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.get_gradient_checkpoint_policy","title":"<code>get_gradient_checkpoint_policy(name)</code>","text":"<p>The get_gradient_checkpoint_policy function is a helper function that returns the gradient checkpoint policy     specified by the name parameter.</p> <p>:param name: Select the checkpoint policy from the dictionary :return: A function that is used in the jax</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def get_gradient_checkpoint_policy(name):\n    \"\"\"\n    The get_gradient_checkpoint_policy function is a helper function that returns the gradient checkpoint policy\n        specified by the name parameter.\n\n    :param name: Select the checkpoint policy from the dictionary\n    :return: A function that is used in the jax\n\n    \"\"\"\n    gradients = dict(\n        everything_saveable=jax.checkpoint_policies.everything_saveable,\n        nothing_saveable=jax.checkpoint_policies.nothing_saveable,\n        dots_saveable=jax.checkpoint_policies.dots_saveable,\n        checkpoint_dots=jax.checkpoint_policies.checkpoint_dots,\n        dots_with_no_batch_dims_saveable=jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n        checkpoint_dots_with_no_batch_dims=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims,\n        save_anything_except_these_names=jax.checkpoint_policies.save_anything_except_these_names,\n        save_any_names_but_these=jax.checkpoint_policies.save_any_names_but_these,\n        save_only_these_names=jax.checkpoint_policies.save_only_these_names,\n        save_from_both_policies=jax.checkpoint_policies.save_from_both_policies\n    )\n    return gradients[name]\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.get_names_from_partition_spec","title":"<code>get_names_from_partition_spec(partition_specs)</code>","text":"<p>The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list. If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:     If the item is None, continue (do nothing) and move on to next iteration of loop.     If the item is an instance of str (i.e., if it's just one string), add that string to names set and move     on to next iteration of loop.     Otherwise, (if not None or str), call get_names_from_partition_spec recurs</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Define the partitioning of a table</p> required <p>Returns:</p> Type Description <p>A list of the names of all partitions</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def get_names_from_partition_spec(partition_specs):\n    \"\"\"The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list.\n    If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:\n        If the item is None, continue (do nothing) and move on to next iteration of loop.\n        If the item is an instance of str (i.e., if it's just one string), add that string to names set and move\n        on to next iteration of loop.\n        Otherwise, (if not None or str), call get_names_from_partition_spec recurs\n\n    Args:\n        partition_specs: Define the partitioning of a table\n\n    Returns:\n        A list of the names of all partitions\n    \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_partition_spec(item))\n\n    return list(names)\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.get_ranks_and_size","title":"<code>get_ranks_and_size(mesh)</code>","text":"<p>The get_ranks_and_size function is used to determine the number of MPI processes (<code>mp_node_size</code>) and the number of devices per process (<code>dp_node_size</code>). The <code>mesh.shape[mp]</code> determines how many MPI processes are needed, and then we divide that by the local device count to get <code>`mp_node_size = max( 1, mp / jax.local )</code>. This means that if there are more than enough devices for all MPI ranks on a node, each rank will only use one device; otherwise it will use</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <p>Get the shape of the mesh</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def get_ranks_and_size(mesh):\n    \"\"\"The get_ranks_and_size function is used to determine the number of MPI processes\n    (``mp_node_size``) and the number of devices per process (``dp_node_size``).\n    The ``mesh.shape[mp]`` determines how many MPI processes are needed,\n    and then we divide that by the local device count to get ``mp_node_size = max( 1, mp / jax.local )`.\n    This means that if there are more than enough devices for all MPI ranks on a node, each rank will only use one device; otherwise it will use\n\n    Args:\n        mesh: Get the shape of the mesh\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    out = dict(mesh=mesh)\n    total_process_size = mesh.shape[\"tp\"] * mesh.shape[\"sp\"]\n    mp_node_size = max(1, total_process_size // jax.local_device_count())\n    dp_node_size = jax.process_count() // mp_node_size\n    out.update(mp_node_size=mp_node_size,\n               dp_node_size=dp_node_size)\n\n    dp_node_rank = jax.process_index() // mp_node_size\n    mp_node_rank = jax.process_index() % mp_node_size\n    out.update(dp_node_rank=dp_node_rank,\n               mp_node_rank=mp_node_rank)\n    return out\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.names_in_mesh","title":"<code>names_in_mesh(*names)</code>","text":"<p>The names_in_mesh function is a decorator that can be used to check whether the names of the axes passed into a function are valid.  It will raise an exception if any of the axis names are not in the physical mesh.  For example, if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <p>Collect all the names passed to the function into a tuple</p> <code>()</code> <p>Returns:</p> Type Description <p>A boolean indicating whether all the given</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def names_in_mesh(*names):\n    \"\"\"The names_in_mesh function is a decorator that can be used to check whether\n    the names of the axes passed into a function are valid.  It will raise an\n    exception if any of the axis names are not in the physical mesh.  For example,\n    if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:\n\n    Args:\n        *names: Collect all the names passed to the function into a\n            tuple\n\n    Returns:\n        A boolean indicating whether all the given\n    \"\"\"\n    return set(names) &lt;= set(pxla.thread_resources.env.physical_mesh.axis_names)\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.repeat_kv_bnsh","title":"<code>repeat_kv_bnsh(x, n_rep)</code>","text":"<p>The repeat_kv_bnsh function is used to repeat the key and value vectors for each head in a multi-head attention module. This function takes as input an array of shape (batch_size, n_heads, sequence_length, head_dim) and returns an array of shape (batch_size, n_heads * nrep, sequence length, head dim). The reason this is necessary is because the attention module expects keys/values/queries to be repeated across heads but not across batches. However we want our keys/values/queries to be repeated both across heads AND batches so that we can use them</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>chex.Array: Pass in the input to the function</p> required <code>n_rep</code> <code>int</code> <p>int: Repeat the key and value heads</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A new array with the same shape as x, except for the second</p> <code>Array</code> <p>dimension which is n_kv_heads * n_rep</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def repeat_kv_bnsh(x: chex.Array, n_rep: int) -&gt; chex.Array:\n    \"\"\"The repeat_kv_bnsh function is used to repeat the key and value vectors for each head in a multi-head attention\n    module. This function takes as input an array of shape (batch_size, n_heads, sequence_length, head_dim) and returns\n    an array of shape (batch_size, n_heads * nrep, sequence length, head dim). The reason this is necessary is because the\n    attention module expects keys/values/queries to be repeated across heads but not across batches. However we want our\n    keys/values/queries to be repeated both across heads AND batches so that we can use them\n\n    Args:\n        x: chex.Array: Pass in the input to the function\n        n_rep: int: Repeat the key and value heads\n\n    Returns:\n        A new array with the same shape as x, except for the second\n        dimension which is n_kv_heads * n_rep\n    \"\"\"\n    bs, n_kv_heads, s, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    return x.reshape(bs, n_kv_heads * n_rep, s, head_dim)\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.repeat_kv_bsnh","title":"<code>repeat_kv_bsnh(x, n_rep)</code>","text":"<p>The repeat_kv_bsnh function is used to repeat the key and value vectors for each head.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>chex.Array: Specify the input array</p> required <code>n_rep</code> <code>int</code> <p>int: Repeat the key-value attention heads n_rep times</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A new array with the same batch size, sequence length, and head</p> <code>Array</code> <p>dimension as the input array</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def repeat_kv_bsnh(x: chex.Array, n_rep: int) -&gt; chex.Array:\n    \"\"\"The repeat_kv_bsnh function is used to repeat the key and value vectors for each head.\n\n    Args:\n        x: chex.Array: Specify the input array\n        n_rep: int: Repeat the key-value attention heads n_rep times\n\n    Returns:\n        A new array with the same batch size, sequence length, and head\n        dimension as the input array\n    \"\"\"\n    bs, s, n_kv_heads, head_dim = x.shape\n    x = x.transpose(0, 2, 1, 3)\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    x = x.transpose(0, 2, 1, 3)\n\n    return x.reshape(bs, s, n_kv_heads * n_rep, head_dim)\n</code></pre>"},{"location":"generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>The rotate_half function takes a complex-valued array and rotates the phase of its second half by 180 degrees. This is equivalent to multiplying the second half by -i, or equivalently rotating it 90 degrees counterclockwise.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Specify the input array</p> required <p>Returns:</p> Type Description <p>A new array that is the same as the input</p> Source code in <code>src/python/easydel/modules/flax_modelling_utils.py</code> <pre><code>def rotate_half(x):\n    \"\"\"The rotate_half function takes a complex-valued array and rotates the\n    phase of its second half by 180 degrees. This is equivalent to multiplying\n    the second half by -i, or equivalently rotating it 90 degrees counterclockwise.\n\n    Args:\n        x: Specify the input array\n\n    Returns:\n        A new array that is the same as the input\n    \"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return jax.numpy.concatenate((-x2, x1), axis=-1)\n</code></pre>"},{"location":"generated-modules-gemma-gemma_configuration/","title":"modules.gemma.gemma_configuration","text":""},{"location":"generated-modules-gemma-gemma_configuration/#src.python.easydel.modules.gemma.gemma_configuration.GemmaConfig","title":"<code>GemmaConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/gemma/gemma_configuration.py</code> <pre><code>class GemmaConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"gemma\"\n\n    def __init__(\n            self,\n            vocab_size=256000,\n            hidden_size=3072,\n            intermediate_size=24576,\n            num_hidden_layers=28,\n            num_attention_heads=16,\n            num_key_value_heads=16,\n            head_dim=256,\n            hidden_act=\"gelu_pytorch_tanh\",\n            max_position_embeddings=8192,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=0,\n            eos_token_id=1,\n            bos_token_id=2,\n            tie_word_embeddings=True,\n            rope_theta=10000.0,\n            attention_bias=False,\n            attention_dropout=0.0,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            scan_layers: bool = False,\n            hidden_activation=None,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object, which are sometimes called fields or properties.\n        The __init__ function can accept arguments, but self must be the first one.\n        \"\"\"\n\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        self.scan_layers = scan_layers\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.head_dim = head_dim\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.hidden_activation = hidden_activation\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            pad_token_id=pad_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n        \"\"\"\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-gemma-gemma_configuration/#src.python.easydel.modules.gemma.gemma_configuration.GemmaConfig.__init__","title":"<code>__init__(vocab_size=256000, hidden_size=3072, intermediate_size=24576, num_hidden_layers=28, num_attention_heads=16, num_key_value_heads=16, head_dim=256, hidden_act='gelu_pytorch_tanh', max_position_embeddings=8192, initializer_range=0.02, rms_norm_eps=1e-06, use_cache=True, pad_token_id=0, eos_token_id=1, bos_token_id=2, tie_word_embeddings=True, rope_theta=10000.0, attention_bias=False, attention_dropout=0.0, gradient_checkpointing='nothing_saveable', bits=None, scan_layers=False, hidden_activation=None, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object, which are sometimes called fields or properties. The init function can accept arguments, but self must be the first one.</p> Source code in <code>src/python/easydel/modules/gemma/gemma_configuration.py</code> <pre><code>def __init__(\n        self,\n        vocab_size=256000,\n        hidden_size=3072,\n        intermediate_size=24576,\n        num_hidden_layers=28,\n        num_attention_heads=16,\n        num_key_value_heads=16,\n        head_dim=256,\n        hidden_act=\"gelu_pytorch_tanh\",\n        max_position_embeddings=8192,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        eos_token_id=1,\n        bos_token_id=2,\n        tie_word_embeddings=True,\n        rope_theta=10000.0,\n        attention_bias=False,\n        attention_dropout=0.0,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        scan_layers: bool = False,\n        hidden_activation=None,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object, which are sometimes called fields or properties.\n    The __init__ function can accept arguments, but self must be the first one.\n    \"\"\"\n\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n    self.scan_layers = scan_layers\n    self.vocab_size = vocab_size\n    self.max_position_embeddings = max_position_embeddings\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.head_dim = head_dim\n    self.num_key_value_heads = num_key_value_heads\n    self.hidden_act = hidden_act\n    self.initializer_range = initializer_range\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.rope_theta = rope_theta\n    self.attention_bias = attention_bias\n    self.attention_dropout = attention_dropout\n    self.hidden_activation = hidden_activation\n    super().__init__(\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        pad_token_id=pad_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        bits=bits,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-modules-gemma-gemma_configuration/#src.python.easydel.modules.gemma.gemma_configuration.GemmaConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', bits=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> Source code in <code>src/python/easydel/modules/gemma/gemma_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n    \"\"\"\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-gemma-gemma_configuration/#src.python.easydel.modules.gemma.gemma_configuration.GemmaConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/gemma/gemma_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/","title":"modules.gemma.modelling_gemma_flax","text":""},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.FlaxGemmaAttention","title":"<code>FlaxGemmaAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>class FlaxGemmaAttention(BaseJAXAttentionModule):\n    config: GemmaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n    causal: bool = True\n    is_cross_attention: bool = False\n\n    def setup(self):\n        config = self.config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.head_dim\n        self.attention_softmax_in_fp32 = self.dtype is not jnp.float32\n\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n\n        kernel = jax.nn.initializers.normal(self.config.initializer_range)\n        self.q_proj = Linear(\n            self.num_heads * self.head_dim,\n            use_bias=config.attention_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=kernel,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            self.num_key_value_heads * self.head_dim,\n            use_bias=config.attention_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=kernel,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            self.num_key_value_heads * self.head_dim,\n            use_bias=config.attention_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=kernel,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            self.embed_dim,\n            use_bias=config.attention_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=kernel,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=self.head_dim ** -0.5,\n            axis_name=self.config.attention_axis_name\n        )\n\n        self.rotary_emb = FlaxGemmaRotaryEmbedding(config, dtype=self.dtype)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads * self.head_dim,))\n\n    def _split_heads(self, hidden_states, num_heads):\n        return hidden_states.reshape(hidden_states.shape[:2] + (num_heads, self.head_dim))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary_emb(\n            position_ids=position_ids, query_states=query, key_states=key, freq_cis=freq_cis\n        )\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n    ):\n        (\n            query_states,\n            key_states,\n            value_states\n        ) = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query_states.shape[0],\n            query_states.shape[1],\n            query_states,\n            key_states,\n            value_states,\n            freq_cis,\n            position_ids\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n\n        dropout_rng = None\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n        )\n\n        key_states = jnp.repeat(key_states, repeats=self.num_key_value_groups, axis=2)\n        value_states = jnp.repeat(value_states, repeats=self.num_key_value_groups, axis=2)\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        return (\n            attn_output,\n            attentions.attention_weights\n        ) if output_attentions else (attn_output,)\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.FlaxGemmaAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary_emb(\n        position_ids=position_ids, query_states=query, key_states=key, freq_cis=freq_cis\n    )\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.FlaxGemmaPreTrainedModel","title":"<code>FlaxGemmaPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>class FlaxGemmaPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    \"\"\"An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = GemmaConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: GemmaConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\"),\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            precision=precision,\n            param_dtype=param_dtype,\n            **kwargs\n        )\n        super().__init__(\n            config,\n            module,\n            input_shape=input_shape,\n            seed=seed,\n            dtype=dtype,\n            _do_init=_do_init\n        )\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        random_params = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            return_dict=False\n        )[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.FlaxGemmaPreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.add_positional_embedding","title":"<code>add_positional_embedding(input_embedding, position, theta=10000)</code>","text":"<p>Adds positional embeddings to input embeddings. From DeepMind Gemma</p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>def add_positional_embedding(\n        input_embedding: jax.Array,\n        position: int,\n        theta: int = 10_000,\n) -&gt; jax.Array:\n    \"\"\"Adds positional embeddings to input embeddings. From DeepMind Gemma\"\"\"\n    embed_dim = input_embedding.shape[-1]\n    num_timescales = embed_dim // 2\n    log_timescale_increment = jnp.log(float(theta)) / jnp.maximum(\n        jnp.asarray(num_timescales, dtype=jnp.float32) - 1, 1\n    )\n    inv_timescales = jnp.exp(\n        jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment\n    )\n    scaled_time = position * inv_timescales\n    signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)])\n    signal = jnp.pad(signal, [[0, jnp.mod(embed_dim, 2)]])\n    position_embedding = signal.astype(jnp.float32)\n\n    return input_embedding + position_embedding\n</code></pre>"},{"location":"generated-modules-gemma-modelling_gemma_flax/#src.python.easydel.modules.gemma.modelling_gemma_flax.apply_rope","title":"<code>apply_rope(inputs, positions, head_dim, theta=10000)</code>","text":"<p>Applies RoPE. From DeepMind Gemma</p> Source code in <code>src/python/easydel/modules/gemma/modelling_gemma_flax.py</code> <pre><code>def apply_rope(\n        inputs: jax.Array,  # [B, L]\n        positions: jax.Array,  # [B, L]\n        head_dim: int,\n        theta: int = 10_000,\n) -&gt; jax.Array:\n    \"\"\"Applies RoPE. From DeepMind Gemma\"\"\"\n    fraction = 2 * jnp.arange(0, head_dim // 2) / head_dim\n    timescale = theta ** fraction\n\n    sinusoid_inp = (\n            positions[..., jnp.newaxis] / timescale[jnp.newaxis, jnp.newaxis, :]\n    )\n    sinusoid_inp = sinusoid_inp[..., jnp.newaxis, :]\n    sin = jnp.sin(sinusoid_inp)\n    cos = jnp.cos(sinusoid_inp)\n\n    first_half, second_half = jnp.split(inputs, 2, axis=-1)\n    first_part = first_half * cos - second_half * sin\n    second_part = second_half * cos + first_half * sin\n    out = jnp.concatenate([first_part, second_part], axis=-1)\n    return out.astype(inputs.dtype)\n</code></pre>"},{"location":"generated-modules-gpt2-gpt2_configuration/","title":"modules.gpt2.gpt2_configuration","text":""},{"location":"generated-modules-gpt2-modelling_gpt2_flax/","title":"modules.gpt2.modelling_gpt2_flax","text":""},{"location":"generated-modules-gpt_j-gpt_j_configuration/","title":"modules.gpt_j.gpt_j_configuration","text":""},{"location":"generated-modules-gpt_j-modelling_gpt_j_flax/","title":"modules.gpt_j.modelling_gpt_j_flax","text":"<p>GPT-J model configuration</p>"},{"location":"generated-modules-gpt_neo_x-gpt_neo_x_configuration/","title":"modules.gpt_neo_x.gpt_neo_x_configuration","text":""},{"location":"generated-modules-gpt_neo_x-modelling_gpt_neo_x_flax/","title":"modules.gpt_neo_x.modelling_gpt_neo_x_flax","text":""},{"location":"generated-modules-grok_1-grok_1_configuration/","title":"modules.grok_1.grok_1_configuration","text":""},{"location":"generated-modules-grok_1-grok_1_configuration/#src.python.easydel.modules.grok_1.grok_1_configuration.Grok1Config","title":"<code>Grok1Config</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/grok_1/grok_1_configuration.py</code> <pre><code>class Grok1Config(EasyDeLPretrainedConfig):\n    model_type: str = \"grok-1\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=32768,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=32,\n            attn_output_multiplier=1.0,\n            max_attn_value=1.0,\n            max_position_embeddings=4096,\n            embedding_multiplier_scale: float = 1.0,\n            output_multiplier_scale: float = 1.0,\n            rms_norm_eps=1e-5,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=True,\n            num_experts_per_tok=2,\n            num_experts=8,\n            output_router_logits=False,\n            router_aux_loss_coef=0.001,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs\n    ):\n        self.vocab_size = vocab_size\n        self.attn_output_multiplier = attn_output_multiplier\n        self.max_attn_value = max_attn_value\n        self.max_position_embeddings = max_position_embeddings\n        self.embedding_multiplier_scale = embedding_multiplier_scale\n        self.output_multiplier_scale = output_multiplier_scale\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n\n        self.num_experts_per_tok = num_experts_per_tok\n        self.num_experts = num_experts\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"linear_1/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"post_attn_norm/kernel\", PartitionSpec(None)),\n            (\"pre_attn_norm/kernel\", PartitionSpec(None)),\n            (\"pre_moe_norm/kernel\", PartitionSpec(None)),\n            (\"post_moe_norm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"linear_1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"post_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"pre_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"pre_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"post_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"model/norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            tie_word_embeddings: bool: Tie the word embeddings to the\n                decoder\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n        \"\"\"\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout'\n</code></pre>"},{"location":"generated-modules-grok_1-grok_1_configuration/#src.python.easydel.modules.grok_1.grok_1_configuration.Grok1Config.add_jax_args","title":"<code>add_jax_args(tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', bits=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> Source code in <code>src/python/easydel/modules/grok_1/grok_1_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        tie_word_embeddings: bool: Tie the word embeddings to the\n            decoder\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n    \"\"\"\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-grok_1-grok_1_configuration/#src.python.easydel.modules.grok_1.grok_1_configuration.Grok1Config.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/grok_1/grok_1_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"linear_1/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"post_attn_norm/kernel\", PartitionSpec(None)),\n        (\"pre_attn_norm/kernel\", PartitionSpec(None)),\n        (\"pre_moe_norm/kernel\", PartitionSpec(None)),\n        (\"post_moe_norm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"linear/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"linear_1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"linear_v/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"post_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"pre_attn_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"pre_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"post_moe_norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"model/norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/","title":"modules.grok_1.modelling_grok_1_flax","text":""},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1Attention","title":"<code>FlaxGrok1Attention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1Attention(BaseJAXAttentionModule):\n    config: Grok1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxGrok1Embedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name\n        )\n        self.resid_dropout = flax.linen.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1Attention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n    outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1Attention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1BLockSparseMLP","title":"<code>FlaxGrok1BLockSparseMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1BLockSparseMLP(nn.Module):\n    config: Grok1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.linear = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.linear_1 = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.linear_v = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout #\n                IGNORED\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        return self.linear_1(nn.gelu(self.linear(x)) * self.linear_v(x))\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1BLockSparseMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout # IGNORED</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout #\n            IGNORED\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    return self.linear_1(nn.gelu(self.linear(x)) * self.linear_v(x))\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1DecoderLayer","title":"<code>FlaxGrok1DecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1DecoderLayer(nn.Module):\n    config: Grok1Config\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        # hidden_states: chex.Array\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array\n        # causal_mask: chex.Array\n        # position_ids: chex.Array\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = True\n\n        attn_block = FlaxGrok1Attention\n        mlp_block = FlaxGrok1SparseMoeBlock\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = re_mat(\n                attn_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    3, 5, 6, 7, 8\n                )\n            )\n            mlp_block = re_mat(\n                mlp_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    1,\n                )\n            )\n        self.attn = attn_block(\n            config=self.config,\n            layer_index=self.layer_index,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.moe_block = mlp_block(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.pre_attn_norm = FlaxGrok1RMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attn_norm = FlaxGrok1RMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.pre_moe_norm = FlaxGrok1RMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_moe_norm = FlaxGrok1RMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True,\n            output_router_logits: Optional[bool] = False,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states and attention_output\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.pre_attn_norm(hidden_states)\n        hidden_states, attention_weights, present_key_value = self.attn(\n            hidden_states,\n            freq_cis,\n            attention_mask,\n            causal_mask,\n            position_ids,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions\n        )\n\n        hidden_states = self.post_attn_norm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.pre_moe_norm(hidden_states)\n        hidden_states, router_logits = self.moe_block(hidden_states)\n        hidden_states = self.post_moe_norm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (attention_weights,)\n        if output_router_logits:\n            outputs += (router_logits,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1DecoderLayer.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True, output_router_logits=False)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states and attention_output</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True,\n        output_router_logits: Optional[bool] = False,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states and attention_output\n    \"\"\"\n    residual = hidden_states\n    hidden_states = self.pre_attn_norm(hidden_states)\n    hidden_states, attention_weights, present_key_value = self.attn(\n        hidden_states,\n        freq_cis,\n        attention_mask,\n        causal_mask,\n        position_ids,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions\n    )\n\n    hidden_states = self.post_attn_norm(hidden_states)\n    hidden_states = residual + hidden_states\n\n    residual = hidden_states\n    hidden_states = self.pre_moe_norm(hidden_states)\n    hidden_states, router_logits = self.moe_block(hidden_states)\n    hidden_states = self.post_moe_norm(hidden_states)\n    hidden_states = residual + hidden_states\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (attention_weights,)\n    if output_router_logits:\n        outputs += (router_logits,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1DecoderLayerCollection","title":"<code>FlaxGrok1DecoderLayerCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1DecoderLayerCollection(nn.Module):\n    config: Grok1Config\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.blocks = [\n            FlaxGrok1DecoderLayer(\n                layer_index=layer_index,\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(layer_index)\n            )\n\n            for layer_index in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_hidden_states: Optional[bool] = False,\n            output_attentions: Optional[bool] = False,\n            output_router_logits: Optional[bool] = False,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states, attention_output,\n            all_hidden_states and all_router_logits\n        \"\"\"\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_router_logits = () if output_router_logits else None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                output_attentions=output_attentions,\n                output_router_logits=output_router_logits,\n                init_cache=init_cache,\n                freq_cis=freq_cis,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n            )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n            if output_router_logits:\n                all_router_logits += (layer_outputs[-1],)\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (all_self_attns,)\n        if output_hidden_states:\n            outputs += (all_hidden_states,)\n        if output_router_logits:\n            outputs += (all_router_logits,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1DecoderLayerCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, deterministic=True, init_cache=False, output_hidden_states=False, output_attentions=False, output_router_logits=False)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states, attention_output,</p> <p>all_hidden_states and all_router_logits</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_hidden_states: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        output_router_logits: Optional[bool] = False,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states, attention_output,\n        all_hidden_states and all_router_logits\n    \"\"\"\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    all_router_logits = () if output_router_logits else None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            output_router_logits=output_router_logits,\n            init_cache=init_cache,\n            freq_cis=freq_cis,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n        )\n\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n\n        if output_router_logits:\n            all_router_logits += (layer_outputs[-1],)\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (all_self_attns,)\n    if output_hidden_states:\n        outputs += (all_hidden_states,)\n    if output_router_logits:\n        outputs += (all_router_logits,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1ForCausalLM","title":"<code>FlaxGrok1ForCausalLM</code>","text":"<p>               Bases: <code>Grok1PreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1ForCausalLM(Grok1PreTrainedModel):\n    module_class = FlaxGrok1ForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1ForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>:param self: Access variables that belong to the class :param input_ids: Pass in the input tokens :param max_length: Set the length of the sequence to be generated :param attention_mask: Optional[chex.Array]: Mask the attention weights :return: A dictionary of the past_key_values, attention_mask and position ids</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"\n    The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    :param self: Access variables that belong to the class\n    :param input_ids: Pass in the input tokens\n    :param max_length: Set the length of the sequence to be generated\n    :param attention_mask: Optional[chex.Array]: Mask the attention weights\n    :return: A dictionary of the past_key_values, attention_mask and position ids\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.FlaxGrok1SparseMoeBlock","title":"<code>FlaxGrok1SparseMoeBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>This implementation is strictly equivalent to standard MoE with full capacity (no dropped tokens). It's faster since it formulates MoE operations in terms of block-sparse operations to accomodate imbalanced assignments of tokens to experts, whereas standard MoE either (1) drop tokens at the cost of reduced performance or (2) set capacity factor to number of experts and thus waste computation and memory on padding.</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class FlaxGrok1SparseMoeBlock(nn.Module):\n    \"\"\"This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n    config: Grok1Config\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[\n        Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.gate = Linear(\n            self.config.num_experts,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n        )\n\n        self.experts = FlaxGrok1BlocKSparesTop2MLPCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            e: bool = False  # Ignored\n    ) -&gt; Tuple[chex.Array, chex.Array]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n\n        router_logits = self.gate(hidden_states).astype(\n            jnp.promote_types(self.dtype, jnp.float32)\n        )\n        routing_weights, selected_experts = jax.lax.top_k(\n            router_logits,\n            k=self.config.num_experts_per_tok\n        )\n        routing_weights = jax.nn.softmax(\n            routing_weights.astype(\n                jnp.promote_types(self.dtype, jnp.float32)\n            ), axis=-1\n        )\n\n        return self.experts(\n            selected_experts=selected_experts,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            hidden_dim=hidden_dim,\n            hidden_states=hidden_states,\n            routing_weights=routing_weights\n        ), router_logits\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.Grok1PreTrainedModel","title":"<code>Grok1PreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>class Grok1PreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class: Grok1Config = Grok1Config\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    # main_input_name = \"input_ids\"\n\n    def __init__(\n            self,\n            config: Grok1Config,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\n                \"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple,\n                     params: Optional[FrozenDict] = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n\n        self.config.initialization_of_moe = True\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        self.config.initialization_of_moe = False\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            # output_router_logits: Optional[bool] = None\n            output_router_logits,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.Grok1PreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n        # attention_mask: Optional[chex.Array] = None\n        jnp.array(attention_mask, dtype=\"i4\"),\n        # position_ids: Optional[chex.Array] = None\n        jnp.array(position_ids, dtype=\"i4\"),\n        None,  # inputs_embeds: Optional[chex.Array] = None\n        output_attentions,  # output_attentions: Optional[bool] = None\n        # output_hidden_states: Optional[bool] = None\n        output_hidden_states,\n        # output_router_logits: Optional[bool] = None\n        output_router_logits,\n        False,  # init_cache: bool = False\n        not train,  # deterministic: bool = True\n        return_dict,  # return_dict: bool = True\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-grok_1-modelling_grok_1_flax/#src.python.easydel.modules.grok_1.modelling_grok_1_flax.Grok1PreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>Optional[FrozenDict]</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/grok_1/modelling_grok_1_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple,\n                 params: Optional[FrozenDict] = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n\n    self.config.initialization_of_moe = True\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n        input_shape,\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False\n        )\n    random_params = module_init_outputs[\"params\"]\n\n    self.config.initialization_of_moe = False\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-jetmoe-jetmoe_configuration/","title":"modules.jetmoe.jetmoe_configuration","text":""},{"location":"generated-modules-jetmoe-jetmoe_configuration/#src.python.easydel.modules.jetmoe.jetmoe_configuration.JetMoEConfig","title":"<code>JetMoEConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/jetmoe/jetmoe_configuration.py</code> <pre><code>class JetMoEConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"jetmoe\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=2048,\n            num_hidden_layers=12,\n            num_attention_heads=32,\n            num_key_value_heads=16,\n            kv_channels=128,\n            ffn_hidden_size=5632,\n            max_position_embeddings=4096,\n            activation_function=\"silu\",\n            glu=True,\n            moe_num_experts=8,\n            moe_top_k=2,\n            use_cache=True,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=True,\n            bias=True,\n            rope_theta=10000.0,\n            rms_norm_eps=1e-6,\n            initializer_range=0.01,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.ffn_hidden_size = ffn_hidden_size\n        self.kv_channels = kv_channels\n        self.bias = bias\n        self.glu = glu\n        self.moe_num_experts = moe_num_experts\n        self.moe_top_k = moe_top_k\n        self.activation_function = activation_function\n        self.rope_theta = rope_theta\n        self.initializer_range = initializer_range\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            tie_word_embeddings: bool: Tie the word embeddings to the\n                decoder\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n        \"\"\"\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout'\n</code></pre>"},{"location":"generated-modules-jetmoe-jetmoe_configuration/#src.python.easydel.modules.jetmoe.jetmoe_configuration.JetMoEConfig.add_jax_args","title":"<code>add_jax_args(tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', bits=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> Source code in <code>src/python/easydel/modules/jetmoe/jetmoe_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        tie_word_embeddings: bool: Tie the word embeddings to the\n            decoder\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n    \"\"\"\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-jetmoe-jetmoe_configuration/#src.python.easydel.modules.jetmoe.jetmoe_configuration.JetMoEConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/jetmoe/jetmoe_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-jetmoe-modelling_jetmoe_flax/","title":"modules.jetmoe.modelling_jetmoe_flax","text":""},{"location":"generated-modules-jetmoe-modelling_jetmoe_flax/#src.python.easydel.modules.jetmoe.modelling_jetmoe_flax.compute_gating","title":"<code>compute_gating(k, num_experts, top_k_gates, top_k_indices)</code>","text":"<p>Compute gating values for the mixture of experts based on probabilities and top-k indices.</p> Source code in <code>src/python/easydel/modules/jetmoe/modelling_jetmoe_flax.py</code> <pre><code>def compute_gating(k: int, num_experts: int, top_k_gates: jnp.ndarray, top_k_indices: jnp.ndarray) -&gt; Tuple[\n    chex.Array, chex.Array, chex.Array, chex.Array\n]:\n    \"\"\"Compute gating values for the mixture of experts based on probabilities and top-k indices.\"\"\"\n    zeros = jnp.zeros([top_k_gates.shape[0], num_experts], dtype=top_k_gates.dtype)\n    gates = zeros.at[jnp.arange(zeros.shape[0])[:, None], top_k_indices].set(1)\n    expert_size = gates.astype(jnp.int32).sum(axis=0)\n    top_k_gates = top_k_gates.flatten()\n    top_k_experts = top_k_indices.flatten()\n    index_sorted_experts = jnp.argsort(top_k_experts)\n    batch_index = lax.div(index_sorted_experts, k)\n    batch_gates = top_k_gates[index_sorted_experts]\n    return batch_gates, batch_index, expert_size, index_sorted_experts\n</code></pre>"},{"location":"generated-modules-llama-llama_configuration/","title":"modules.llama.llama_configuration","text":""},{"location":"generated-modules-llama-llama_configuration/#src.python.easydel.modules.llama.llama_configuration.LlamaConfig","title":"<code>LlamaConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/llama/llama_configuration.py</code> <pre><code>class LlamaConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"llama\"\n\n    def __init__(\n            self,\n            vocab_size: int = 32000,\n            hidden_size: int = 4096,\n            intermediate_size: int = 11008,\n            num_hidden_layers: int = 32,\n            num_attention_heads: int = 32,\n            number_rep_kv: int = 1,\n            num_key_value_heads: Optional[int] = None,\n            max_position_embeddings: int = 2048,\n            rms_norm_eps: float = 1e-6,\n            initializer_range: float = 0.02,\n            use_cache: bool = True,\n            bos_token_id: int = 0,\n            eos_token_id: int = 1,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attention_dropout: float = 0.0,\n            rope_theta: float = 10000.,\n            attention_bias: bool = False,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = -1,\n            fcm_max_ratio: float = -1,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            hidden_act: str = 'silu',\n            pretraining_tp: int = 1,\n            scan_layers: bool = False,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object, which are sometimes called fields or properties.\n        The __init__ function can accept arguments, but self must be the first one.\n\n        Args:\n            self: Refer to the object itself\n            vocab_size: int: Set the size of the vocabulary\n            hidden_size: int: Set the size of the hidden layers in each\n                transformer block\n            intermediate_size: int: Set the size of the intermediate\n                layer\n            num_hidden_layers: int: Determine the number of layers in\n                the transformer\n            num_attention_heads: int: Determine the number of attention\n                heads\n            number_rep_kv: int: Set the number of times to repeat the\n                key and value vectors\n            num_key_value_heads: Optional[int]: Define the number of\n                key-value heads\n            max_position_embeddings: int: Set the maximum length of a\n                sequence\n            rms_norm_eps: float: Prevent division by zero in the rms\n                normalization\n            initializer_range: float: Initialize the weights of the\n                model\n            use_cache: bool: Determine whether the attention layer\n                should use a cache for faster computation\n            bos_token_id: int: Set the beginning of sequence token\n            eos_token_id: int: Specify the end of sentence token\n            resid_pdrop: float: Set the dropout rate for residual\n                connections\n            embd_pdrop: float: Dropout the embedding layer\n            attention_dropout: float: Dropout the attention weights\n            tie_word_embeddings: bool: Tie the word embeddings and\n                output layer weights\n            gradient_checkpointing: str: Specify how to checkpoint the\n                gradients\n            fcm_min_ratio: float: Set the minimum ratio of the number of\n                elements in a tensor to be processed by flash\n            fcm_max_ratio: float: Determine the maximum ratio of\n            rope_scaling: Dict[str: Define the scaling of the rope\n            Union[str: Specify the type of the parameter\n            float]]: Specify the type of the parameter\n            shard_attention_computation: bool: when ever to use\n                shard_map for attention\n            bits: Optional[int]: Specify the number of bits used to\n                quantize the weights\n            rope_theta: float : rope_theta for compute rope\n            attention_bias: bool : whenever to use attention bias or no\n            hidden_act: str : hidden_act for mlp\n            axis_dims: Sequence[int]: Specify the dimensions of each\n                axis\n            axis_names: Sequence[str]: Specify the names of the axes in\n                a tensor\n            scan_layers: bool: Determine whether to use the scan_layers\n                or not\n            **kwargs: Pass a variable number of keyword arguments to a\n                function\n        :param : Define the number of layers in the model\n\n        Returns:\n            Nothing\n        \"\"\"\n        num_key_value_heads = num_key_value_heads or number_rep_kv * num_attention_heads\n        self.num_key_value_heads = num_key_value_heads\n        self.vocab_size = vocab_size\n\n        self.number_rep_kv = number_rep_kv\n        self.hidden_size = hidden_size\n        self.initializer_range = initializer_range\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.num_attention_heads = num_attention_heads\n        self.max_position_embeddings = max_position_embeddings\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.pretraining_tp = pretraining_tp\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attention_dropout = attention_dropout\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.hidden_act = hidden_act\n        self.fcm_max_ratio = fcm_max_ratio\n        self.rope_scaling = rope_scaling\n        self.bits = bits\n        self.scan_layers = scan_layers\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attention_dropout: float = 0.0,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = 0.0,\n            fcm_max_ratio: float = 0.0,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            rope_theta: float = 10000.,\n            attention_bias: bool = False,\n            hidden_act: str = 'silu',\n            scan_layers: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            resid_pdrop: float: Set the dropout rate for residual\n                connections\n            embd_pdrop: float: Set the probability of dropping an\n                embedding\n            attention_dropout: float: Set the probability of dropping\n                out the attention layer\n            tie_word_embeddings: bool: Tie the word embeddings to the\n                decoder\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            fcm_min_ratio: float: Control the minimum ratio of the\n                number of chunks to be used in flash-based computation\n            fcm_max_ratio: float: Set the maximum ratio of the number of\n                input tokens to output tokens\n            number_rep_kv: int: Determine how many times the key and\n                value vectors are repeated\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n            rope_theta: float : rope_theta for compute rope\n            attention_bias: bool : whenever to use attention bias or no\n            hidden_act: str : hidden_act for mlp\n            scan_layers: bool: Determine whether to use scan layers or\n                not\n        \"\"\"\n        self.scan_layers = scan_layers\n        self.embd_pdrop = embd_pdrop\n        self.number_rep_kv = number_rep_kv\n        self.resid_pdrop = resid_pdrop\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-llama-llama_configuration/#src.python.easydel.modules.llama.llama_configuration.LlamaConfig.__init__","title":"<code>__init__(vocab_size=32000, hidden_size=4096, intermediate_size=11008, num_hidden_layers=32, num_attention_heads=32, number_rep_kv=1, num_key_value_heads=None, max_position_embeddings=2048, rms_norm_eps=1e-06, initializer_range=0.02, use_cache=True, bos_token_id=0, eos_token_id=1, resid_pdrop=0.0, embd_pdrop=0.0, attention_dropout=0.0, rope_theta=10000.0, attention_bias=False, tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', fcm_min_ratio=-1, fcm_max_ratio=-1, rope_scaling=None, scan_mlp_chunk_size=1024, bits=None, hidden_act='silu', pretraining_tp=1, scan_layers=False, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object, which are sometimes called fields or properties. The init function can accept arguments, but self must be the first one.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>vocab_size</code> <code>int</code> <p>int: Set the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <code>int</code> <p>int: Set the size of the hidden layers in each transformer block</p> <code>4096</code> <code>intermediate_size</code> <code>int</code> <p>int: Set the size of the intermediate layer</p> <code>11008</code> <code>num_hidden_layers</code> <code>int</code> <p>int: Determine the number of layers in the transformer</p> <code>32</code> <code>num_attention_heads</code> <code>int</code> <p>int: Determine the number of attention heads</p> <code>32</code> <code>number_rep_kv</code> <code>int</code> <p>int: Set the number of times to repeat the key and value vectors</p> <code>1</code> <code>num_key_value_heads</code> <code>Optional[int]</code> <p>Optional[int]: Define the number of key-value heads</p> <code>None</code> <code>max_position_embeddings</code> <code>int</code> <p>int: Set the maximum length of a sequence</p> <code>2048</code> <code>rms_norm_eps</code> <code>float</code> <p>float: Prevent division by zero in the rms normalization</p> <code>1e-06</code> <code>initializer_range</code> <code>float</code> <p>float: Initialize the weights of the model</p> <code>0.02</code> <code>use_cache</code> <code>bool</code> <p>bool: Determine whether the attention layer should use a cache for faster computation</p> <code>True</code> <code>bos_token_id</code> <code>int</code> <p>int: Set the beginning of sequence token</p> <code>0</code> <code>eos_token_id</code> <code>int</code> <p>int: Specify the end of sentence token</p> <code>1</code> <code>resid_pdrop</code> <code>float</code> <p>float: Set the dropout rate for residual connections</p> <code>0.0</code> <code>embd_pdrop</code> <code>float</code> <p>float: Dropout the embedding layer</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>float: Dropout the attention weights</p> <code>0.0</code> <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings and output layer weights</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify how to checkpoint the gradients</p> <code>'nothing_saveable'</code> <code>fcm_min_ratio</code> <code>float</code> <p>float: Set the minimum ratio of the number of elements in a tensor to be processed by flash</p> <code>-1</code> <code>fcm_max_ratio</code> <code>float</code> <p>float: Determine the maximum ratio of</p> <code>-1</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str: Define the scaling of the rope</p> <code>None</code> <code>Union[str</code> <p>Specify the type of the parameter</p> required <code>float]]</code> <p>Specify the type of the parameter</p> required <code>shard_attention_computation</code> <p>bool: when ever to use shard_map for attention</p> required <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used to quantize the weights</p> <code>None</code> <code>rope_theta</code> <code>float</code> <p>float : rope_theta for compute rope</p> <code>10000.0</code> <code>attention_bias</code> <code>bool</code> <p>bool : whenever to use attention bias or no</p> <code>False</code> <code>hidden_act</code> <code>str</code> <p>str : hidden_act for mlp</p> <code>'silu'</code> <code>axis_dims</code> <p>Sequence[int]: Specify the dimensions of each axis</p> required <code>axis_names</code> <p>Sequence[str]: Specify the names of the axes in a tensor</p> required <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use the scan_layers or not</p> <code>False</code> <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <p>:param : Define the number of layers in the model</p> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/python/easydel/modules/llama/llama_configuration.py</code> <pre><code>def __init__(\n        self,\n        vocab_size: int = 32000,\n        hidden_size: int = 4096,\n        intermediate_size: int = 11008,\n        num_hidden_layers: int = 32,\n        num_attention_heads: int = 32,\n        number_rep_kv: int = 1,\n        num_key_value_heads: Optional[int] = None,\n        max_position_embeddings: int = 2048,\n        rms_norm_eps: float = 1e-6,\n        initializer_range: float = 0.02,\n        use_cache: bool = True,\n        bos_token_id: int = 0,\n        eos_token_id: int = 1,\n        resid_pdrop: float = 0.0,\n        embd_pdrop: float = 0.0,\n        attention_dropout: float = 0.0,\n        rope_theta: float = 10000.,\n        attention_bias: bool = False,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        fcm_min_ratio: float = -1,\n        fcm_max_ratio: float = -1,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        hidden_act: str = 'silu',\n        pretraining_tp: int = 1,\n        scan_layers: bool = False,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object, which are sometimes called fields or properties.\n    The __init__ function can accept arguments, but self must be the first one.\n\n    Args:\n        self: Refer to the object itself\n        vocab_size: int: Set the size of the vocabulary\n        hidden_size: int: Set the size of the hidden layers in each\n            transformer block\n        intermediate_size: int: Set the size of the intermediate\n            layer\n        num_hidden_layers: int: Determine the number of layers in\n            the transformer\n        num_attention_heads: int: Determine the number of attention\n            heads\n        number_rep_kv: int: Set the number of times to repeat the\n            key and value vectors\n        num_key_value_heads: Optional[int]: Define the number of\n            key-value heads\n        max_position_embeddings: int: Set the maximum length of a\n            sequence\n        rms_norm_eps: float: Prevent division by zero in the rms\n            normalization\n        initializer_range: float: Initialize the weights of the\n            model\n        use_cache: bool: Determine whether the attention layer\n            should use a cache for faster computation\n        bos_token_id: int: Set the beginning of sequence token\n        eos_token_id: int: Specify the end of sentence token\n        resid_pdrop: float: Set the dropout rate for residual\n            connections\n        embd_pdrop: float: Dropout the embedding layer\n        attention_dropout: float: Dropout the attention weights\n        tie_word_embeddings: bool: Tie the word embeddings and\n            output layer weights\n        gradient_checkpointing: str: Specify how to checkpoint the\n            gradients\n        fcm_min_ratio: float: Set the minimum ratio of the number of\n            elements in a tensor to be processed by flash\n        fcm_max_ratio: float: Determine the maximum ratio of\n        rope_scaling: Dict[str: Define the scaling of the rope\n        Union[str: Specify the type of the parameter\n        float]]: Specify the type of the parameter\n        shard_attention_computation: bool: when ever to use\n            shard_map for attention\n        bits: Optional[int]: Specify the number of bits used to\n            quantize the weights\n        rope_theta: float : rope_theta for compute rope\n        attention_bias: bool : whenever to use attention bias or no\n        hidden_act: str : hidden_act for mlp\n        axis_dims: Sequence[int]: Specify the dimensions of each\n            axis\n        axis_names: Sequence[str]: Specify the names of the axes in\n            a tensor\n        scan_layers: bool: Determine whether to use the scan_layers\n            or not\n        **kwargs: Pass a variable number of keyword arguments to a\n            function\n    :param : Define the number of layers in the model\n\n    Returns:\n        Nothing\n    \"\"\"\n    num_key_value_heads = num_key_value_heads or number_rep_kv * num_attention_heads\n    self.num_key_value_heads = num_key_value_heads\n    self.vocab_size = vocab_size\n\n    self.number_rep_kv = number_rep_kv\n    self.hidden_size = hidden_size\n    self.initializer_range = initializer_range\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.rope_theta = rope_theta\n    self.attention_bias = attention_bias\n    self.num_attention_heads = num_attention_heads\n    self.max_position_embeddings = max_position_embeddings\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.pretraining_tp = pretraining_tp\n    self.resid_pdrop = resid_pdrop\n    self.embd_pdrop = embd_pdrop\n    self.attention_dropout = attention_dropout\n    self.gradient_checkpointing = gradient_checkpointing\n    self.fcm_min_ratio = fcm_min_ratio\n    self.hidden_act = hidden_act\n    self.fcm_max_ratio = fcm_max_ratio\n    self.rope_scaling = rope_scaling\n    self.bits = bits\n    self.scan_layers = scan_layers\n    super().__init__(\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        scan_mlp_chunk_size=scan_mlp_chunk_size,\n        bits=bits,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-modules-llama-llama_configuration/#src.python.easydel.modules.llama.llama_configuration.LlamaConfig.add_jax_args","title":"<code>add_jax_args(resid_pdrop=0.0, embd_pdrop=0.0, attention_dropout=0.0, tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', fcm_min_ratio=0.0, fcm_max_ratio=0.0, number_rep_kv=1, bits=None, rope_theta=10000.0, attention_bias=False, hidden_act='silu', scan_layers=True, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>resid_pdrop</code> <code>float</code> <p>float: Set the dropout rate for residual connections</p> <code>0.0</code> <code>embd_pdrop</code> <code>float</code> <p>float: Set the probability of dropping an embedding</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>float: Set the probability of dropping out the attention layer</p> <code>0.0</code> <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>fcm_min_ratio</code> <code>float</code> <p>float: Control the minimum ratio of the number of chunks to be used in flash-based computation</p> <code>0.0</code> <code>fcm_max_ratio</code> <code>float</code> <p>float: Set the maximum ratio of the number of input tokens to output tokens</p> <code>0.0</code> <code>number_rep_kv</code> <code>int</code> <p>int: Determine how many times the key and value vectors are repeated</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> <code>rope_theta</code> <code>float</code> <p>float : rope_theta for compute rope</p> <code>10000.0</code> <code>attention_bias</code> <code>bool</code> <p>bool : whenever to use attention bias or no</p> <code>False</code> <code>hidden_act</code> <code>str</code> <p>str : hidden_act for mlp</p> <code>'silu'</code> <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use scan layers or not</p> <code>True</code> Source code in <code>src/python/easydel/modules/llama/llama_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        resid_pdrop: float = 0.0,\n        embd_pdrop: float = 0.0,\n        attention_dropout: float = 0.0,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        fcm_min_ratio: float = 0.0,\n        fcm_max_ratio: float = 0.0,\n        number_rep_kv: int = 1,\n        bits: Optional[int] = None,\n        rope_theta: float = 10000.,\n        attention_bias: bool = False,\n        hidden_act: str = 'silu',\n        scan_layers: bool = True,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        resid_pdrop: float: Set the dropout rate for residual\n            connections\n        embd_pdrop: float: Set the probability of dropping an\n            embedding\n        attention_dropout: float: Set the probability of dropping\n            out the attention layer\n        tie_word_embeddings: bool: Tie the word embeddings to the\n            decoder\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        fcm_min_ratio: float: Control the minimum ratio of the\n            number of chunks to be used in flash-based computation\n        fcm_max_ratio: float: Set the maximum ratio of the number of\n            input tokens to output tokens\n        number_rep_kv: int: Determine how many times the key and\n            value vectors are repeated\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n        rope_theta: float : rope_theta for compute rope\n        attention_bias: bool : whenever to use attention bias or no\n        hidden_act: str : hidden_act for mlp\n        scan_layers: bool: Determine whether to use scan layers or\n            not\n    \"\"\"\n    self.scan_layers = scan_layers\n    self.embd_pdrop = embd_pdrop\n    self.number_rep_kv = number_rep_kv\n    self.resid_pdrop = resid_pdrop\n    self.rope_theta = rope_theta\n    self.attention_bias = attention_bias\n    self.attention_dropout = attention_dropout\n    self.hidden_act = hidden_act\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.fcm_min_ratio = fcm_min_ratio\n    self.fcm_max_ratio = fcm_max_ratio\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-llama-llama_configuration/#src.python.easydel.modules.llama.llama_configuration.LlamaConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/llama/llama_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/","title":"modules.llama.modelling_llama_flax","text":""},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaAttention","title":"<code>FlaxLlamaAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaAttention(BaseJAXAttentionModule):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxLlamaEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name\n        )\n        self.resid_dropout = flax.linen.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        (\n            query_states,\n            key_states,\n            value_states\n        ) = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    (\n        query_states,\n        key_states,\n        value_states\n    ) = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n    outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaBlock","title":"<code>FlaxLlamaBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaBlock(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxLlamaAttention\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = nn_partitioning.remat(\n                FlaxLlamaAttention, static_argnums=(1, 3, 4, 6, 7, 8),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxLlamaMLP\n\n        if self.config.gradient_checkpointing != \"\":\n            mlp_block = nn_partitioning.remat(\n                FlaxLlamaMLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.post_attention_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        Args:\n            self: Refer to the class instance itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency information\n            attention_mask: chex.Array: Mask out the attention weights\n                for padding tokens\n            position_ids: chex.Array: Determine the position of each\n                token in the sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Control whether the dropout is applied\n                or not\n            init_cache: bool: Initialize the cache in the attention\n                layer\n            output_attentions: bool: Return the attention weights\n            fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n\n        Returns:\n            A tuple of two items\n        \"\"\"\n        attn_outputs = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n        if self.config.use_scan_mlp:\n            feed_forward_hidden_states = block_wise_ffn(\n                self.mlp,\n                feed_forward_input,\n                self.config.scan_mlp_chunk_size,\n                deterministic,\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaBlock.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <p>:param : Control the dropout in the self attention layer</p> <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask: Optional[jnp.ndarray] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    Args:\n        self: Refer to the class instance itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency information\n        attention_mask: chex.Array: Mask out the attention weights\n            for padding tokens\n        position_ids: chex.Array: Determine the position of each\n            token in the sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Control whether the dropout is applied\n            or not\n        init_cache: bool: Initialize the cache in the attention\n            layer\n        output_attentions: bool: Return the attention weights\n        fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n\n    Returns:\n        A tuple of two items\n    \"\"\"\n    attn_outputs = self.self_attn(\n        self.input_layernorm(hidden_states),\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n    hidden_states = hidden_states + attn_output\n\n    feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n    if self.config.use_scan_mlp:\n        feed_forward_hidden_states = block_wise_ffn(\n            self.mlp,\n            feed_forward_input,\n            self.config.scan_mlp_chunk_size,\n            deterministic,\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n\n    return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaBlockCollection","title":"<code>FlaxLlamaBlockCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaBlockCollection(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxLlamaBlock(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model\n         in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module,\n        and return all outputs that are computed by this module.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the input tensor to the\n                encoder\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency of each token\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Determine whether the model is in\n                training or evaluation mode\n            init_cache: bool: Initialize the cache for each layer\n            output_attentions: bool: Determine whether to output the\n                attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states of each layer\n            return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n\n        Returns:\n            A tuple of 3 values\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaBlockCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model  in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <p>:param : Determine whether to use the forgetful causal mask</p> <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model\n     in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module,\n    and return all outputs that are computed by this module.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the input tensor to the\n            encoder\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency of each token\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Determine whether the model is in\n            training or evaluation mode\n        init_cache: bool: Initialize the cache for each layer\n        output_attentions: bool: Determine whether to output the\n            attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states of each layer\n        return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n\n    Returns:\n        A tuple of 3 values\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += (layer_outputs[1],)\n\n    outputs = (hidden_states, all_hidden_states, all_attentions)\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLM","title":"<code>FlaxLlamaForCausalLM</code>","text":"<p>               Bases: <code>FlaxLlamaPreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForCausalLM(FlaxLlamaPreTrainedModel):\n    module_class = FlaxLlamaForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLMModule","title":"<code>FlaxLlamaForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForCausalLMModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxLlamaModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass the input token ids to the model\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the input sequence\n            deterministic: bool: Control whether the model is trained or\n                not\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states\n            return_dict: bool: Return a dictionary of the outputs or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the word that we want to predict\n            None]]: Pass in the extra embedding\n\n        Returns:\n            The logits and the hidden states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass the input token ids to the model\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the input sequence\n        deterministic: bool: Control whether the model is trained or\n            not\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states\n        return_dict: bool: Return a dictionary of the outputs or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the word that we want to predict\n        None]]: Pass in the extra embedding\n\n    Returns:\n        The logits and the hidden states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule","title":"<code>FlaxLlamaForSequenceClassificationModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        Args:\n            self: Access variables that belong to the class\n\n        Returns:\n            A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxLlamaModule(self.config, dtype=self.dtype)\n        self.classifier = Linear(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n            &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        Args:\n            self: Refer to the class instance\n            input_ids: chex.Array: Pass the input to the model\n            attention_mask: chex.Array: Specify which tokens are masked\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Control whether the model is run in\n                deterministic or stochastic mode\n            init_cache: bool: Initialize the cache for the transformer\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of outputs\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of a new word\n            None]]: Pass the extra embedding to the model\n\n        Returns:\n            A tuple of logits and hidden_states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in all the inputs to the model and returns all outputs from it. The call function can be called directly on an instance of a class, or by using parentheses after an instance:     &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class     &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to call</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Specify which tokens are masked</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is run in deterministic or stochastic mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the transformer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of outputs</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of a new word</p> <code>None</code> <code>None]]</code> <p>Pass the extra embedding to the model</p> required <p>Returns:</p> Type Description <p>A tuple of logits and hidden_states</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module.\n    It takes in all the inputs to the model and returns all outputs from it.\n    The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n        &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n        &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n    Args:\n        self: Refer to the class instance\n        input_ids: chex.Array: Pass the input to the model\n        attention_mask: chex.Array: Specify which tokens are masked\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Control whether the model is run in\n            deterministic or stochastic mode\n        init_cache: bool: Initialize the cache for the transformer\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of outputs\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of a new word\n        None]]: Pass the extra embedding to the model\n\n    Returns:\n        A tuple of logits and hidden_states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n    prediction = self.classifier(hidden_states)\n    if return_dict:\n        return FlaxSequenceClassifierOutput(\n            logits=prediction,\n            hidden_states=hidden_states\n        )\n    else:\n        return prediction,\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule.setup","title":"<code>setup()</code>","text":"<p>The setup function is called once at the beginning of training. It initializes the model and optimizer, and sets up any other state that needs to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <p>Returns:</p> Type Description <p>A tuple of the model and the classifier</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def setup(self):\n    \"\"\"The setup function is called once at the beginning of training.\n    It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n    Args:\n        self: Access variables that belong to the class\n\n    Returns:\n        A tuple of the model and the classifier\n    \"\"\"\n    self.model = FlaxLlamaModule(self.config, dtype=self.dtype)\n    self.classifier = Linear(\n        self.num_classes,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        use_bias=False,\n        kernel_init=jax.nn.initializers.normal(\n            stddev=self.config.initializer_range),\n        precision=self.precision,\n    )\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaMLP","title":"<code>FlaxLlamaMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaMLP(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.gate_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.dropout = flax.linen.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        x = self.dropout(x, deterministic=deterministic)\n        return x\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n    x = self.dropout(x, deterministic=deterministic)\n    return x\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaModule","title":"<code>FlaxLlamaModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.dropout = flax.linen.Dropout(rate=self.config.embd_pdrop)\n        self.layers = FlaxLlamaBlockCollection(self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                                               precision=self.precision)\n        self.norm = RMSNorm(self.config.hidden_size, eps=self.config.rms_norm_eps, dtype=self.dtype,\n                            param_dtype=self.param_dtype)\n        config = self.config\n        self.causal_mask = flax.linen.make_causal_mask(\n            jnp.ones(\n                (1, getattr(self.config, \"c_max_position_embeddings\", self.config.max_position_embeddings)),\n                dtype=\"bool\"\n            ), dtype=\"bool\"\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if config.rope_scaling is not None:\n            scaling_type = config.rope_scaling[\"type\"]\n            scaling_factor = config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=(\n                getattr(self.config, \"freq_max_position_embeddings\", self.config.max_position_embeddings)\n            ),\n            dim=config.hidden_size // config.num_attention_heads,\n            base=config.rope_theta,\n            **initial_rope_kwargs\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input token ids\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in a sequence\n            deterministic: bool: Control whether dropout is applied or\n                not\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attentions or not\n            output_hidden_states: bool: Determine whether to return\n                hidden states\n            return_dict: bool: Return a dictionary of the output or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the\n            None]]: Pass in the extra embedding\n\n        Returns:\n            A tuple of:\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length, _ = inputs_embeds.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n        inputs_embeds = inputs_embeds + \\\n                        extra_embedding if extra_embedding is not None else inputs_embeds\n        hidden_states = self.dropout(\n            inputs_embeds, deterministic=deterministic)\n\n        outputs = self.layers(\n            hidden_states=hidden_states,\n            freq_cis=self.freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids and returns the output of the model. The call function also has optional arguments that can be used to control the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when calling a Flax model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input token ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether dropout is applied or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attentions or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the output or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>A tuple of:</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n    and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n    the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n    calling a Flax model.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input token ids\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in a sequence\n        deterministic: bool: Control whether dropout is applied or\n            not\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attentions or not\n        output_hidden_states: bool: Determine whether to return\n            hidden states\n        return_dict: bool: Return a dictionary of the output or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the\n        None]]: Pass in the extra embedding\n\n    Returns:\n        A tuple of:\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n    batch_size, sequence_length, _ = inputs_embeds.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n    inputs_embeds = inputs_embeds + \\\n                    extra_embedding if extra_embedding is not None else inputs_embeds\n    hidden_states = self.dropout(\n        inputs_embeds, deterministic=deterministic)\n\n    outputs = self.layers(\n        hidden_states=hidden_states,\n        freq_cis=self.freq_cis,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        causal_mask=self.causal_mask,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(v for v in outputs if v is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel","title":"<code>FlaxLlamaPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: LlamaConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: LlamaConfig: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the input\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of layers in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape,\n                         seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input\n            position_ids: chex.Array: Create the positional embeddings\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass in the past key values from a\n                previous call to __call__\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all layers\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, extra_embedding=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input\n        position_ids: chex.Array: Create the positional embeddings\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass in the past key values from a\n            previous call to __call__\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all layers\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    if self.config.bits is not None:\n        rngs['params'] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>LlamaConfig</code> <p>LlamaConfig: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of layers in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __init__(\n        self,\n        config: LlamaConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: LlamaConfig: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the input\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of layers in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape,\n                     seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-llama-modelling_llama_flax/#src.python.easydel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/llama/modelling_llama_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-llama-modelling_vision_llama_flax/","title":"modules.llama.modelling_vision_llama_flax","text":""},{"location":"generated-modules-llama-modelling_vision_llama_flax/#src.python.easydel.modules.llama.modelling_vision_llama_flax.FlaxVisionLlamaPreTrainedModel","title":"<code>FlaxVisionLlamaPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/llama/modelling_vision_llama_flax.py</code> <pre><code>class FlaxVisionLlamaPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = VisionLlamaConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: VisionLlamaConfig,\n            input_shape: Tuple = (4, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_cache(self, batch_size, max_length):\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n        vision_mask = jnp.ones((batch_size, max_length), dtype=bool)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, vision_mask, attention_mask, position_ids,\n            return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Specify the shape of the input tensor\n        :param params: FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        vision_mask = jnp.ones(input_ids.shape, dtype=bool)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n\n        random_params = self.module.init(\n            {\n                \"params\": params_rng,\n                \"dropout\": dropout_rng\n            },\n            input_ids,\n            vision_mask,\n            attention_mask,\n            position_ids,\n            return_dict=False\n        )[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            vision_mask: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(vision_mask, dtype=\"f4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-llama-modelling_vision_llama_flax/#src.python.easydel.modules.llama.modelling_vision_llama_flax.FlaxVisionLlamaPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>:param self: Access variables that belong to the class :param rng: jax.random.PRNGKey: Initialize the weights of the model :param input_shape: Tuple: Specify the shape of the input tensor :param params: FrozenDict: Pass in the parameters of a pre-trained model :return: A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/llama/modelling_vision_llama_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"\n    The init_weights function is used to initialize the weights of a model.\n\n    :param self: Access variables that belong to the class\n    :param rng: jax.random.PRNGKey: Initialize the weights of the model\n    :param input_shape: Tuple: Specify the shape of the input tensor\n    :param params: FrozenDict: Pass in the parameters of a pre-trained model\n    :return: A frozendict of parameters\n\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    vision_mask = jnp.ones(input_ids.shape, dtype=bool)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n\n    random_params = self.module.init(\n        {\n            \"params\": params_rng,\n            \"dropout\": dropout_rng\n        },\n        input_ids,\n        vision_mask,\n        attention_mask,\n        position_ids,\n        return_dict=False\n    )[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-llama-vision_llama_configuration/","title":"modules.llama.vision_llama_configuration","text":""},{"location":"generated-modules-llama-vision_llama_configuration/#src.python.easydel.modules.llama.vision_llama_configuration.VisionLlamaConfig","title":"<code>VisionLlamaConfig</code>","text":"<p>               Bases: <code>LlamaConfig</code></p> Source code in <code>src/python/easydel/modules/llama/vision_llama_configuration.py</code> <pre><code>class VisionLlamaConfig(LlamaConfig):\n    def __init__(\n            self,\n            vision_vocab_size=8448,\n            tie_vision_embeddings=False,\n            sample_mode=\"all\",\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.vision_vocab_size = vision_vocab_size\n        self.tie_vision_embeddings = tie_vision_embeddings\n        self.sample_mode = sample_mode\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"model/embed_vision/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"model/embed_vision/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n</code></pre>"},{"location":"generated-modules-llama-vision_llama_configuration/#src.python.easydel.modules.llama.vision_llama_configuration.VisionLlamaConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/llama/vision_llama_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"model/embed_vision/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"model/embed_vision/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-lucid_transformer-lt_configuration/","title":"modules.lucid_transformer.lt_configuration","text":""},{"location":"generated-modules-lucid_transformer-modelling_lt_flax/","title":"modules.lucid_transformer.modelling_lt_flax","text":""},{"location":"generated-modules-mamba-mamba_configuration/","title":"modules.mamba.mamba_configuration","text":""},{"location":"generated-modules-mamba-modelling_mamba_flax/","title":"modules.mamba.modelling_mamba_flax","text":""},{"location":"generated-modules-mamba-modelling_mamba_flax/#src.python.easydel.modules.mamba.modelling_mamba_flax.FlaxMambaPretrainedModel","title":"<code>FlaxMambaPretrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/mamba/modelling_mamba_flax.py</code> <pre><code>class FlaxMambaPretrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = MambaConfig\n    base_model_prefix = \"backbone\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: MambaConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[Union[str, lax.Precision]] = None,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: MambaConfig: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the model ra\n            param_dtype: jnp.dtype: Specify the data type of the\n                param_dtype\n            precision: Optional[Union[str, lax.Precision]]: precision\n                for model operations\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of layers in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n        super().__init__(\n            config,\n            module,\n            input_shape=(input_shape[0], 1),\n            seed=seed,\n            dtype=dtype,\n            _do_init=_do_init\n        )\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            return_dict=False\n        )\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        return None\n\n    def __call__(\n            self,\n            input_ids: Optional[chex.Array] = None,\n            inputs_embeds: Optional[chex.Array] = None,\n            cache_params: dict = None,\n            deterministic: bool = True,\n            params: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            attention_mask: Optional[chex.Array] = None,  # Ignored(we are using an SSM model not attention)\n            use_cache: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Optional[chex.Array]: Pass in the input tokens\n            inputs_embeds: Optional[chex.Array]: Pass in the embedded\n                tokens\n            cache_params: dict: Pass in the past cache_params from a\n                previous call to __call__\n            params: dict: Pass in the parameters of the model\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all layers\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n        if cache_params is not None:\n            assert isinstance(cache_params, FlaxMambaCache), f\"Wrong cache input_type of {type(cache_params)}\"\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        rngs[\"params\"] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n\n        # input_ids: Optional[chex.Array] = None,\n        # inputs_embeds: Optional[chex.Array] = None,\n        # cache_params: Optional[chex.Array] = None,\n        # deterministic: bool = True,\n        # use_cache: Optional[bool] = None,\n        # output_hidden_states: Optional[bool] = None,\n        # return_dict: Optional[bool] = None,\n\n        return self.module.apply(\n            inputs,\n            input_ids,\n            inputs_embeds,\n            cache_params,\n            train,\n            use_cache,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=False,\n        )\n</code></pre>"},{"location":"generated-modules-mamba-modelling_mamba_flax/#src.python.easydel.modules.mamba.modelling_mamba_flax.FlaxMambaPretrainedModel.__call__","title":"<code>__call__(input_ids=None, inputs_embeds=None, cache_params=None, deterministic=True, params=None, dropout_rng=None, train=False, output_hidden_states=None, return_dict=None, extra_embedding=None, add_params_field=False, attention_mask=None, use_cache=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Pass in the input tokens</p> <code>None</code> <code>inputs_embeds</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Pass in the embedded tokens</p> <code>None</code> <code>cache_params</code> <code>dict</code> <p>dict: Pass in the past cache_params from a previous call to call</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/mamba/modelling_mamba_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: Optional[chex.Array] = None,\n        inputs_embeds: Optional[chex.Array] = None,\n        cache_params: dict = None,\n        deterministic: bool = True,\n        params: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        attention_mask: Optional[chex.Array] = None,  # Ignored(we are using an SSM model not attention)\n        use_cache: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Optional[chex.Array]: Pass in the input tokens\n        inputs_embeds: Optional[chex.Array]: Pass in the embedded\n            tokens\n        cache_params: dict: Pass in the past cache_params from a\n            previous call to __call__\n        params: dict: Pass in the parameters of the model\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all layers\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n    if cache_params is not None:\n        assert isinstance(cache_params, FlaxMambaCache), f\"Wrong cache input_type of {type(cache_params)}\"\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    rngs[\"params\"] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n\n    # input_ids: Optional[chex.Array] = None,\n    # inputs_embeds: Optional[chex.Array] = None,\n    # cache_params: Optional[chex.Array] = None,\n    # deterministic: bool = True,\n    # use_cache: Optional[bool] = None,\n    # output_hidden_states: Optional[bool] = None,\n    # return_dict: Optional[bool] = None,\n\n    return self.module.apply(\n        inputs,\n        input_ids,\n        inputs_embeds,\n        cache_params,\n        train,\n        use_cache,\n        output_hidden_states,\n        return_dict,\n        rngs=rngs,\n        mutable=False,\n    )\n</code></pre>"},{"location":"generated-modules-mamba-modelling_mamba_flax/#src.python.easydel.modules.mamba.modelling_mamba_flax.FlaxMambaPretrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, param_dtype=jnp.float32, precision=None, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>MambaConfig</code> <p>MambaConfig: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the model ra</p> <code>float32</code> <code>param_dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the param_dtype</p> <code>float32</code> <code>precision</code> <code>Optional[Union[str, Precision]]</code> <p>Optional[Union[str, lax.Precision]]: precision for model operations</p> <code>None</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of layers in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/mamba/modelling_mamba_flax.py</code> <pre><code>def __init__(\n        self,\n        config: MambaConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        param_dtype: jnp.dtype = jnp.float32,\n        precision: Optional[Union[str, lax.Precision]] = None,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: MambaConfig: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the model ra\n        param_dtype: jnp.dtype: Specify the data type of the\n            param_dtype\n        precision: Optional[Union[str, lax.Precision]]: precision\n            for model operations\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of layers in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(\n        config=config,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        precision=precision,\n        **kwargs\n    )\n    super().__init__(\n        config,\n        module,\n        input_shape=(input_shape[0], 1),\n        seed=seed,\n        dtype=dtype,\n        _do_init=_do_init\n    )\n</code></pre>"},{"location":"generated-modules-mamba-modelling_mamba_flax/#src.python.easydel.modules.mamba.modelling_mamba_flax.FlaxMambaPretrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/mamba/modelling_mamba_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    module_init_outputs = self.module.init(\n        rngs,\n        input_ids,\n        return_dict=False\n    )\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-mistral-mistral_configuration/","title":"modules.mistral.mistral_configuration","text":""},{"location":"generated-modules-mistral-mistral_configuration/#src.python.easydel.modules.mistral.mistral_configuration.MistralConfig","title":"<code>MistralConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/mistral/mistral_configuration.py</code> <pre><code>class MistralConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"mistral\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            sliding_window=4096,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attention_dropout: float = 0.0,\n            bits: Optional[int] = None,\n            attention_bias: bool = False,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        Args:\n            self: Represent the instance of the class\n            vocab_size: Define the size of the vocabulary\n            hidden_size: Determine the size of the embedding layers\n            intermediate_size: Define the size of the intermediate layer\n                in each transformer block\n            num_hidden_layers: Determine the number of layers in the\n                encoder and decoder\n            num_attention_heads: Determine the number of attention heads\n                in each layer\n            num_key_value_heads: Specify the number of heads for key and\n                value\n            hidden_act: Specify the activation function used in the\n                hidden layers\n            max_position_embeddings: Set the maximum length of the\n                sequence\n            initializer_range: Initialize the weights of the model\n            rms_norm_eps: Avoid division by zero in the rms\n                normalization\n            use_cache: Determine whether to use the cache in the decoder\n            pad_token_id: Specify the token id of the padding token\n            bos_token_id: Specify the beginning of sentence token id\n            eos_token_id: Specify the end of sentence token\n            tie_word_embeddings: Tie the word embeddings and the output\n                layer\n            rope_theta: Control the number of tokens in a rope\n            sliding_window: Control the number of tokens that are\n                processed in parallel\n            gradient_checkpointing: str: Specify whether to use gradient\n                checkpointing\n            use_scan_mlp: bool: Determine whether or not to use the\n                scan_mlp function\n            scan_mlp_chunk_size: int: Specify the chunk size of the scan\n                mlp\n            number_rep_kv: int: Specify the number of times to repeat\n                the key and value vectors\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            bits: Optional[int]: Specify the number of bits used for\n                quantization\n            axis_dims: Sequence[int]: Specify the dimension of each axis\n            axis_names: Sequence[str]: Specify the names of each axis in\n                the tensor\n            &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n            attention_bias: bool: when ever to use attention_bias\n            **kwargs: Pass a variable number of keyword arguments to a\n                function\n        :param : Define the number of layers in the model\n\n        Returns:\n            An instance of the class\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to use\n                the fully_sharded_data_parallel partitioning scheme or\n                not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        ) if not fully_sharded_data_parallel else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            attention_dropout: float = 0.0,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            attention_bias: bool = False,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the model:\n\n        Args:\n            self: Bind the attributes and methods of a class to an\n                instance of that class\n            gradient_checkpointing: str: Determine whether to use\n                gradient checkpointing\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or notn\n            scan_mlp_chunk_size: int: Chunk the input to the mlp\n            number_rep_kv: int: Control the number of times that the key\n                and value vectors are repeated\n            bits: Optional[int]: Specify the number of bits to use for\n                quantization\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            attention_bias: bool: when ever to use attention_bias\n            rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n                rope\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n\n        self.attention_bias = attention_bias\n        self.rope_scaling = rope_scaling\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attention_dropout = attention_dropout\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-mistral-mistral_configuration/#src.python.easydel.modules.mistral.mistral_configuration.MistralConfig.__init__","title":"<code>__init__(vocab_size=32000, hidden_size=4096, intermediate_size=14336, num_hidden_layers=32, num_attention_heads=32, num_key_value_heads=8, hidden_act='silu', max_position_embeddings=4096 * 32, initializer_range=0.02, rms_norm_eps=1e-06, use_cache=True, pad_token_id=None, bos_token_id=1, eos_token_id=2, tie_word_embeddings=False, rope_theta=10000.0, rope_scaling=None, sliding_window=4096, gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, number_rep_kv=1, attention_dropout=0.0, bits=None, attention_bias=False, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It allows the class to initialize the attributes of a class. The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>vocab_size</code> <p>Define the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <p>Determine the size of the embedding layers</p> <code>4096</code> <code>intermediate_size</code> <p>Define the size of the intermediate layer in each transformer block</p> <code>14336</code> <code>num_hidden_layers</code> <p>Determine the number of layers in the encoder and decoder</p> <code>32</code> <code>num_attention_heads</code> <p>Determine the number of attention heads in each layer</p> <code>32</code> <code>num_key_value_heads</code> <p>Specify the number of heads for key and value</p> <code>8</code> <code>hidden_act</code> <p>Specify the activation function used in the hidden layers</p> <code>'silu'</code> <code>max_position_embeddings</code> <p>Set the maximum length of the sequence</p> <code>4096 * 32</code> <code>initializer_range</code> <p>Initialize the weights of the model</p> <code>0.02</code> <code>rms_norm_eps</code> <p>Avoid division by zero in the rms normalization</p> <code>1e-06</code> <code>use_cache</code> <p>Determine whether to use the cache in the decoder</p> <code>True</code> <code>pad_token_id</code> <p>Specify the token id of the padding token</p> <code>None</code> <code>bos_token_id</code> <p>Specify the beginning of sentence token id</p> <code>1</code> <code>eos_token_id</code> <p>Specify the end of sentence token</p> <code>2</code> <code>tie_word_embeddings</code> <p>Tie the word embeddings and the output layer</p> <code>False</code> <code>rope_theta</code> <p>Control the number of tokens in a rope</p> <code>10000.0</code> <code>sliding_window</code> <p>Control the number of tokens that are processed in parallel</p> <code>4096</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether or not to use the scan_mlp function</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the scan mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Specify the number of times to repeat the key and value vectors</p> <code>1</code> <code>attention_dropout</code> <code>float</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used for quantization</p> <code>None</code> <code>axis_dims</code> <p>Sequence[int]: Specify the dimension of each axis</p> required <code>axis_names</code> <p>Sequence[str]: Specify the names of each axis in the tensor</p> required <code>&amp;quot;mp&amp;quot;)</code> <p>Define the maximum position embeddings</p> required <code>attention_bias</code> <code>bool</code> <p>bool: when ever to use attention_bias</p> <code>False</code> <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <p>:param : Define the number of layers in the model</p> <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>src/python/easydel/modules/mistral/mistral_configuration.py</code> <pre><code>def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=14336,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=8,\n        hidden_act=\"silu\",\n        max_position_embeddings=4096 * 32,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=None,\n        bos_token_id=1,\n        eos_token_id=2,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        sliding_window=4096,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        attention_dropout: float = 0.0,\n        bits: Optional[int] = None,\n        attention_bias: bool = False,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It allows the class to initialize the attributes of a class.\n    The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n    Args:\n        self: Represent the instance of the class\n        vocab_size: Define the size of the vocabulary\n        hidden_size: Determine the size of the embedding layers\n        intermediate_size: Define the size of the intermediate layer\n            in each transformer block\n        num_hidden_layers: Determine the number of layers in the\n            encoder and decoder\n        num_attention_heads: Determine the number of attention heads\n            in each layer\n        num_key_value_heads: Specify the number of heads for key and\n            value\n        hidden_act: Specify the activation function used in the\n            hidden layers\n        max_position_embeddings: Set the maximum length of the\n            sequence\n        initializer_range: Initialize the weights of the model\n        rms_norm_eps: Avoid division by zero in the rms\n            normalization\n        use_cache: Determine whether to use the cache in the decoder\n        pad_token_id: Specify the token id of the padding token\n        bos_token_id: Specify the beginning of sentence token id\n        eos_token_id: Specify the end of sentence token\n        tie_word_embeddings: Tie the word embeddings and the output\n            layer\n        rope_theta: Control the number of tokens in a rope\n        sliding_window: Control the number of tokens that are\n            processed in parallel\n        gradient_checkpointing: str: Specify whether to use gradient\n            checkpointing\n        use_scan_mlp: bool: Determine whether or not to use the\n            scan_mlp function\n        scan_mlp_chunk_size: int: Specify the chunk size of the scan\n            mlp\n        number_rep_kv: int: Specify the number of times to repeat\n            the key and value vectors\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        bits: Optional[int]: Specify the number of bits used for\n            quantization\n        axis_dims: Sequence[int]: Specify the dimension of each axis\n        axis_names: Sequence[str]: Specify the names of each axis in\n            the tensor\n        &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n        attention_bias: bool: when ever to use attention_bias\n        **kwargs: Pass a variable number of keyword arguments to a\n            function\n    :param : Define the number of layers in the model\n\n    Returns:\n        An instance of the class\n    \"\"\"\n    self.vocab_size = vocab_size\n    self.max_position_embeddings = max_position_embeddings\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.sliding_window = sliding_window\n    self.bits = bits\n    # for backward compatibility\n    if num_key_value_heads is None:\n        num_key_value_heads = num_attention_heads\n\n    self.num_key_value_heads = num_key_value_heads\n    self.hidden_act = hidden_act\n    self.initializer_range = initializer_range\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.rope_theta = rope_theta\n    self.rope_scaling = rope_scaling\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.attention_bias = attention_bias\n    self.attention_dropout = attention_dropout\n\n    super().__init__(\n        pad_token_id=pad_token_id,\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        use_scan_mlp=use_scan_mlp,\n        scan_mlp_chunk_size=scan_mlp_chunk_size,\n        bits=bits,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-modules-mistral-mistral_configuration/#src.python.easydel.modules.mistral.mistral_configuration.MistralConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, number_rep_kv=1, bits=None, attention_dropout=0.0, rope_scaling=None, attention_bias=False, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the attributes and methods of a class to an instance of that class</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Determine whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or notn</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Chunk the input to the mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Control the number of times that the key and value vectors are repeated</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits to use for quantization</p> <code>None</code> <code>attention_dropout</code> <code>float</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>attention_bias</code> <code>bool</code> <p>bool: when ever to use attention_bias</p> <code>False</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str, Union[str, float]]: rope_scaling for rope</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/mistral/mistral_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        bits: Optional[int] = None,\n        attention_dropout: float = 0.0,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        attention_bias: bool = False,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the model:\n\n    Args:\n        self: Bind the attributes and methods of a class to an\n            instance of that class\n        gradient_checkpointing: str: Determine whether to use\n            gradient checkpointing\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or notn\n        scan_mlp_chunk_size: int: Chunk the input to the mlp\n        number_rep_kv: int: Control the number of times that the key\n            and value vectors are repeated\n        bits: Optional[int]: Specify the number of bits to use for\n            quantization\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        attention_bias: bool: when ever to use attention_bias\n        rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n            rope\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n\n    self.attention_bias = attention_bias\n    self.rope_scaling = rope_scaling\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.attention_dropout = attention_dropout\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-mistral-mistral_configuration/#src.python.easydel.modules.mistral.mistral_configuration.MistralConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>  <code>staticmethod</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to use the fully_sharded_data_parallel partitioning scheme or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/mistral/mistral_configuration.py</code> <pre><code>@staticmethod\ndef get_partition_rules(fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to use\n            the fully_sharded_data_parallel partitioning scheme or\n            not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    ) if not fully_sharded_data_parallel else (\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/","title":"modules.mistral.modelling_mistral_flax","text":""},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralAttention","title":"<code>FlaxMistralAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralAttention(BaseJAXAttentionModule):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxMistralRotaryEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            attention_dropout=self.config.attention_dropout,\n            num_attention_heads=self.config.num_attention_heads,\n            head_dims=self.head_dim,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            base_module_class=self.config\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n        attention_mask = jnp.broadcast_to(\n            attention_mask, causal_mask.shape\n        )\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        outputs = (\n            attn_output, attentions.attention_weights\n        ) if output_attentions else (\n            attn_output,\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n    attention_mask = jnp.broadcast_to(\n        attention_mask, causal_mask.shape\n    )\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    outputs = (\n        attn_output, attentions.attention_weights\n    ) if output_attentions else (\n        attn_output,\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralDecoderLayer","title":"<code>FlaxMistralDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralDecoderLayer(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxMistralAttention\n        mlp_block = FlaxMistralMLP\n\n        if self.config.gradient_checkpointing != \"\":\n            # hidden_states: chex.Array,\n            # freq_cis: Tuple[chex.Array, chex.Array],\n            # attention_mask: chex.Array,\n            # position_ids: chex.Array,\n            # causal_mask: chex.Array,\n            # segment_ids: Optional[chex.Array] = None,\n            # deterministic: bool = True,\n            # init_cache: bool = False,\n            # output_attentions: bool = False,\n            # fcm_mask = None,\n            attn_block = re_mat(\n                attn_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(1, 3, 4, 6, 7, 8)\n            )\n            mlp_block = re_mat(\n                mlp_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(1,)\n            )\n        self.self_attn = attn_block(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = mlp_block(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n            by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,\n            used for computing self-attention weights and biases in a more efficient manner than using position\n            embeddings or sinusoidal positional encoding vectors would allow for [2].\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states and attention_output\n        \"\"\"\n\n        # hidden_states: chex.Array,\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array,\n        # position_ids: chex.Array,\n        # causal_mask: chex.Array,\n        # segment_ids: Optional[chex.Array] = None,\n        # deterministic: bool = True,\n        # init_cache: bool = False,\n        # output_attentions: bool = False,\n        # fcm_mask = None,\n        residual = hidden_states\n        attention_output = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            None\n        )\n\n        hidden_states = attention_output[0] + residual\n        ffd_inp = self.post_attention_layernorm(hidden_states)\n        if self.config.use_scan_mlp:\n            feed_forward_hidden_states = block_wise_ffn(\n                self.mlp,\n                ffd_inp,\n                self.config.scan_mlp_chunk_size,\n                deterministic,\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                ffd_inp,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += attention_output[1],\n        return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralDecoderLayer.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed     by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,     used for computing self-attention weights and biases in a more efficient manner than using position     embeddings or sinusoidal positional encoding vectors would allow for [2].</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states and attention_output</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed\n        by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,\n        used for computing self-attention weights and biases in a more efficient manner than using position\n        embeddings or sinusoidal positional encoding vectors would allow for [2].\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states and attention_output\n    \"\"\"\n\n    # hidden_states: chex.Array,\n    # freq_cis: Tuple[chex.Array, chex.Array],\n    # attention_mask: chex.Array,\n    # position_ids: chex.Array,\n    # causal_mask: chex.Array,\n    # segment_ids: Optional[chex.Array] = None,\n    # deterministic: bool = True,\n    # init_cache: bool = False,\n    # output_attentions: bool = False,\n    # fcm_mask = None,\n    residual = hidden_states\n    attention_output = self.self_attn(\n        self.input_layernorm(hidden_states),\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        None\n    )\n\n    hidden_states = attention_output[0] + residual\n    ffd_inp = self.post_attention_layernorm(hidden_states)\n    if self.config.use_scan_mlp:\n        feed_forward_hidden_states = block_wise_ffn(\n            self.mlp,\n            ffd_inp,\n            self.config.scan_mlp_chunk_size,\n            deterministic,\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            ffd_inp,\n            deterministic,\n        )\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += attention_output[1],\n    return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralForCausalLMModule","title":"<code>FlaxMistralForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralForCausalLMModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model: FlaxMistralModule = FlaxMistralModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n        and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n        as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n        the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n        output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Determine whether to use dropout in the\n                model\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of the outputs or\n                just the logits\n        :param : Determine whether to return the logits or not\n\n        Returns:\n            A tuple of (lm_logits, hidden_states, attentions)\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            inputs_embeds=inputs_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax module. It defines how the model will be called, and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask as inputs (these are defined in init). We also have some optional arguments that can be passed to the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings), output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout in the model</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or just the logits</p> <code>True</code> <p>:param : Determine whether to return the logits or not</p> <p>Returns:</p> Type Description <p>A tuple of (lm_logits, hidden_states, attentions)</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n    and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n    as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n    the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n    output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Determine whether to use dropout in the\n            model\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of the outputs or\n            just the logits\n    :param : Determine whether to return the logits or not\n\n    Returns:\n        A tuple of (lm_logits, hidden_states, attentions)\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        deterministic=deterministic,\n        inputs_embeds=inputs_embeds,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.transformer.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    # lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralModule","title":"<code>FlaxMistralModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMistralDecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = MistralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if self.config.rope_scaling is not None:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=(\n                getattr(self.config, \"freq_max_position_embeddings\", self.config.max_position_embeddings)\n            ),\n            dim=self.config.hidden_size // self.config.num_attention_heads,\n            base=self.config.rope_theta,\n            **initial_rope_kwargs\n        )\n        self.causal_mask = flax.linen.make_causal_mask(\n            jnp.ones(\n                (1, getattr(self.config, \"c_max_position_embeddings\", self.config.max_position_embeddings)),\n                dtype=\"bool\"\n            ), dtype=\"bool\"\n        )\n\n    def __call__(\n            self,\n            input_ids: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input ids\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain tokens\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            deterministic: bool: Determine whether to use dropout or not\n            inputs_embeds: chex.Array: Pass in the embedding of the\n                input_ids\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            output_hidden_states: bool: Return all hidden states or just\n                the last one\n            return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n\n        Returns:\n            A tuple of the hidden states, all hidden states, and\n            attentions\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_states=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralModule.__call__","title":"<code>__call__(input_ids=None, attention_mask=None, position_ids=None, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids as inputs to the model. The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Optional[Array]</code> <p>chex.Array: Pass in the input ids</p> <code>None</code> <code>attention_mask</code> <code>Optional[Array]</code> <p>chex.Array: Mask out the attention weights for certain tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>chex.Array: Determine the position of each token in a sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embedding of the input_ids</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return all hidden states or just the last one</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <p>:param : Determine whether the model is in training mode or not</p> <p>Returns:</p> Type Description <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>A tuple of the hidden states, all hidden states, and</p> <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>attentions</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: Optional[chex.Array] = None,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n    \"\"\"The __call__ function is the main function of a Flax model.\n    It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n    The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input ids\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain tokens\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        deterministic: bool: Determine whether to use dropout or not\n        inputs_embeds: chex.Array: Pass in the embedding of the\n            input_ids\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        output_hidden_states: bool: Return all hidden states or just\n            the last one\n        return_dict: bool: Return a dictionary of the outputs or not\n    :param : Determine whether the model is in training mode or not\n\n    Returns:\n        A tuple of the hidden states, all hidden states, and\n        attentions\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n    if attention_mask.ndim == 2:\n        b, s = attention_mask.shape\n        attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n    outputs = self.layers(\n        hidden_states=inputs_embeds,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        freq_cis=self.freq_cis,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        deterministic=deterministic,\n        causal_mask=self.causal_mask\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(value for value in outputs if value is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel","title":"<code>FlaxMistralPretrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralPretrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = MistralConfig\n    base_model_prefix = 'mistral'\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: MistralConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape,\n                         seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -&gt; flax.core.FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rng_s, input_ids, attention_mask, position_ids, return_dict=False\n            )\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0),\n            input_ids,\n            attention_mask,\n            position_ids,\n            return_dict=False,\n            init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        None,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in an rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: flax.core.FrozenDict = None\n) -&gt; flax.core.FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rng_s,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rng_s, input_ids, attention_mask, position_ids, return_dict=False\n        )\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-mistral-modelling_mistral_flax/#src.python.easydel.modules.mistral.modelling_mistral_flax.matmul_4d_loop","title":"<code>matmul_4d_loop(x, y)</code>","text":"<p>Computes the matrix product of two 4D arrays x and y using a loop.</p> Source code in <code>src/python/easydel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def matmul_4d_loop(x, y):\n    \"\"\"Computes the matrix product of two 4D arrays x and y using a loop.\"\"\"\n    result = jnp.zeros(*x.shape[:-2] + x.shape[-2] + y.shape[-1])\n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            for key in range(x.shape[2]):\n                for l in range(y.shape[3]):\n                    result[i, j, key, l] += x[i, j, key, :] * y[key, l, :, :]\n    return result\n</code></pre>"},{"location":"generated-modules-mistral-modelling_vision_mistral_flax/","title":"modules.mistral.modelling_vision_mistral_flax","text":""},{"location":"generated-modules-mistral-modelling_vision_mistral_flax/#src.python.easydel.modules.mistral.modelling_vision_mistral_flax.FlaxVisionMistralPreTrainedModel","title":"<code>FlaxVisionMistralPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/mistral/modelling_vision_mistral_flax.py</code> <pre><code>class FlaxVisionMistralPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = VisionMistralConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: VisionMistralConfig,\n            input_shape: Tuple = (4, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_cache(self, batch_size, max_length):\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n        vision_mask = jnp.ones((batch_size, max_length), dtype=bool)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, vision_mask, attention_mask, position_ids,\n            return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        vision_mask = jnp.ones(input_ids.shape, dtype=bool)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n\n        random_params = self.module.init(\n            {\n                \"params\": params_rng,\n                \"dropout\": dropout_rng\n            },\n            input_ids,\n            vision_mask,\n            attention_mask,\n            position_ids,\n            return_dict=False\n        )[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            vision_mask: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(vision_mask, dtype=\"f4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-mistral-modelling_vision_mistral_flax/#src.python.easydel.modules.mistral.modelling_vision_mistral_flax.FlaxVisionMistralPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/mistral/modelling_vision_mistral_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    vision_mask = jnp.ones(input_ids.shape, dtype=bool)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n\n    random_params = self.module.init(\n        {\n            \"params\": params_rng,\n            \"dropout\": dropout_rng\n        },\n        input_ids,\n        vision_mask,\n        attention_mask,\n        position_ids,\n        return_dict=False\n    )[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-mistral-vision_mistral_configuration/","title":"modules.mistral.vision_mistral_configuration","text":""},{"location":"generated-modules-mistral-vision_mistral_configuration/#src.python.easydel.modules.mistral.vision_mistral_configuration.VisionMistralConfig","title":"<code>VisionMistralConfig</code>","text":"<p>               Bases: <code>MistralConfig</code></p> Source code in <code>src/python/easydel/modules/mistral/vision_mistral_configuration.py</code> <pre><code>class VisionMistralConfig(MistralConfig):\n    def __init__(\n            self,\n            vision_vocab_size=8448,\n            tie_vision_embeddings=False,\n            sample_mode=\"all\",\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.vision_vocab_size = vision_vocab_size\n        self.tie_vision_embeddings = tie_vision_embeddings\n        self.sample_mode = sample_mode\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"model/embed_vision/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"model/embed_vision/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n</code></pre>"},{"location":"generated-modules-mistral-vision_mistral_configuration/#src.python.easydel.modules.mistral.vision_mistral_configuration.VisionMistralConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/mistral/vision_mistral_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"model/embed_vision/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"model/embed_vision/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"vision_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-mixtral-mixtral_configuration/","title":"modules.mixtral.mixtral_configuration","text":""},{"location":"generated-modules-mixtral-mixtral_configuration/#src.python.easydel.modules.mixtral.mixtral_configuration.MixtralConfig","title":"<code>MixtralConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/mixtral/mixtral_configuration.py</code> <pre><code>class MixtralConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"mixtral\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-5,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=1e6,\n            sliding_window=4096,\n            attention_dropout=0.0,\n            num_experts_per_tok=2,\n            num_local_experts=8,\n            output_router_logits=False,\n            router_aux_loss_coef=0.001,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            attention_bias: bool = False,\n            initialization_of_moe: bool = False,\n            router_jitter_noise=0.0,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        Args:\n            self: Represent the instance of the class\n            vocab_size: Define the size of the vocabulary\n            hidden_size: Determine the size of the embedding layers\n            intermediate_size: Define the size of the intermediate layer\n                in each transformer block\n            num_hidden_layers: Determine the number of layers in the\n                encoder and decoder\n            num_attention_heads: Determine the number of attention heads\n                in each layer\n            num_key_value_heads: Specify the number of heads for key and\n                value\n            hidden_act: Specify the activation function used in the\n                hidden layers\n            max_position_embeddings: Set the maximum length of the\n                sequence\n            initializer_range: Initialize the weights of the model\n            rms_norm_eps: Avoid division by zero in the rms\n                normalization\n            use_cache: Determine whether to use the cache in the decoder\n            pad_token_id: Specify the token id of the padding token\n            bos_token_id: Specify the beginning of sentence token id\n            eos_token_id: Specify the end of sentence token\n            tie_word_embeddings: Tie the word embeddings and the output\n                layer\n            rope_theta: Control the number of tokens in a rope\n            sliding_window: Control the number of tokens that are\n                processed in parallel\n            gradient_checkpointing: str: Specify whether to use gradient\n                checkpointing\n            use_scan_mlp: bool: Determine whether or not to use the\n                scan_mlp function\n            scan_mlp_chunk_size: int: Specify the chunk size of the scan\n                mlp\n            number_rep_kv: int: Specify the number of times to repeat\n                the key and value vectors\n            bits: Optional[int]: Specify the number of bits used for\n                quantization\n            axis_dims: Sequence[int]: Specify the dimension of each axis\n            axis_names: Sequence[str]: Specify the names of each axis in\n                the tensor\n            &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n            **kwargs: Pass a variable number of keyword arguments to a\n                function\n            rope_scaling: Dict[str, Union[str, float]]: rope scaling\n                information\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            initialization_of_moe: bool: initialization of moe needs to\n                disable some dynamic part's this boolean variable will\n                turn them off.\n            attention_bias: bool: when ever to use attention_bias\n        :param : Define the number of layers in the model\n\n        Returns:\n            An instance of the class\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        self.attention_dropout = attention_dropout\n        self.num_local_experts = num_local_experts\n        self.num_experts_per_tok = num_experts_per_tok\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.attention_bias = attention_bias\n        # for backward compatibility\n        self.rope_scaling = rope_scaling\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initialization_of_moe = initialization_of_moe\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.router_jitter_noise = router_jitter_noise\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to use\n                the fully_sharded_data_parallel partitioning scheme or\n                not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            attention_dropout: float = 0.0,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            attention_bias: bool = False,\n            initialization_of_moe: bool = False,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the model:\n\n        Args:\n            self: Bind the attributes and methods of a class to an\n                instance of that class\n            gradient_checkpointing: str: Determine whether to use\n                gradient checkpointing\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or not\n            scan_mlp_chunk_size: int: Chunk the input to the mlp\n            number_rep_kv: int: Control the number of times that the key\n                and value vectors are repeated\n            bits: Optional[int]: Specify the number of bits to use for\n                quantization\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            attention_bias: bool: when ever to use attention_bias\n            initialization_of_moe: bool: initialization of moe needs to\n                disable some dynamic part's this boolean variable will\n                turn them off.\n            rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n                rope\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        self.attention_dropout = attention_dropout\n        self.attention_bias = attention_bias\n        self.rope_scaling = rope_scaling\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.initialization_of_moe = initialization_of_moe\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"generated-modules-mixtral-mixtral_configuration/#src.python.easydel.modules.mixtral.mixtral_configuration.MixtralConfig.__init__","title":"<code>__init__(vocab_size=32000, hidden_size=4096, intermediate_size=14336, num_hidden_layers=32, num_attention_heads=32, num_key_value_heads=8, hidden_act='silu', max_position_embeddings=4096 * 32, initializer_range=0.02, rms_norm_eps=1e-05, use_cache=True, pad_token_id=None, bos_token_id=1, eos_token_id=2, tie_word_embeddings=False, rope_theta=1000000.0, sliding_window=4096, attention_dropout=0.0, num_experts_per_tok=2, num_local_experts=8, output_router_logits=False, router_aux_loss_coef=0.001, gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, number_rep_kv=1, bits=None, rope_scaling=None, attention_bias=False, initialization_of_moe=False, router_jitter_noise=0.0, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It allows the class to initialize the attributes of a class. The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>vocab_size</code> <p>Define the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <p>Determine the size of the embedding layers</p> <code>4096</code> <code>intermediate_size</code> <p>Define the size of the intermediate layer in each transformer block</p> <code>14336</code> <code>num_hidden_layers</code> <p>Determine the number of layers in the encoder and decoder</p> <code>32</code> <code>num_attention_heads</code> <p>Determine the number of attention heads in each layer</p> <code>32</code> <code>num_key_value_heads</code> <p>Specify the number of heads for key and value</p> <code>8</code> <code>hidden_act</code> <p>Specify the activation function used in the hidden layers</p> <code>'silu'</code> <code>max_position_embeddings</code> <p>Set the maximum length of the sequence</p> <code>4096 * 32</code> <code>initializer_range</code> <p>Initialize the weights of the model</p> <code>0.02</code> <code>rms_norm_eps</code> <p>Avoid division by zero in the rms normalization</p> <code>1e-05</code> <code>use_cache</code> <p>Determine whether to use the cache in the decoder</p> <code>True</code> <code>pad_token_id</code> <p>Specify the token id of the padding token</p> <code>None</code> <code>bos_token_id</code> <p>Specify the beginning of sentence token id</p> <code>1</code> <code>eos_token_id</code> <p>Specify the end of sentence token</p> <code>2</code> <code>tie_word_embeddings</code> <p>Tie the word embeddings and the output layer</p> <code>False</code> <code>rope_theta</code> <p>Control the number of tokens in a rope</p> <code>1000000.0</code> <code>sliding_window</code> <p>Control the number of tokens that are processed in parallel</p> <code>4096</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether or not to use the scan_mlp function</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the scan mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Specify the number of times to repeat the key and value vectors</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used for quantization</p> <code>None</code> <code>axis_dims</code> <p>Sequence[int]: Specify the dimension of each axis</p> required <code>axis_names</code> <p>Sequence[str]: Specify the names of each axis in the tensor</p> required <code>&amp;quot;mp&amp;quot;)</code> <p>Define the maximum position embeddings</p> required <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str, Union[str, float]]: rope scaling information</p> <code>None</code> <code>attention_dropout</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>initialization_of_moe</code> <code>bool</code> <p>bool: initialization of moe needs to disable some dynamic part's this boolean variable will turn them off.</p> <code>False</code> <code>attention_bias</code> <code>bool</code> <p>bool: when ever to use attention_bias</p> <code>False</code> <p>:param : Define the number of layers in the model</p> <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>src/python/easydel/modules/mixtral/mixtral_configuration.py</code> <pre><code>def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=14336,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=8,\n        hidden_act=\"silu\",\n        max_position_embeddings=4096 * 32,\n        initializer_range=0.02,\n        rms_norm_eps=1e-5,\n        use_cache=True,\n        pad_token_id=None,\n        bos_token_id=1,\n        eos_token_id=2,\n        tie_word_embeddings=False,\n        rope_theta=1e6,\n        sliding_window=4096,\n        attention_dropout=0.0,\n        num_experts_per_tok=2,\n        num_local_experts=8,\n        output_router_logits=False,\n        router_aux_loss_coef=0.001,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        bits: Optional[int] = None,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        attention_bias: bool = False,\n        initialization_of_moe: bool = False,\n        router_jitter_noise=0.0,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It allows the class to initialize the attributes of a class.\n    The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n    Args:\n        self: Represent the instance of the class\n        vocab_size: Define the size of the vocabulary\n        hidden_size: Determine the size of the embedding layers\n        intermediate_size: Define the size of the intermediate layer\n            in each transformer block\n        num_hidden_layers: Determine the number of layers in the\n            encoder and decoder\n        num_attention_heads: Determine the number of attention heads\n            in each layer\n        num_key_value_heads: Specify the number of heads for key and\n            value\n        hidden_act: Specify the activation function used in the\n            hidden layers\n        max_position_embeddings: Set the maximum length of the\n            sequence\n        initializer_range: Initialize the weights of the model\n        rms_norm_eps: Avoid division by zero in the rms\n            normalization\n        use_cache: Determine whether to use the cache in the decoder\n        pad_token_id: Specify the token id of the padding token\n        bos_token_id: Specify the beginning of sentence token id\n        eos_token_id: Specify the end of sentence token\n        tie_word_embeddings: Tie the word embeddings and the output\n            layer\n        rope_theta: Control the number of tokens in a rope\n        sliding_window: Control the number of tokens that are\n            processed in parallel\n        gradient_checkpointing: str: Specify whether to use gradient\n            checkpointing\n        use_scan_mlp: bool: Determine whether or not to use the\n            scan_mlp function\n        scan_mlp_chunk_size: int: Specify the chunk size of the scan\n            mlp\n        number_rep_kv: int: Specify the number of times to repeat\n            the key and value vectors\n        bits: Optional[int]: Specify the number of bits used for\n            quantization\n        axis_dims: Sequence[int]: Specify the dimension of each axis\n        axis_names: Sequence[str]: Specify the names of each axis in\n            the tensor\n        &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n        **kwargs: Pass a variable number of keyword arguments to a\n            function\n        rope_scaling: Dict[str, Union[str, float]]: rope scaling\n            information\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        initialization_of_moe: bool: initialization of moe needs to\n            disable some dynamic part's this boolean variable will\n            turn them off.\n        attention_bias: bool: when ever to use attention_bias\n    :param : Define the number of layers in the model\n\n    Returns:\n        An instance of the class\n    \"\"\"\n    self.vocab_size = vocab_size\n    self.max_position_embeddings = max_position_embeddings\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.sliding_window = sliding_window\n    self.bits = bits\n    self.attention_dropout = attention_dropout\n    self.num_local_experts = num_local_experts\n    self.num_experts_per_tok = num_experts_per_tok\n    self.output_router_logits = output_router_logits\n    self.router_aux_loss_coef = router_aux_loss_coef\n    self.attention_bias = attention_bias\n    # for backward compatibility\n    self.rope_scaling = rope_scaling\n    if num_key_value_heads is None:\n        num_key_value_heads = num_attention_heads\n\n    self.num_key_value_heads = num_key_value_heads\n    self.hidden_act = hidden_act\n    self.initialization_of_moe = initialization_of_moe\n    self.initializer_range = initializer_range\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.rope_theta = rope_theta\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.router_jitter_noise = router_jitter_noise\n    super().__init__(\n        pad_token_id=pad_token_id,\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        use_scan_mlp=use_scan_mlp,\n        scan_mlp_chunk_size=scan_mlp_chunk_size,\n        bits=bits,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-modules-mixtral-mixtral_configuration/#src.python.easydel.modules.mixtral.mixtral_configuration.MixtralConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, number_rep_kv=1, bits=None, attention_dropout=0.0, rope_scaling=None, attention_bias=False, initialization_of_moe=False, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the attributes and methods of a class to an instance of that class</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Determine whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Chunk the input to the mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Control the number of times that the key and value vectors are repeated</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits to use for quantization</p> <code>None</code> <code>attention_dropout</code> <code>float</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>attention_bias</code> <code>bool</code> <p>bool: when ever to use attention_bias</p> <code>False</code> <code>initialization_of_moe</code> <code>bool</code> <p>bool: initialization of moe needs to disable some dynamic part's this boolean variable will turn them off.</p> <code>False</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str, Union[str, float]]: rope_scaling for rope</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/mixtral/mixtral_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        bits: Optional[int] = None,\n        attention_dropout: float = 0.0,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        attention_bias: bool = False,\n        initialization_of_moe: bool = False,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the model:\n\n    Args:\n        self: Bind the attributes and methods of a class to an\n            instance of that class\n        gradient_checkpointing: str: Determine whether to use\n            gradient checkpointing\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or not\n        scan_mlp_chunk_size: int: Chunk the input to the mlp\n        number_rep_kv: int: Control the number of times that the key\n            and value vectors are repeated\n        bits: Optional[int]: Specify the number of bits to use for\n            quantization\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        attention_bias: bool: when ever to use attention_bias\n        initialization_of_moe: bool: initialization of moe needs to\n            disable some dynamic part's this boolean variable will\n            turn them off.\n        rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n            rope\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    self.attention_dropout = attention_dropout\n    self.attention_bias = attention_bias\n    self.rope_scaling = rope_scaling\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n    self.initialization_of_moe = initialization_of_moe\n</code></pre>"},{"location":"generated-modules-mixtral-mixtral_configuration/#src.python.easydel.modules.mixtral.mixtral_configuration.MixtralConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to use the fully_sharded_data_parallel partitioning scheme or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/mixtral/mixtral_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to use\n            the fully_sharded_data_parallel partitioning scheme or\n            not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/","title":"modules.mixtral.modelling_mixtral_flax","text":""},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention","title":"<code>FlaxMixtralAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class FlaxMixtralAttention(BaseJAXAttentionModule):\n    config: MixtralConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            Linear,\n            use_bias=getattr(self.config, \"attention_bias\", False),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMixtralRotaryEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name\n        )\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length,\n                              self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length,\n                          self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length,\n                              self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        Args:\n            self: Refer to the object itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                model\n            freq_cis: Tuple[chex.Array, chex.Array],: Create the\n                apply_rotary variable\n            attention_mask: chex.Array: Mask the attention weights\n            causal_mask: chex.Array: Mask the attention weights\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights\n\n        Returns:\n            A tuple of (out, attn_output)\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n        outputs = (\n            attn_output, attentions.attention_weights\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice. The call method takes an input tensor (x) and returns an output tensor (y). In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the model</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Create the apply_rotary variable</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of (out, attn_output)</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n    The __call__ method takes an input tensor (x) and returns an output tensor (y).\n    In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n    Args:\n        self: Refer to the object itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            model\n        freq_cis: Tuple[chex.Array, chex.Array],: Create the\n            apply_rotary variable\n        attention_mask: chex.Array: Mask the attention weights\n        causal_mask: chex.Array: Mask the attention weights\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights\n\n    Returns:\n        A tuple of (out, attn_output)\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n    outputs = (\n        attn_output, attentions.attention_weights\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer","title":"<code>FlaxMixtralDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class FlaxMixtralDecoderLayer(nn.Module):\n    config: MixtralConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        # hidden_states: chex.Array\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array\n        # causal_mask: chex.Array\n        # position_ids: chex.Array\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = True\n\n        attn_block = FlaxMixtralAttention\n        mlp_block = FlaxMixtralSparseMoeBlock\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = re_mat(\n                attn_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    1, 3, 4, 6, 7, 8, 9\n                )\n            )\n            mlp_block = re_mat(\n                mlp_block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(\n                    1,\n                )\n            )\n        self.self_attn = attn_block(\n            config=self.config,\n            layer_index=self.layer_index,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.block_sparse_moe = mlp_block(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MixtralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MixtralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True,\n            output_router_logits: Optional[bool] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states and attention_output\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # hidden_states: chex.Array\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # attention_mask: chex.Array\n        # causal_mask: chex.Array\n        # position_ids: chex.Array\n        # segment_ids: Optional[chex.Array] = None\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = True\n\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states,\n            freq_cis,\n            attention_mask,\n            causal_mask,\n            position_ids,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions\n        )\n\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (self_attn_weights,)\n        if output_router_logits:\n            outputs += (router_logits,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, segment_ids=None, deterministic=True, init_cache=False, output_attentions=True, output_router_logits=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states and attention_output</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True,\n        output_router_logits: Optional[bool] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states and attention_output\n    \"\"\"\n    residual = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n\n    # hidden_states: chex.Array\n    # freq_cis: Tuple[chex.Array, chex.Array],\n    # attention_mask: chex.Array\n    # causal_mask: chex.Array\n    # position_ids: chex.Array\n    # segment_ids: Optional[chex.Array] = None\n    # deterministic: bool = True\n    # init_cache: bool = False\n    # output_attentions: bool = True\n\n    hidden_states, self_attn_weights = self.self_attn(\n        hidden_states,\n        freq_cis,\n        attention_mask,\n        causal_mask,\n        position_ids,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions\n    )\n\n    hidden_states = residual + hidden_states\n\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n    hidden_states = residual + hidden_states\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (self_attn_weights,)\n    if output_router_logits:\n        outputs += (router_logits,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection","title":"<code>FlaxMixtralDecoderLayerCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class FlaxMixtralDecoderLayerCollection(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.blocks = [\n            FlaxMixtralDecoderLayer(\n                layer_index=layer_index,\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(layer_index)\n            )\n\n            for layer_index in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_hidden_states: Optional[bool] = False,\n            output_attentions: Optional[bool] = False,\n            output_router_logits: Optional[bool] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Represent the input to the\n                encoder layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n                information to the attention layer\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain positions\n            causal_mask: chex.Array: Mask the future tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in the sequence\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache for the self-\n                attention layer\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n\n        Returns:\n            A tuple of hidden_states, attention_output,\n            all_hidden_states and all_router_logits\n        \"\"\"\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_router_logits = () if output_router_logits else None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                output_attentions=output_attentions,\n                output_router_logits=output_router_logits,\n                init_cache=init_cache,\n                freq_cis=freq_cis,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n            )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n            if output_router_logits:\n                all_router_logits += (layer_outputs[-1],)\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (all_self_attns,)\n        if output_hidden_states:\n            outputs += (all_hidden_states,)\n        if output_router_logits:\n            outputs += (all_router_logits,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, causal_mask, position_ids, deterministic=True, init_cache=False, output_hidden_states=False, output_attentions=False, output_router_logits=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self- attention layer</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of hidden_states, attention_output,</p> <p>all_hidden_states and all_router_logits</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_hidden_states: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        output_router_logits: Optional[bool] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Represent the input to the\n            encoder layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency\n            information to the attention layer\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain positions\n        causal_mask: chex.Array: Mask the future tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in the sequence\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache for the self-\n            attention layer\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n\n    Returns:\n        A tuple of hidden_states, attention_output,\n        all_hidden_states and all_router_logits\n    \"\"\"\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    all_router_logits = () if output_router_logits else None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            output_router_logits=output_router_logits,\n            init_cache=init_cache,\n            freq_cis=freq_cis,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n        )\n\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n\n        if output_router_logits:\n            all_router_logits += (layer_outputs[-1],)\n\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (all_self_attns,)\n    if output_hidden_states:\n        outputs += (all_hidden_states,)\n    if output_router_logits:\n        outputs += (all_router_logits,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM","title":"<code>FlaxMixtralForCausalLM</code>","text":"<p>               Bases: <code>MixtralPreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class FlaxMixtralForCausalLM(MixtralPreTrainedModel):\n    module_class = FlaxMixtralForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock","title":"<code>FlaxMixtralSparseMoeBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>This implementation is strictly equivalent to standard MoE with full capacity (no dropped tokens). It's faster since it formulates MoE operations in terms of block-sparse operations to accomodate imbalanced assignments of tokens to experts, whereas standard MoE either (1) drop tokens at the cost of reduced performance or (2) set capacity factor to number of experts and thus waste computation and memory on padding.</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class FlaxMixtralSparseMoeBlock(nn.Module):\n    \"\"\"This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[\n        Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.gate = Linear(\n            self.config.num_local_experts,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n        )\n\n        self.experts = FlaxMixtralBlocKSparesTop2MLPCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            e: bool = False  # Ignored\n    ) -&gt; Tuple[chex.Array, chex.Array]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n\n        router_logits = self.gate(hidden_states).astype(  # no reshaping is needed\n            jnp.promote_types(self.dtype, jnp.float32)\n        )\n        routing_weights, selected_experts = jax.lax.top_k(\n            router_logits,\n            k=self.config.num_experts_per_tok\n        )\n        routing_weights = jax.nn.softmax(\n            routing_weights.astype(\n                jnp.promote_types(self.dtype, jnp.float32)\n            ), axis=-1\n        )\n\n        return self.experts(\n            selected_experts=selected_experts,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            hidden_dim=hidden_dim,\n            hidden_states=hidden_states,\n            routing_weights=routing_weights\n        ), router_logits\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel","title":"<code>MixtralPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>class MixtralPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class: MixtralConfig = MixtralConfig\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    # main_input_name = \"input_ids\"\n\n    def __init__(\n            self,\n            config: MixtralConfig,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\n                \"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: FrozenDict = None\n    ) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n\n        self.config.initialization_of_moe = True\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        self.config.initialization_of_moe = False\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            # output_router_logits: Optional[bool] = None\n            output_router_logits,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n        # attention_mask: Optional[chex.Array] = None\n        jnp.array(attention_mask, dtype=\"i4\"),\n        # position_ids: Optional[chex.Array] = None\n        jnp.array(position_ids, dtype=\"i4\"),\n        None,  # inputs_embeds: Optional[chex.Array] = None\n        output_attentions,  # output_attentions: Optional[bool] = None\n        # output_hidden_states: Optional[bool] = None\n        output_hidden_states,\n        # output_router_logits: Optional[bool] = None\n        output_router_logits,\n        False,  # init_cache: bool = False\n        not train,  # deterministic: bool = True\n        return_dict,  # return_dict: bool = True\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-mixtral-modelling_mixtral_flax/#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: FrozenDict = None\n) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n\n    self.config.initialization_of_moe = True\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n        input_shape,\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False\n        )\n    random_params = module_init_outputs[\"params\"]\n\n    self.config.initialization_of_moe = False\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-mosaic_mpt-modelling_mpt_flax/","title":"modules.mosaic_mpt.modelling_mpt_flax","text":""},{"location":"generated-modules-mosaic_mpt-modelling_mpt_flax/#src.python.easydel.modules.mosaic_mpt.modelling_mpt_flax.FlaxMptAttention","title":"<code>FlaxMptAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/mosaic_mpt/modelling_mpt_flax.py</code> <pre><code>class FlaxMptAttention(BaseJAXAttentionModule):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n\n        self.Wqkv = Linear(\n            self.config.hidden_size * 3,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            use_bias=self.config.use_bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision)\n        self.out_proj = Linear(\n            self.config.hidden_size,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            use_bias=self.config.use_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.dropout = nn.Dropout(self.config.attn_config.attn_pdrop)\n\n        self.hidden_size = self.config.hidden_size\n        self.n_heads = self.config.n_heads\n        self.max_seq_length = self.config.max_seq_len\n        self.head_dim = self.hidden_size // self.n_heads\n        self.softmax_scale = self.config.attn_config.softmax_scale\n        if self.softmax_scale is None:\n            self.softmax_scale = 1 / math.sqrt(self.hidden_size / self.n_heads)\n\n        self.attention_performer = AttentionModule(\n            attention_dropout=self.config.attn_config.attn_pdrop,\n            num_attention_heads=self.config.num_attention_heads,\n            head_dims=self.head_dim,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            base_module_class=self.config\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            attention_mask: chex.Array,\n            position_bias: chex.Array,\n            causal_mask: chex.Array,\n            init_cache: bool = False,\n            deterministic: bool = False\n    ):\n\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, just like any other Python function.\n        The difference is that __call__ can also take in state (e.g., parameters) from the module itself,\n        and it can update that state as part of its computation.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the input to the attention\n                layer\n            attention_mask: chex.Array: Mask out certain positions in\n                the sequence\n            position_bias: chex.Array: Add a bias to the attention\n                scores\n            causal_mask: chex.Array: Mask out certain positions in the\n                sequence\n            init_cache: bool: Initialize the cache\n            deterministic: bool: deterministic to activate dropouts and\n                detect training process\n\n        Returns:\n            The output of the attention layer\n        \"\"\"\n        inp_shape = hidden_states.shape\n        mixed_qkv = self.Wqkv(hidden_states)\n        query_states, key_states, value_states = jnp.split(mixed_qkv, 3, -1)\n\n        query_states = rearrange(query_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n        key_states = rearrange(key_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n        value_states = rearrange(value_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        if position_bias is not None:\n            key_length = key_states.shape[1]\n\n            position_bias_query_index = max(0, position_bias.shape[2] - query_length)\n            position_bias_key_index = max(0, position_bias.shape[3] - key_length)\n\n            position_bias = position_bias[:, :, position_bias_query_index:, position_bias_key_index:]\n        attention_mask = attention_mask.repeat(position_bias.shape[1], 1)\n        attention_bias = lax.select(\n            attention_mask.astype(\"bool\"),\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype) + position_bias.astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        attention = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            causal_mask=causal_mask,\n            attention_mask=attention_mask,\n            deterministic=deterministic,\n            segment_ids=None,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            bias=attention_bias,\n            causal=False,\n        )\n\n        attn_output = self.out_proj(attention.attention_outputs.reshape(inp_shape))\n\n        return attn_output, attention.attention_weights\n</code></pre>"},{"location":"generated-modules-mosaic_mpt-modelling_mpt_flax/#src.python.easydel.modules.mosaic_mpt.modelling_mpt_flax.FlaxMptAttention.__call__","title":"<code>__call__(hidden_states, attention_mask, position_bias, causal_mask, init_cache=False, deterministic=False)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, just like any other Python function. The difference is that call can also take in state (e.g., parameters) from the module itself, and it can update that state as part of its computation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain positions in the sequence</p> required <code>position_bias</code> <code>Array</code> <p>chex.Array: Add a bias to the attention scores</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out certain positions in the sequence</p> required <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>deterministic</code> <code>bool</code> <p>bool: deterministic to activate dropouts and detect training process</p> <code>False</code> <p>Returns:</p> Type Description <p>The output of the attention layer</p> Source code in <code>src/python/easydel/modules/mosaic_mpt/modelling_mpt_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        attention_mask: chex.Array,\n        position_bias: chex.Array,\n        causal_mask: chex.Array,\n        init_cache: bool = False,\n        deterministic: bool = False\n):\n\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, just like any other Python function.\n    The difference is that __call__ can also take in state (e.g., parameters) from the module itself,\n    and it can update that state as part of its computation.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the input to the attention\n            layer\n        attention_mask: chex.Array: Mask out certain positions in\n            the sequence\n        position_bias: chex.Array: Add a bias to the attention\n            scores\n        causal_mask: chex.Array: Mask out certain positions in the\n            sequence\n        init_cache: bool: Initialize the cache\n        deterministic: bool: deterministic to activate dropouts and\n            detect training process\n\n    Returns:\n        The output of the attention layer\n    \"\"\"\n    inp_shape = hidden_states.shape\n    mixed_qkv = self.Wqkv(hidden_states)\n    query_states, key_states, value_states = jnp.split(mixed_qkv, 3, -1)\n\n    query_states = rearrange(query_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n    key_states = rearrange(key_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n    value_states = rearrange(value_states, \"b s (h d) -&gt; b s h d\", h=self.config.n_heads)\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    if position_bias is not None:\n        key_length = key_states.shape[1]\n\n        position_bias_query_index = max(0, position_bias.shape[2] - query_length)\n        position_bias_key_index = max(0, position_bias.shape[3] - key_length)\n\n        position_bias = position_bias[:, :, position_bias_query_index:, position_bias_key_index:]\n    attention_mask = attention_mask.repeat(position_bias.shape[1], 1)\n    attention_bias = lax.select(\n        attention_mask.astype(\"bool\"),\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype) + position_bias.astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    attention = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        causal_mask=causal_mask,\n        attention_mask=attention_mask,\n        deterministic=deterministic,\n        segment_ids=None,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        bias=attention_bias,\n        causal=False,\n    )\n\n    attn_output = self.out_proj(attention.attention_outputs.reshape(inp_shape))\n\n    return attn_output, attention.attention_weights\n</code></pre>"},{"location":"generated-modules-mosaic_mpt-mosaic_configuration/","title":"modules.mosaic_mpt.mosaic_configuration","text":""},{"location":"generated-modules-olmo-modelling_olmo_flax/","title":"modules.olmo.modelling_olmo_flax","text":""},{"location":"generated-modules-olmo-olmo_configuration/","title":"modules.olmo.olmo_configuration","text":""},{"location":"generated-modules-olmo-olmo_configuration/#src.python.easydel.modules.olmo.olmo_configuration.OLMoConfig","title":"<code>OLMoConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> <p>OLMo (model) configuration.</p> Source code in <code>src/python/easydel/modules/olmo/olmo_configuration.py</code> <pre><code>class OLMoConfig(EasyDeLPretrainedConfig):\n    \"\"\"OLMo (model) configuration.\"\"\"\n\n    def __init__(\n            self,\n            d_model: int = 768,\n            n_heads: int = 12,\n            n_layers: int = 12,\n            mlp_ratio: int = 4,\n            mlp_hidden_size: Optional[int] = None,\n            activation_type: ActivationType = ActivationType.swiglu,\n            block_type: BlockType = BlockType.sequential,\n            block_group_size: int = 1,\n            alibi: bool = False,\n            alibi_bias_max: float = 8.0,\n            rope: bool = False,\n            rope_full_precision: bool = True,\n            flash_attention: bool = False,\n            attention_dropout: float = 0.1,\n            multi_query_attention: bool = False,\n            attention_layer_norm: bool = False,\n            residual_dropout: float = 0.1,\n            embedding_dropout: float = 0.1,\n            layer_norm_type: LayerNormType = LayerNormType.default,\n            layer_norm_with_affine: bool = True,\n            attention_layer_norm_with_affine: bool = True,\n            max_sequence_length: int = 1024,\n            include_bias: bool = True,\n            bias_for_layer_norm: Optional[bool] = None,\n            scale_logits: bool = False,\n            vocab_size: int = 50257,\n            embedding_size: Optional[int] = 50304,\n            weight_tying: bool = True,\n            eos_token_id: int = 50256,\n            pad_token_id: int = 50256,\n            init_std: float = 0.02,\n            init_cutoff_factor: Optional[float] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        _ = kwargs.pop(\"precision\", None)\n        _ = kwargs.pop(\"init_fn\", None)\n        _ = kwargs.pop(\"init_device\", None)\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.mlp_ratio = mlp_ratio\n        self.mlp_hidden_size = mlp_hidden_size\n        self.activation_type = activation_type\n        self.block_type = block_type\n        self.block_group_size = block_group_size\n        self.alibi = alibi\n        self.alibi_bias_max = alibi_bias_max\n        self.rope = rope\n        self.rope_full_precision = rope_full_precision\n        self.flash_attention = flash_attention\n        self.attention_dropout = attention_dropout\n        self.multi_query_attention = multi_query_attention\n        self.attention_layer_norm = attention_layer_norm\n        self.residual_dropout = residual_dropout\n        self.embedding_dropout = embedding_dropout\n        self.layer_norm_type = layer_norm_type\n        self.layer_norm_with_affine = layer_norm_with_affine\n        self.attention_layer_norm_with_affine = attention_layer_norm_with_affine\n        self.max_sequence_length = max_sequence_length\n        self.include_bias = include_bias\n        self.bias_for_layer_norm = bias_for_layer_norm\n        self.scale_logits = scale_logits\n        self.gradient_checkpointing = gradient_checkpointing\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.weight_tying = weight_tying\n        self.init_std = init_std\n        self.init_cutoff_factor = init_cutoff_factor\n        super().__init__(\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\"\n    ):\n        if not hasattr(self, \"gradient_checkpointing\"):\n            self.gradient_checkpointing = gradient_checkpointing\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n</code></pre>"},{"location":"generated-modules-olmo-olmo_configuration/#src.python.easydel.modules.olmo.olmo_configuration.OLMoConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/olmo/olmo_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/","title":"modules.openelm.modelling_openelm_flax","text":""},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMForCausalLMModule","title":"<code>FlaxOpenELMForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>class FlaxOpenELMForCausalLMModule(nn.Module):\n    config: OpenELMConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.transformer: FlaxOpenELMModule = FlaxOpenELMModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = nn.Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n        and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n        as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n        the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n        output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Determine whether to use dropout in the\n                model\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of the outputs or\n                just the logits\n        :param : Determine whether to return the logits or not\n\n        Returns:\n            A tuple of (lm_logits, hidden_states, attentions)\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            inputs_embeds=inputs_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.share_input_output_layers:\n            shared_kernel = self.transformer.variables[\"params\"][\"token_embeddings\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits[:, : self.config.vocab_size]\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax module. It defines how the model will be called, and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask as inputs (these are defined in init). We also have some optional arguments that can be passed to the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings), output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout in the model</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or just the logits</p> <code>True</code> <p>:param : Determine whether to return the logits or not</p> <p>Returns:</p> Type Description <p>A tuple of (lm_logits, hidden_states, attentions)</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It defines how the model will be called,\n    and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n    as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n    the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n    output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Determine whether to use dropout in the\n            model\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of the outputs or\n            just the logits\n    :param : Determine whether to return the logits or not\n\n    Returns:\n        A tuple of (lm_logits, hidden_states, attentions)\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.transformer(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        deterministic=deterministic,\n        inputs_embeds=inputs_embeds,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.share_input_output_layers:\n        shared_kernel = self.transformer.variables[\"params\"][\"token_embeddings\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = lm_logits[:, : self.config.vocab_size]\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMModule","title":"<code>FlaxOpenELMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>class FlaxOpenELMModule(nn.Module):\n    config: OpenELMConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[str, jax.lax.Precision]] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        config = self.config\n        self.token_embeddings = nn.Embed(\n            config.vocab_size,\n            config.model_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        self.layers = FlaxOpenELMDecoderLayerCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = OpenELMRMSNorm(\n            config.model_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        if config.share_input_output_layers:\n            self.classifier = None\n        else:\n            self.classifier = nn.Linear(\n                config.vocab_size,\n                use_bias=False,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n            )\n        self.num_transformer_layers = config.num_transformer_layers\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if self.config.rope_scaling is not None:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=(\n                getattr(self.config, \"freq_max_position_embeddings\", self.config.rope_max_length)\n            ),\n            dim=self.config.head_dim,\n            base=self.config.rope_freq_constant,\n            **initial_rope_kwargs\n        )\n        self.causal_mask = flax.linen.make_causal_mask(\n            jnp.ones(\n                (1, getattr(self.config, \"c_max_position_embeddings\", self.config.max_context_length)),\n                dtype=\"bool\"\n            ), dtype=\"bool\"\n        )\n\n    def __call__(\n            self,\n            input_ids: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input ids\n            attention_mask: chex.Array: Mask out the attention weights\n                for certain tokens\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            deterministic: bool: Determine whether to use dropout or not\n            inputs_embeds: chex.Array: Pass in the embedding of the\n                input_ids\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            output_hidden_states: bool: Return all hidden states or just\n                the last one\n            return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n\n        Returns:\n            A tuple of the hidden states, all hidden states, and\n            attentions\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.token_embeddings(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_states=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask,\n            output_hidden_states=output_hidden_states,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMModule.__call__","title":"<code>__call__(input_ids=None, attention_mask=None, position_ids=None, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids as inputs to the model. The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Optional[Array]</code> <p>chex.Array: Pass in the input ids</p> <code>None</code> <code>attention_mask</code> <code>Optional[Array]</code> <p>chex.Array: Mask out the attention weights for certain tokens</p> <code>None</code> <code>position_ids</code> <code>Optional[Array]</code> <p>chex.Array: Determine the position of each token in a sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embedding of the input_ids</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return all hidden states or just the last one</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <p>:param : Determine whether the model is in training mode or not</p> <p>Returns:</p> Type Description <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>A tuple of the hidden states, all hidden states, and</p> <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>attentions</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: Optional[chex.Array] = None,\n        attention_mask: Optional[chex.Array] = None,\n        position_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n    \"\"\"The __call__ function is the main function of a Flax model.\n    It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n    The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input ids\n        attention_mask: chex.Array: Mask out the attention weights\n            for certain tokens\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        deterministic: bool: Determine whether to use dropout or not\n        inputs_embeds: chex.Array: Pass in the embedding of the\n            input_ids\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        output_hidden_states: bool: Return all hidden states or just\n            the last one\n        return_dict: bool: Return a dictionary of the outputs or not\n    :param : Determine whether the model is in training mode or not\n\n    Returns:\n        A tuple of the hidden states, all hidden states, and\n        attentions\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.token_embeddings(input_ids.astype(\"i4\"))\n    if attention_mask.ndim == 2:\n        b, s = attention_mask.shape\n        attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n    outputs = self.layers(\n        hidden_states=inputs_embeds,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        freq_cis=self.freq_cis,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        deterministic=deterministic,\n        causal_mask=self.causal_mask,\n        output_hidden_states=output_hidden_states,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(value for value in outputs if value is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMMultiHeadCausalAttention","title":"<code>FlaxOpenELMMultiHeadCausalAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>class FlaxOpenELMMultiHeadCausalAttention(BaseJAXAttentionModule):\n    config: OpenELMConfig\n    layer_idx: int\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        layer_idx = self.layer_idx\n        head_dim = config.head_dim\n        q_heads = config.num_query_heads[layer_idx]\n        k_heads = config.num_kv_heads[layer_idx]\n        v_heads = config.num_kv_heads[layer_idx]\n\n        self.qkv_proj = nn.Linear(\n            (q_heads + k_heads + v_heads) * head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        if config.normalize_qk_projections:\n            self.q_norm = OpenELMRMSNorm(\n                dim=config.head_dim,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype\n            )\n            self.k_norm = OpenELMRMSNorm(\n                dim=config.head_dim,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype\n            )\n        else:\n            self.q_norm = None\n            self.k_norm = None\n\n        self.out_proj = nn.Linear(\n            config.model_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            precision=self.precision,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.head_dim = head_dim\n        self.rotary = FlaxOpenELMRotaryEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=q_heads,\n            attention_dropout=0.0,\n            head_dims=head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n        self.head_dim = config.head_dim\n        self.num_q_heads = q_heads\n        self.num_k_heads = k_heads\n        self.num_v_heads = v_heads\n        self.transformer_dim = config.model_dim\n        self.num_groups = self.num_q_heads // self.num_k_heads\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_q_heads * self.head_dim,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.num_q_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.num_k_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.num_v_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_groups)\n        value = repeat_kv_bnsh(value, self.num_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        output_attentions = False\n\n        # [B, S, d] --&gt; [B, S, (q_h + k_h + v_h) * h]\n        qkv = self.qkv_proj(hidden_states)\n        # [B, S, (q_h + k_h + v_h) * h] --&gt; [B, S, (q_h + k_h + v_h), h]\n        qkv = qkv.reshape(\n            batch_size,\n            sequence_length,\n            self.num_q_heads + self.num_k_heads + self.num_v_heads,\n            self.head_dim,\n        )\n        # [B, S, (q_h + k_h + v_h), h] --&gt; [B, (q_h + k_h + v_h), S, h]\n        qkv = qkv.transpose(0, 2, 1, 3)\n        # [B, (q_h + k_h + v_h), S, h] --&gt; [B, q_h, S h], [B, k_h, S, h], [B, v_h, S, h]\n        query_states = qkv[:, :self.num_q_heads, :, :]\n        key_states = qkv[:, self.num_q_heads:self.num_k_heads + self.num_q_heads, :, :]\n        value_states = qkv[:, self.num_k_heads + self.num_q_heads:, :, :]\n        if self.q_norm is not None:\n            query_states = self.q_norm(query_states)\n\n        if self.k_norm is not None:\n            key_states = self.k_norm(key_states)\n\n        query_states, key_states, value_states = map(\n            lambda x: x.transpose(0, 2, 1, 3),\n            [query_states, key_states, value_states]\n        )\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_groups}\\n\\t\"\n            f\"NH : {self.num_q_heads} KVH : {self.num_k_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.num_q_heads, assert_msg\n        assert key_states.shape[-2] == self.num_q_heads, assert_msg\n        assert value_states.shape[-2] == self.num_q_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n        attention_mask = jnp.broadcast_to(\n            attention_mask, causal_mask.shape\n        )\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.out_proj(attn_output)\n\n        outputs = (\n            attn_output, attentions.attention_weights\n        ) if output_attentions else (\n            attn_output, None\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMMultiHeadCausalAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    output_attentions = False\n\n    # [B, S, d] --&gt; [B, S, (q_h + k_h + v_h) * h]\n    qkv = self.qkv_proj(hidden_states)\n    # [B, S, (q_h + k_h + v_h) * h] --&gt; [B, S, (q_h + k_h + v_h), h]\n    qkv = qkv.reshape(\n        batch_size,\n        sequence_length,\n        self.num_q_heads + self.num_k_heads + self.num_v_heads,\n        self.head_dim,\n    )\n    # [B, S, (q_h + k_h + v_h), h] --&gt; [B, (q_h + k_h + v_h), S, h]\n    qkv = qkv.transpose(0, 2, 1, 3)\n    # [B, (q_h + k_h + v_h), S, h] --&gt; [B, q_h, S h], [B, k_h, S, h], [B, v_h, S, h]\n    query_states = qkv[:, :self.num_q_heads, :, :]\n    key_states = qkv[:, self.num_q_heads:self.num_k_heads + self.num_q_heads, :, :]\n    value_states = qkv[:, self.num_k_heads + self.num_q_heads:, :, :]\n    if self.q_norm is not None:\n        query_states = self.q_norm(query_states)\n\n    if self.k_norm is not None:\n        key_states = self.k_norm(key_states)\n\n    query_states, key_states, value_states = map(\n        lambda x: x.transpose(0, 2, 1, 3),\n        [query_states, key_states, value_states]\n    )\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_groups}\\n\\t\"\n        f\"NH : {self.num_q_heads} KVH : {self.num_k_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.num_q_heads, assert_msg\n    assert key_states.shape[-2] == self.num_q_heads, assert_msg\n    assert value_states.shape[-2] == self.num_q_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n    attention_mask = jnp.broadcast_to(\n        attention_mask, causal_mask.shape\n    )\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.out_proj(attn_output)\n\n    outputs = (\n        attn_output, attentions.attention_weights\n    ) if output_attentions else (\n        attn_output, None\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMMultiHeadCausalAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.num_q_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.num_k_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.num_v_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n    key = repeat_kv_bnsh(key, self.num_groups)\n    value = repeat_kv_bnsh(value, self.num_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMPretrainedModel","title":"<code>FlaxOpenELMPretrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>class FlaxOpenELMPretrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = OpenELMConfig\n    base_model_prefix = \"openelm\"\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: OpenELMConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 param_dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        super().__init__(\n            config,\n            self.module_class(\n                config=config,\n                dtype=dtype,\n                param_dtype=param_dtype,\n                **kwargs\n            ),\n            input_shape=input_shape,\n            seed=seed,\n            dtype=dtype,\n            _do_init=_do_init\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -&gt; flax.core.FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Initialize the input_ids, attention_mask\n                and position_ids\n            params: flax.core.FrozenDict: Pass in the parameters of a\n                pre-trained model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),\n            input_shape\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rng_s, input_ids, attention_mask, position_ids, return_dict=False\n            )\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0),\n            input_ids,\n            attention_mask,\n            position_ids,\n            return_dict=False,\n            init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: Pass the input sequence to the model\n            attention_mask: Mask out the padding tokens\n            position_ids: Specify the position of each token in the\n                sequence\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass the past key values to the model\n            dropout_rng: jax.random.PRNGKey: Pass in a random number\n                generator key to the model\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Determine whether to\n                return the hidden states of all layers\n            return_dict: Optional[bool]: Return a dictionary of the\n                outputs\n            add_params_field: bool: Add a params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of (last_hidden_state, past_key_values)\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMPretrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: Pass the input sequence to the model\n        attention_mask: Mask out the padding tokens\n        position_ids: Specify the position of each token in the\n            sequence\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass the past key values to the model\n        dropout_rng: jax.random.PRNGKey: Pass in a random number\n            generator key to the model\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Determine whether to\n            return the hidden states of all layers\n        return_dict: Optional[bool]: Return a dictionary of the\n            outputs\n        add_params_field: bool: Add a params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of (last_hidden_state, past_key_values)\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\n        \"params\": params or self.params} if add_params_field else params or self.params\n\n    if self.config.bits is not None:\n        rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        None,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-openelm-modelling_openelm_flax/#src.python.easydel.modules.openelm.modelling_openelm_flax.FlaxOpenELMPretrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in a rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/openelm/modelling_openelm_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: flax.core.FrozenDict = None\n) -&gt; flax.core.FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n    It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Initialize the input_ids, attention_mask\n            and position_ids\n        params: flax.core.FrozenDict: Pass in the parameters of a\n            pre-trained model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(\n        jnp.arange(jnp.atleast_2d(input_ids).shape[-1]),\n        input_shape\n    )\n    params_rng, dropout_rng = jax.random.split(rng)\n    rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rng_s,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rng_s, input_ids, attention_mask, position_ids, return_dict=False\n        )\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/","title":"modules.openelm.openelm_configuration","text":""},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.OpenELMConfig","title":"<code>OpenELMConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>class OpenELMConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"openelm\"\n\n    def __init__(\n            self,\n            vocab_size: int = 32000,\n            max_context_length: int = 2048,\n            num_transformer_layers: int = 12,\n            model_dim: int = 2048,\n            head_dim: int = 128,\n            qkv_multipliers: Union[Number, List[Number]] = 1.0,\n            num_query_heads: Union[int, None] = None,\n            num_gqa_groups: int = 1,\n            ffn_multipliers: Union[Number, List[Number]] = 4.0,\n            ffn_with_glu: bool = True,\n            ffn_dim_divisor: int = 256,\n            activation_fn_name: str = \"swish\",\n            normalization_layer_name: str = \"rms_norm\",\n            normalize_qk_projections: bool = False,\n            share_input_output_layers: bool = False,\n            rope_freq_constant: int = 10000,\n            rope_max_length: int = 4096,\n            initializer_range: float = 0.02,\n            use_cache: bool = True,\n            bos_token_id: int = 1,\n            eos_token_id: int = 2,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        Args:\n            self: Represent the instance of the class\n            vocab_size: Define the size of the vocabulary\n            hidden_size: Determine the size of the embedding layers\n            intermediate_size: Define the size of the intermediate layer\n                in each transformer block\n            num_hidden_layers: Determine the number of layers in the\n                encoder and decoder\n            num_attention_heads: Determine the number of attention heads\n                in each layer\n            num_key_value_heads: Specify the number of heads for key and\n                value\n            hidden_act: Specify the activation function used in the\n                hidden layers\n            max_position_embeddings: Set the maximum length of the\n                sequence\n            initializer_range: Initialize the weights of the model\n            rms_norm_eps: Avoid division by zero in the rms\n                normalization\n            use_cache: Determine whether to use the cache in the decoder\n            pad_token_id: Specify the token id of the padding token\n            bos_token_id: Specify the beginning of sentence token id\n            eos_token_id: Specify the end of sentence token\n            tie_word_embeddings: Tie the word embeddings and the output\n                layer\n            rope_theta: Control the number of tokens in a rope\n            sliding_window: Control the number of tokens that are\n                processed in parallel\n            gradient_checkpointing: str: Specify whether to use gradient\n                checkpointing\n            use_scan_mlp: bool: Determine whether or not to use the\n                scan_mlp function\n            scan_mlp_chunk_size: int: Specify the chunk size of the scan\n                mlp\n            number_rep_kv: int: Specify the number of times to repeat\n                the key and value vectors\n            attention_dropout: float: Set the dropout rate for the\n                attention layer\n            bits: Optional[int]: Specify the number of bits used for\n                quantization\n            axis_dims: Sequence[int]: Specify the dimension of each axis\n            axis_names: Sequence[str]: Specify the names of each axis in\n                the tensor\n            &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n            attention_bias: bool: when ever to use attention_bias\n            **kwargs: Pass a variable number of keyword arguments to a\n                function\n        :param : Define the number of layers in the model\n\n        Returns:\n            An instance of the class\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_context_length = max_context_length\n        self.num_transformer_layers = num_transformer_layers\n        self.model_dim = model_dim\n        self.head_dim = head_dim\n        self.qkv_multipliers = qkv_multipliers\n        self.num_query_heads = num_query_heads\n        self.num_gqa_groups = num_gqa_groups\n        self.ffn_multipliers = ffn_multipliers\n        self.ffn_with_glu = ffn_with_glu\n        self.ffn_dim_divisor = ffn_dim_divisor\n        self.activation_fn_name = activation_fn_name\n        self.normalization_layer_name = normalization_layer_name\n        self.normalize_qk_projections = normalize_qk_projections\n        self.share_input_output_layers = share_input_output_layers\n        self.rope_freq_constant = rope_freq_constant\n        self.rope_max_length = rope_max_length\n        self.num_query_heads = (\n            compute_heads(model_dim=model_dim, head_dim=head_dim)\n            if num_query_heads is None\n            else num_query_heads\n        )\n        self.initializer_range = initializer_range\n        self.bits = bits\n        self.initializer_range = initializer_range\n        self.use_cache = use_cache\n        self.rope_scaling = rope_scaling\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to use\n                the fully_sharded_data_parallel partitioning scheme or\n                not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        ) if not fully_sharded_data_parallel else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the model:\n\n        Args:\n            self: Bind the attributes and methods of a class to an\n                instance of that class\n            gradient_checkpointing: str: Determine whether to use\n                gradient checkpointing\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or notn\n            scan_mlp_chunk_size: int: Chunk the input to the mlp\n            bits: Optional[int]: Specify the number of bits to use for\n                quantization\n            rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n                rope\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n\n        self.rope_scaling = rope_scaling\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n\n    def __post_init__(self) -&gt; None:\n        if self.num_gqa_groups is not None:\n            head_multiple_of = self.num_gqa_groups\n        else:\n            head_multiple_of = 2\n\n        if isinstance(self.qkv_multipliers, Number):\n            # All attention layers have the same latent dimensions, resulting in uniform allocation of parameters.\n            qkv_dim = make_divisible(\n                self.model_dim * self.qkv_multipliers,  # type:ignore\n                divisor=self.head_dim * head_multiple_of,\n            )\n            query_dims = [int(qkv_dim)] * self.num_transformer_layers\n\n        elif (\n                isinstance(self.qkv_multipliers, (tuple, list))\n                and len(self.qkv_multipliers) == 2\n        ):\n            # Each attention layer have different latent dimensions assuming qkv_multipliers[0] != qkv_multipliers[1].\n            # This results in variable allocation of parameters in attention layer.\n            # This scaling is known as layer-wise or block-wise scaling: https://arxiv.org/abs/2008.00623\n            qkv_multipliers = [\n                round(v, 2)\n                for v in jnp.linspace(\n                    self.qkv_multipliers[0],\n                    self.qkv_multipliers[1],\n                    num=self.num_transformer_layers,\n                    dtype=float,\n                )\n            ]\n            # Make sure that scaled model dimension is divisible by scaled head dimension.\n            query_dims = [\n                int(\n                    make_divisible(\n                        self.model_dim * m, divisor=self.head_dim * head_multiple_of\n                    )\n                )\n                for m in qkv_multipliers\n            ]\n        else:\n            raise NotImplementedError(\n                f\"QKV multipliers should be a single number or a list containing exactly two numbers. Got: {qkv_multipliers}.\"\n            )\n\n        # compute the number of query, key, and value heads\n        # For multi-head and multi-query attention, the number of heads for query, key, and value are the same.\n        # For group query attention, the number of key and value heads are the same.\n        self.num_query_heads = [\n            int(compute_heads(q_dim, self.head_dim)) for q_dim in query_dims\n        ]\n        self.num_kv_heads = [\n            q_heads // self.num_gqa_groups for q_heads in self.num_query_heads\n        ]\n\n        # Feed-forward network (FFN) multipliers\n        if isinstance(self.ffn_multipliers, Number):\n            # All FFN layers have the same latent dimensions, resulting in uniform allocation of parameters.\n            self.ffn_multipliers = [self.ffn_multipliers] * self.num_transformer_layers\n        elif isinstance(self.ffn_multipliers, (tuple, list)):\n            # Each FFN layer have different latent dimensions assuming ffn_multipliers[0] != ffn_multipliers[1].\n            # This results in variable allocation of parameters in FFN layer.\n            # This scaling is known as layer-wise or block-wise scaling: https://arxiv.org/abs/2008.00623\n            if len(self.ffn_multipliers) == 2:\n                self.ffn_multipliers = [\n                    round(v, 2)\n                    for v in jnp.linspace(\n                        self.ffn_multipliers[0],\n                        self.ffn_multipliers[1],\n                        num=self.num_transformer_layers,\n                        dtype=float,\n                    )\n                ]\n            else:\n                assert (\n                        len(self.ffn_multipliers) == self.num_transformer_layers\n                ), f\"{len(self.ffn_multipliers)=}!={self.num_transformer_layers=}\"\n        else:\n            raise NotImplementedError(\n                f\"FFN multipliers should be a single number or a list containing exactly two numbers. Got: {qkv_multipliers}.\"\n            )\n\n        # check num_query_heads divisible by num_kv_heads for every layer\n        for layer_idx in range(len(query_dims)):\n            assert self.num_query_heads[layer_idx] % self.num_kv_heads[layer_idx] == 0\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.OpenELMConfig.__init__","title":"<code>__init__(vocab_size=32000, max_context_length=2048, num_transformer_layers=12, model_dim=2048, head_dim=128, qkv_multipliers=1.0, num_query_heads=None, num_gqa_groups=1, ffn_multipliers=4.0, ffn_with_glu=True, ffn_dim_divisor=256, activation_fn_name='swish', normalization_layer_name='rms_norm', normalize_qk_projections=False, share_input_output_layers=False, rope_freq_constant=10000, rope_max_length=4096, initializer_range=0.02, use_cache=True, bos_token_id=1, eos_token_id=2, rope_scaling=None, gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, bits=None, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It allows the class to initialize the attributes of a class. The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>vocab_size</code> <code>int</code> <p>Define the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <p>Determine the size of the embedding layers</p> required <code>intermediate_size</code> <p>Define the size of the intermediate layer in each transformer block</p> required <code>num_hidden_layers</code> <p>Determine the number of layers in the encoder and decoder</p> required <code>num_attention_heads</code> <p>Determine the number of attention heads in each layer</p> required <code>num_key_value_heads</code> <p>Specify the number of heads for key and value</p> required <code>hidden_act</code> <p>Specify the activation function used in the hidden layers</p> required <code>max_position_embeddings</code> <p>Set the maximum length of the sequence</p> required <code>initializer_range</code> <code>float</code> <p>Initialize the weights of the model</p> <code>0.02</code> <code>rms_norm_eps</code> <p>Avoid division by zero in the rms normalization</p> required <code>use_cache</code> <code>bool</code> <p>Determine whether to use the cache in the decoder</p> <code>True</code> <code>pad_token_id</code> <p>Specify the token id of the padding token</p> required <code>bos_token_id</code> <code>int</code> <p>Specify the beginning of sentence token id</p> <code>1</code> <code>eos_token_id</code> <code>int</code> <p>Specify the end of sentence token</p> <code>2</code> <code>tie_word_embeddings</code> <p>Tie the word embeddings and the output layer</p> required <code>rope_theta</code> <p>Control the number of tokens in a rope</p> required <code>sliding_window</code> <p>Control the number of tokens that are processed in parallel</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether or not to use the scan_mlp function</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the scan mlp</p> <code>1024</code> <code>number_rep_kv</code> <p>int: Specify the number of times to repeat the key and value vectors</p> required <code>attention_dropout</code> <p>float: Set the dropout rate for the attention layer</p> required <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used for quantization</p> <code>None</code> <code>axis_dims</code> <p>Sequence[int]: Specify the dimension of each axis</p> required <code>axis_names</code> <p>Sequence[str]: Specify the names of each axis in the tensor</p> required <code>&amp;quot;mp&amp;quot;)</code> <p>Define the maximum position embeddings</p> required <code>attention_bias</code> <p>bool: when ever to use attention_bias</p> required <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <p>:param : Define the number of layers in the model</p> <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>def __init__(\n        self,\n        vocab_size: int = 32000,\n        max_context_length: int = 2048,\n        num_transformer_layers: int = 12,\n        model_dim: int = 2048,\n        head_dim: int = 128,\n        qkv_multipliers: Union[Number, List[Number]] = 1.0,\n        num_query_heads: Union[int, None] = None,\n        num_gqa_groups: int = 1,\n        ffn_multipliers: Union[Number, List[Number]] = 4.0,\n        ffn_with_glu: bool = True,\n        ffn_dim_divisor: int = 256,\n        activation_fn_name: str = \"swish\",\n        normalization_layer_name: str = \"rms_norm\",\n        normalize_qk_projections: bool = False,\n        share_input_output_layers: bool = False,\n        rope_freq_constant: int = 10000,\n        rope_max_length: int = 4096,\n        initializer_range: float = 0.02,\n        use_cache: bool = True,\n        bos_token_id: int = 1,\n        eos_token_id: int = 2,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It allows the class to initialize the attributes of a class.\n    The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n    Args:\n        self: Represent the instance of the class\n        vocab_size: Define the size of the vocabulary\n        hidden_size: Determine the size of the embedding layers\n        intermediate_size: Define the size of the intermediate layer\n            in each transformer block\n        num_hidden_layers: Determine the number of layers in the\n            encoder and decoder\n        num_attention_heads: Determine the number of attention heads\n            in each layer\n        num_key_value_heads: Specify the number of heads for key and\n            value\n        hidden_act: Specify the activation function used in the\n            hidden layers\n        max_position_embeddings: Set the maximum length of the\n            sequence\n        initializer_range: Initialize the weights of the model\n        rms_norm_eps: Avoid division by zero in the rms\n            normalization\n        use_cache: Determine whether to use the cache in the decoder\n        pad_token_id: Specify the token id of the padding token\n        bos_token_id: Specify the beginning of sentence token id\n        eos_token_id: Specify the end of sentence token\n        tie_word_embeddings: Tie the word embeddings and the output\n            layer\n        rope_theta: Control the number of tokens in a rope\n        sliding_window: Control the number of tokens that are\n            processed in parallel\n        gradient_checkpointing: str: Specify whether to use gradient\n            checkpointing\n        use_scan_mlp: bool: Determine whether or not to use the\n            scan_mlp function\n        scan_mlp_chunk_size: int: Specify the chunk size of the scan\n            mlp\n        number_rep_kv: int: Specify the number of times to repeat\n            the key and value vectors\n        attention_dropout: float: Set the dropout rate for the\n            attention layer\n        bits: Optional[int]: Specify the number of bits used for\n            quantization\n        axis_dims: Sequence[int]: Specify the dimension of each axis\n        axis_names: Sequence[str]: Specify the names of each axis in\n            the tensor\n        &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n        attention_bias: bool: when ever to use attention_bias\n        **kwargs: Pass a variable number of keyword arguments to a\n            function\n    :param : Define the number of layers in the model\n\n    Returns:\n        An instance of the class\n    \"\"\"\n    self.vocab_size = vocab_size\n    self.max_context_length = max_context_length\n    self.num_transformer_layers = num_transformer_layers\n    self.model_dim = model_dim\n    self.head_dim = head_dim\n    self.qkv_multipliers = qkv_multipliers\n    self.num_query_heads = num_query_heads\n    self.num_gqa_groups = num_gqa_groups\n    self.ffn_multipliers = ffn_multipliers\n    self.ffn_with_glu = ffn_with_glu\n    self.ffn_dim_divisor = ffn_dim_divisor\n    self.activation_fn_name = activation_fn_name\n    self.normalization_layer_name = normalization_layer_name\n    self.normalize_qk_projections = normalize_qk_projections\n    self.share_input_output_layers = share_input_output_layers\n    self.rope_freq_constant = rope_freq_constant\n    self.rope_max_length = rope_max_length\n    self.num_query_heads = (\n        compute_heads(model_dim=model_dim, head_dim=head_dim)\n        if num_query_heads is None\n        else num_query_heads\n    )\n    self.initializer_range = initializer_range\n    self.bits = bits\n    self.initializer_range = initializer_range\n    self.use_cache = use_cache\n    self.rope_scaling = rope_scaling\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n\n    super().__init__(\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        use_scan_mlp=use_scan_mlp,\n        scan_mlp_chunk_size=scan_mlp_chunk_size,\n        bits=bits,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.OpenELMConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, bits=None, rope_scaling=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the attributes and methods of a class to an instance of that class</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Determine whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or notn</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Chunk the input to the mlp</p> <code>1024</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits to use for quantization</p> <code>None</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str, Union[str, float]]: rope_scaling for rope</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the model:\n\n    Args:\n        self: Bind the attributes and methods of a class to an\n            instance of that class\n        gradient_checkpointing: str: Determine whether to use\n            gradient checkpointing\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or notn\n        scan_mlp_chunk_size: int: Chunk the input to the mlp\n        bits: Optional[int]: Specify the number of bits to use for\n            quantization\n        rope_scaling: Dict[str, Union[str, float]]: rope_scaling for\n            rope\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n\n    self.rope_scaling = rope_scaling\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.OpenELMConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to use the fully_sharded_data_parallel partitioning scheme or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to use\n            the fully_sharded_data_parallel partitioning scheme or\n            not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    ) if not fully_sharded_data_parallel else (\n        (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.compute_heads","title":"<code>compute_heads(model_dim, head_dim)</code>","text":"<p>Compute the number of heads. Args:     model_dim: Model dimension.     head_dim: Head dimension. Returns:     An integer denoting number of heads in multi-head attention is returned. Raises:     ValueError: if model dimension is not divisible by head dimension.</p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>def compute_heads(model_dim: int, head_dim: int) -&gt; int:\n    \"\"\"Compute the number of heads.\n    Args:\n        model_dim: Model dimension.\n        head_dim: Head dimension.\n    Returns:\n        An integer denoting number of heads in multi-head attention is returned.\n    Raises:\n        ValueError: if model dimension is not divisible by head dimension.\n    \"\"\"\n    if model_dim % head_dim == 0:\n        return model_dim // head_dim\n    else:\n        raise ValueError(\n            f\"Model dimension should be divisible by head dimension. Got: {model_dim} and {head_dim}.\"\n        )\n</code></pre>"},{"location":"generated-modules-openelm-openelm_configuration/#src.python.easydel.modules.openelm.openelm_configuration.make_divisible","title":"<code>make_divisible(v, divisor=8, min_value=None)</code>","text":"<p>This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by the divisor It can be seen at: https://github.com/tensorflow/models/blob/2cfc99eff5e5eb729c6793d2f3d03aa1c9be2b15/research/slim/nets/mobilenet/mobilenet.py#L62 Args:     v: input value     divisor: default to 8     min_value: minimum divisor value Returns:     new_v: new divisible value</p> Source code in <code>src/python/easydel/modules/openelm/openelm_configuration.py</code> <pre><code>def make_divisible(\n        v: Union[float, int],\n        divisor: Optional[int] = 8,\n        min_value: Optional[Union[float, int]] = None,\n) -&gt; Union[float, int]:\n    \"\"\"This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by the divisor\n    It can be seen at:\n    https://github.com/tensorflow/models/blob/2cfc99eff5e5eb729c6793d2f3d03aa1c9be2b15/research/slim/nets/mobilenet/mobilenet.py#L62\n    Args:\n        v: input value\n        divisor: default to 8\n        min_value: minimum divisor value\n    Returns:\n        new_v: new divisible value\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n</code></pre>"},{"location":"generated-modules-opt-modelling_opt_flax/","title":"modules.opt.modelling_opt_flax","text":"<p>Flax OPT model.</p>"},{"location":"generated-modules-opt-modelling_opt_flax/#src.python.easydel.modules.opt.modelling_opt_flax.FlaxOPTLearnedPositionalEmbedding","title":"<code>FlaxOPTLearnedPositionalEmbedding</code>","text":"<p>               Bases: <code>Embed</code></p> Source code in <code>src/python/easydel/modules/opt/modelling_opt_flax.py</code> <pre><code>class FlaxOPTLearnedPositionalEmbedding(nn.Embed):\n\n    def setup(self):\n        self.offset = 2\n        self.embedding = self.param(\n            \"embedding\", self.embedding_init, (self.num_embeddings + self.offset, self.features), self.param_dtype\n        )\n\n    def __call__(self, positions):\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n\n        return super().__call__(positions + self.offset)\n</code></pre>"},{"location":"generated-modules-opt-modelling_opt_flax/#src.python.easydel.modules.opt.modelling_opt_flax.FlaxOPTLearnedPositionalEmbedding.__call__","title":"<code>__call__(positions)</code>","text":"<p><code>input_ids_shape</code> is expected to be [bsz x seqlen].</p> Source code in <code>src/python/easydel/modules/opt/modelling_opt_flax.py</code> <pre><code>def __call__(self, positions):\n    \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n\n    return super().__call__(positions + self.offset)\n</code></pre>"},{"location":"generated-modules-opt-opt_configuration/","title":"modules.opt.opt_configuration","text":""},{"location":"generated-modules-palm-modelling_palm_flax/","title":"modules.palm.modelling_palm_flax","text":""},{"location":"generated-modules-palm-palm_configuration/","title":"modules.palm.palm_configuration","text":""},{"location":"generated-modules-phi-modelling_phi_flax/","title":"modules.phi.modelling_phi_flax","text":""},{"location":"generated-modules-phi-modelling_phi_flax/#src.python.easydel.modules.phi.modelling_phi_flax.FlaxPhiAttention","title":"<code>FlaxPhiAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> <p>Multi-headed attention from 'Attention Is All You Need' paper</p> Source code in <code>src/python/easydel/modules/phi/modelling_phi_flax.py</code> <pre><code>class FlaxPhiAttention(BaseJAXAttentionModule):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    config: PhiConfig\n    layer_idx: Optional[int] = None\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        config = self.config\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.partial_rotary_factor = config.partial_rotary_factor\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        dense_class = functools.partial(\n            Linear,\n            use_bias=True,\n            precision=self.precision,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits)\n        )\n\n        self.q_proj = dense_class(self.num_heads * self.head_dim)\n        self.k_proj = dense_class(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense_class(self.num_key_value_heads * self.head_dim)\n        self.dense = dense_class(self.hidden_size)\n        self.rotary_emb_dim = int(self.config.partial_rotary_factor * self.head_dim)\n        self.qk_layernorm = config.qk_layernorm\n        if self.qk_layernorm:\n            self.q_layernorm = nn.LayerNorm(\n                epsilon=config.layer_norm_eps,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                use_bias=True\n            )\n            self.k_layernorm = nn.LayerNorm(\n                epsilon=config.layer_norm_eps,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                use_bias=True\n            )\n\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query_states, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query_states, key and value matrices.\n\n        Args:\n            query_states: Get the attention weights for each of the\n                heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query_states, key and value matrices\n        \"\"\"\n        return jnp.transpose(query_states, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value,\n                                                                                                          (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query_states, key and value tensors\n            sequence_length: Reshape the query_states, key and value\n                tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query_states, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        query_rot, query_pass = (\n            query[..., : self.rotary_emb_dim],\n            query[..., self.rotary_emb_dim:],\n        )\n        key_rot, key_pass = (\n            key[..., : self.rotary_emb_dim],\n            key[..., self.rotary_emb_dim:],\n        )\n\n        key_rot = apply_rotary_pos_emb(key_rot, sin, cos)\n        query_rot = apply_rotary_pos_emb(query_rot, sin, cos)\n\n        query = jnp.concatenate((query_rot, query_pass), axis=-1)\n        key = jnp.concatenate((key_rot, key_pass), axis=-1)\n\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: Optional[chex.Array],\n            position_ids: Optional[chex.Array],\n            causal_mask: Optional[chex.Array],\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            output_attentions: bool = False,\n            init_cache: bool = False,\n    ):\n        batch_size, sequence_length = hidden_states.shape[:2]\n        (\n            query_states,\n            key_states,\n            value_states\n        ) = self.q_proj(\n            hidden_states\n        ), self.k_proj(\n            hidden_states\n        ), self.v_proj(\n            hidden_states\n        )\n\n        if self.qk_layernorm:\n            query_states = self.q_layernorm(query_states)\n            key_states = self.k_layernorm(key_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim\n        )\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim\n        )\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim\n        )\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.dense(attn_output)\n\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-phi-modelling_phi_flax/#src.python.easydel.modules.phi.modelling_phi_flax.FlaxPhiAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query_states, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query_states, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query_states, key and value</p> Source code in <code>src/python/easydel/modules/phi/modelling_phi_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query_states, key and value tensors\n        sequence_length: Reshape the query_states, key and value\n            tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query_states, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n\n    sin, cos = freq_cis\n\n    sin = sin[position_ids][:, None, :, :]\n    cos = cos[position_ids][:, None, :, :]\n\n    query_rot, query_pass = (\n        query[..., : self.rotary_emb_dim],\n        query[..., self.rotary_emb_dim:],\n    )\n    key_rot, key_pass = (\n        key[..., : self.rotary_emb_dim],\n        key[..., self.rotary_emb_dim:],\n    )\n\n    key_rot = apply_rotary_pos_emb(key_rot, sin, cos)\n    query_rot = apply_rotary_pos_emb(query_rot, sin, cos)\n\n    query = jnp.concatenate((query_rot, query_pass), axis=-1)\n    key = jnp.concatenate((key_rot, key_pass), axis=-1)\n\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-phi-modelling_phi_flax/#src.python.easydel.modules.phi.modelling_phi_flax.FlaxPhiMLP","title":"<code>FlaxPhiMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/phi/modelling_phi_flax.py</code> <pre><code>class FlaxPhiMLP(nn.Module):\n    config: PhiConfig\n    layer_idx: Optional[int] = None\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    \"\"\"Multi-Layer Perceptron.\n    Reference:\n        Attention Is All You Need.\n        https://arxiv.org/pdf/1706.03762.pdf.\n    \"\"\"\n\n    def setup(\n            self\n    ) -&gt; None:\n        self.fc1 = Linear(\n            self.config.intermediate_size,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.fc2 = Linear(\n            self.config.n_embd,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.act = ACT2FN[self.config.hidden_act]\n\n    def __call__(\n            self,\n            hidden_states: Array,\n            e: bool = False  # Ignored\n    ) -&gt; Array:\n        return self.fc2(self.act(self.fc1(hidden_states)))\n</code></pre>"},{"location":"generated-modules-phi-modelling_phi_flax/#src.python.easydel.modules.phi.modelling_phi_flax.FlaxPhiMLP.precision","title":"<code>precision: Optional[jax.lax.Precision] = jax.lax.Precision('fastest')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Multi-Layer Perceptron. Reference:     Attention Is All You Need.     https://arxiv.org/pdf/1706.03762.pdf.</p>"},{"location":"generated-modules-phi-modelling_phi_flax/#src.python.easydel.modules.phi.modelling_phi_flax.FlaxPhiPreTrainedModel","title":"<code>FlaxPhiPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> <p>Phi pre-trained model.</p> Source code in <code>src/python/easydel/modules/phi/modelling_phi_flax.py</code> <pre><code>class FlaxPhiPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    \"\"\"Phi pre-trained model.\"\"\"\n    module_class = None\n    config_class = PhiConfig\n    base_model_prefix = \"transformer\"\n\n    def __init__(\n            self,\n            config: PhiConfig,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape=(1, 1),\n            seed: int = 42,\n            _do_init: bool = False\n    ) -&gt; None:\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision\n        )\n        super().__init__(\n            config=config,\n            module=module,\n            input_shape=input_shape,\n            _do_init=_do_init,\n            seed=seed\n        )\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(rngs, input_ids, attention_mask)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            input_ids=input_ids,\n            inputs_embeds=None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            extra_embedding=extra_embedding,\n            deterministic=not train,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            init_cache=False,\n            return_dict=return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-phi-phi_configuration/","title":"modules.phi.phi_configuration","text":""},{"location":"generated-modules-phi-phi_configuration/#src.python.easydel.modules.phi.phi_configuration.PhiConfig","title":"<code>PhiConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> <p>Phi configuration.</p> Source code in <code>src/python/easydel/modules/phi/phi_configuration.py</code> <pre><code>class PhiConfig(EasyDeLPretrainedConfig):\n    \"\"\"Phi configuration.\"\"\"\n\n    model_type: str = \"phi\"\n    attribute_map = {\n        \"max_position_embeddings\": \"n_positions\",\n        \"hidden_size\": \"n_embd\",\n        \"num_attention_heads\": \"num_attention_heads\",\n        \"num_hidden_layers\": \"num_hidden_layers\",\n    }\n\n    def __init__(\n            self,\n            vocab_size=51200,\n            hidden_size=2048,\n            intermediate_size=8192,\n            num_hidden_layers=24,\n            num_attention_heads=32,\n            num_key_value_heads=None,\n            resid_pdrop=0.0,\n            embd_pdrop=0.0,\n            attention_dropout=0.0,\n            hidden_act=\"gelu_new\",\n            max_position_embeddings=2048,\n            initializer_range=0.02,\n            layer_norm_eps=1e-5,\n            use_cache=True,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            rope_scaling=None,\n            partial_rotary_factor=0.5,\n            qk_layernorm=False,\n            bos_token_id=1,\n            eos_token_id=2,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ) -&gt; None:\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.partial_rotary_factor = partial_rotary_factor\n        self.qk_layernorm = qk_layernorm\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            bits=bits,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        return (\n            (\"embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"final_layernorm/(scale|bias)\", PartitionSpec(None, )),\n            (\"final_layernorm/(scale|bias)\", PartitionSpec(None, )),\n            (\"mlp/fc1/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/fc1/bias\", PartitionSpec(\"tp\", )),\n            (\"mlp/fc2/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/fc2/bias\", PartitionSpec((\"fsdp\", \"sp\"), )),\n            (\"self_attn/dense/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"self_attn/dense/bias\", PartitionSpec(\"tp\")),\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/(q_proj|k_proj|v_proj)/bias\", PartitionSpec(\"tp\", )),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"lm_head/bias\", PartitionSpec(\"tp\")),\n            (\".*\", PartitionSpec(None, ))\n        ) if fully_sharded_data_parallel else (\n            (\"embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n            (\"final_layernorm/(scale|bias)\", PartitionSpec(None, )),\n            (\"final_layernorm/(scale|bias)\", PartitionSpec(None, )),\n            (\"mlp/fc1/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/fc1/bias\", PartitionSpec(\"tp\", )),\n            (\"mlp/fc2/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/fc2/bias\", PartitionSpec((\"fsdp\", \"sp\"), )),\n            (\"self_attn/dense/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n            (\"self_attn/dense/bias\", PartitionSpec(\"tp\")),\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/(q_proj|k_proj|v_proj)/bias\", PartitionSpec(\"tp\", )),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"lm_head/bias\", PartitionSpec(\"tp\")),\n            (\".*\", PartitionSpec(None, ))\n        )\n</code></pre>"},{"location":"generated-modules-phi3-modelling_phi3_flax/","title":"modules.phi3.modelling_phi3_flax","text":""},{"location":"generated-modules-phi3-modelling_phi3_flax/#src.python.easydel.modules.phi3.modelling_phi3_flax.FlaxPhi3Attention","title":"<code>FlaxPhi3Attention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> <p>Multi-headed attention from 'Attention Is All You Need' paper</p> Source code in <code>src/python/easydel/modules/phi3/modelling_phi3_flax.py</code> <pre><code>class FlaxPhi3Attention(BaseJAXAttentionModule):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    config: Phi3Config\n    layer_idx: Optional[int] = None\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        config = self.config\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.original_max_position_embeddings = config.original_max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.rope_scaling = config.rope_scaling\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        dense_class = functools.partial(\n            Linear,\n            use_bias=False,\n            precision=self.precision,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits)\n        )\n\n        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense_class(self.hidden_size)\n        self.qkv_proj = dense_class(op_size)\n        self.rotary = FlaxPhi3Embedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query_states, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query_states, key and value matrices.\n\n        Args:\n            query_states: Get the attention weights for each of the\n                heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query_states, key and value matrices\n        \"\"\"\n        return jnp.transpose(query_states, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value,\n                                                                                                          (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query_states, key and value tensors\n            sequence_length: Reshape the query_states, key and value\n                tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query_states, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n\n        query, key = self.rotary(query=query, key=key, freq_cis=freq_cis, position_ids=position_ids)\n\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        batch_size, sequence_length = hidden_states.shape[:2]\n        qkv = self.qkv_proj(hidden_states)\n        query_pos = self.num_heads * self.head_dim\n        query_states = qkv[..., :query_pos]\n        key_states = qkv[..., query_pos: query_pos + self.num_key_value_heads * self.head_dim]\n        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim:]\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-phi3-modelling_phi3_flax/#src.python.easydel.modules.phi3.modelling_phi3_flax.FlaxPhi3Attention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query_states, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query_states, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query_states, key and value</p> Source code in <code>src/python/easydel/modules/phi3/modelling_phi3_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query_states, key and value tensors\n        sequence_length: Reshape the query_states, key and value\n            tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query_states, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n\n    query, key = self.rotary(query=query, key=key, freq_cis=freq_cis, position_ids=position_ids)\n\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-phi3-modelling_phi3_flax/#src.python.easydel.modules.phi3.modelling_phi3_flax.FlaxPhi3MLP","title":"<code>FlaxPhi3MLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/phi3/modelling_phi3_flax.py</code> <pre><code>class FlaxPhi3MLP(nn.Module):\n    config: Phi3Config\n    layer_idx: Optional[int] = None\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    \"\"\"Multi-Layer Perceptron.\n    Reference:\n        Attention Is All You Need.\n        https://arxiv.org/pdf/1706.03762.pdf.\n    \"\"\"\n\n    def setup(\n            self\n    ) -&gt; None:\n        self.gate_up_proj = Linear(\n            2 * self.config.intermediate_size,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            use_bias=False\n        )\n        self.down_proj = Linear(\n            self.config.hidden_size,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            use_bias=False\n        )\n        self.activation_fn = ACT2FN[self.config.hidden_act]\n\n    def __call__(\n            self,\n            hidden_states: Array,\n            e: bool = False  # Ignored\n    ) -&gt; Array:\n        up_states = self.gate_up_proj(hidden_states)\n\n        gate, up_states = jnp.split(up_states, 2, axis=-1)\n        up_states = up_states * self.activation_fn(gate)\n\n        return self.down_proj(up_states)\n</code></pre>"},{"location":"generated-modules-phi3-modelling_phi3_flax/#src.python.easydel.modules.phi3.modelling_phi3_flax.FlaxPhi3MLP.precision","title":"<code>precision: Optional[jax.lax.Precision] = jax.lax.Precision('fastest')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Multi-Layer Perceptron. Reference:     Attention Is All You Need.     https://arxiv.org/pdf/1706.03762.pdf.</p>"},{"location":"generated-modules-phi3-modelling_phi3_flax/#src.python.easydel.modules.phi3.modelling_phi3_flax.FlaxPhiPreTrainedModel","title":"<code>FlaxPhiPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> <p>Phi pre-trained model.</p> Source code in <code>src/python/easydel/modules/phi3/modelling_phi3_flax.py</code> <pre><code>class FlaxPhiPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    \"\"\"Phi pre-trained model.\"\"\"\n    module_class = None\n    config_class = Phi3Config\n    base_model_prefix = \"transformer\"\n\n    def __init__(\n            self,\n            config: Phi3Config,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape=(1, 1),\n            seed: int = 42,\n            _do_init: bool = False\n    ) -&gt; None:\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision\n        )\n        super().__init__(\n            config=config,\n            module=module,\n            input_shape=input_shape,\n            _do_init=_do_init,\n            seed=seed\n        )\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(rngs, input_ids, attention_mask)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            input_ids=input_ids,\n            inputs_embeds=None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            extra_embedding=extra_embedding,\n            deterministic=not train,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            init_cache=False,\n            return_dict=return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-phi3-phi3_configuration/","title":"modules.phi3.phi3_configuration","text":""},{"location":"generated-modules-phi3-phi3_configuration/#src.python.easydel.modules.phi3.phi3_configuration.Phi3Config","title":"<code>Phi3Config</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> <p>Phi configuration.</p> Source code in <code>src/python/easydel/modules/phi3/phi3_configuration.py</code> <pre><code>class Phi3Config(EasyDeLPretrainedConfig):\n    \"\"\"Phi configuration.\"\"\"\n\n    model_type: str = \"phi3\"\n\n    def __init__(\n            self,\n            vocab_size=32064,\n            hidden_size=3072,\n            intermediate_size=8192,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=None,\n            resid_pdrop=0.0,\n            embd_pdrop=0.0,\n            attention_dropout=0.0,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096,\n            original_max_position_embeddings=4096,\n            initializer_range=0.02,\n            rms_norm_eps=1e-5,\n            use_cache=True,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            rope_scaling=None,\n            bos_token_id=1,\n            eos_token_id=32000,\n            pad_token_id=32000,\n            sliding_window=None,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ) -&gt; None:\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.max_position_embeddings = max_position_embeddings\n        self.original_max_position_embeddings = original_max_position_embeddings\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self._rope_scaling_validation()\n        self.sliding_window = sliding_window\n\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            bits=bits,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        return (\n            (\"embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"norm/kernel\", PartitionSpec((\"fsdp\", \"sp\"), )),\n            (\"post_attention_layernorm/kernel\", PartitionSpec((\"fsdp\", \"sp\"), )),\n            (\"input_layernorm/kernel\", PartitionSpec((\"fsdp\", \"sp\"),)),\n\n            (\"mlp/gate_up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/qkv_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None, ))\n\n        ) if fully_sharded_data_parallel else (\n            (\"embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"norm/kernel\", PartitionSpec(None, )),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None, )),\n            (\"input_layernorm/kernel\", PartitionSpec(None, )),\n\n            (\"mlp/gate_up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n            (\"self_attn/qkv_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None, ))\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"Validate the `rope_scaling` configuration.\"\"\"\n        if self.rope_scaling is None:\n            return\n\n        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 3:\n            raise ValueError(\n                \"`rope_scaling` must be a dictionary with three fields, `type`, `short_factor` and `long_factor`, \"\n                f\"got {self.rope_scaling}\"\n            )\n        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n        rope_scaling_short_factor = self.rope_scaling.get(\"short_factor\", None)\n        rope_scaling_long_factor = self.rope_scaling.get(\"long_factor\", None)\n        if rope_scaling_type is None or rope_scaling_type not in [\"su\", \"yarn\"]:\n            raise ValueError(f\"`rope_scaling`'s type field must be one of ['su', 'yarn'], got {rope_scaling_type}\")\n        if not (\n                isinstance(rope_scaling_short_factor, list)\n                and all(isinstance(x, (int, float)) for x in rope_scaling_short_factor)\n        ):\n            raise ValueError(\n                f\"`rope_scaling`'s short_factor field must be a list of numbers, got {rope_scaling_short_factor}\"\n            )\n        if not len(rope_scaling_short_factor) == self.hidden_size // self.num_attention_heads // 2:\n            raise ValueError(\n                f\"`rope_scaling`'s short_factor field must have length {self.hidden_size // self.num_attention_heads // 2}, got {len(rope_scaling_short_factor)}\"\n            )\n        if not (\n                isinstance(rope_scaling_long_factor, list)\n                and all(isinstance(x, (int, float)) for x in rope_scaling_long_factor)\n        ):\n            raise ValueError(\n                f\"`rope_scaling`'s long_factor field must be a list of numbers, got {rope_scaling_long_factor}\"\n            )\n        if not len(rope_scaling_long_factor) == self.hidden_size // self.num_attention_heads // 2:\n            raise ValueError(\n                f\"`rope_scaling`'s long_factor field must have length {self.hidden_size // self.num_attention_heads // 2}, got {len(rope_scaling_long_factor)}\"\n            )\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/","title":"modules.qwen1.modelling_qwen1_flax","text":""},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Attention","title":"<code>FlaxQwen1Attention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1Attention(BaseJAXAttentionModule):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n\n        self.hidden_size = config.hidden_size\n        self.head_dim = config.hidden_size // config.num_attention_heads\n        self.projection_size = config.kv_channels * config.num_attention_heads\n        assert self.projection_size % config.num_attention_heads == 0\n        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads\n\n        self.c_attn = Linear(\n            self.projection_size * 3,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(config.bits, config.easy_method)\n        )\n\n        self.c_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=not self.config.no_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        logn_list = [\n            math.log(i, self.config.seq_length) if i &gt; self.config.seq_length else 1\n            for i in range(1, 32768)\n        ]\n        logn_tensor = jnp.asarray(logn_list)[None, :, None, None]\n        self.logn_tensor = logn_tensor\n        self.rotary = FlaxQwen1EmbeddingApplyer(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attn_dropout_prob,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, rotary_pos_emb_list, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, rotary_pos_emb_list, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query_states: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            rotary_pos_emb_list: Calculate the frequency of each word in\n                the vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query_states, key and value\n        \"\"\"\n        query_states, key = self.rotary(\n            position_ids=position_ids, query_states=query_states, key=key, rotary_pos_emb_list=rotary_pos_emb_list\n        )\n        return query_states, key, value\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            rotary_pos_emb_list: list[chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            encoder_hidden_states: Optional[chex.Array] = None,\n            encoder_attention_mask: Optional[chex.Array] = None,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            rotary_pos_emb_list: list[chex.Array]: Pass in the frequency\n                coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        mixed_x_layer: chex.Array = self.c_attn(hidden_states)\n        query_states, key_states, value_states = jnp.split(mixed_x_layer, 3, 2)\n\n        query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        value_states = value_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            rotary_pos_emb_list=rotary_pos_emb_list,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n\n        attn_output = self.c_proj(attn_output)\n\n        outputs = (\n            attn_output, attentions.attention_weights\n        ) if output_attentions else (\n            attn_output,\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Attention.__call__","title":"<code>__call__(hidden_states, rotary_pos_emb_list, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, encoder_hidden_states=None, encoder_attention_mask=None, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>rotary_pos_emb_list</code> <code>list[Array]</code> <p>list[chex.Array]: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        rotary_pos_emb_list: list[chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        encoder_hidden_states: Optional[chex.Array] = None,\n        encoder_attention_mask: Optional[chex.Array] = None,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        rotary_pos_emb_list: list[chex.Array]: Pass in the frequency\n            coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    mixed_x_layer: chex.Array = self.c_attn(hidden_states)\n    query_states, key_states, value_states = jnp.split(mixed_x_layer, 3, 2)\n\n    query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    value_states = value_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        rotary_pos_emb_list=rotary_pos_emb_list,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n\n    attn_output = self.c_proj(attn_output)\n\n    outputs = (\n        attn_output, attentions.attention_weights\n    ) if output_attentions else (\n        attn_output,\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Attention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, rotary_pos_emb_list, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, rotary_pos_emb_list, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query_states</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>rotary_pos_emb_list</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query_states, key and value</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, rotary_pos_emb_list, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, rotary_pos_emb_list, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query_states: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        rotary_pos_emb_list: Calculate the frequency of each word in\n            the vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query_states, key and value\n    \"\"\"\n    query_states, key = self.rotary(\n        position_ids=position_ids, query_states=query_states, key=key, rotary_pos_emb_list=rotary_pos_emb_list\n    )\n    return query_states, key, value\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Block","title":"<code>FlaxQwen1Block</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1Block(nn.Module):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxQwen1Attention\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = nn_partitioning.remat(\n                FlaxQwen1Attention, static_argnums=(1, 3, 4, 6, 7, 8, 9, 10, 11),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxQwen1MLP\n\n        if self.config.gradient_checkpointing != \"\":\n            mlp_block = nn_partitioning.remat(\n                FlaxQwen1MLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.ln_1 = Qwen1RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.layer_norm_epsilon,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.ln_2 = Qwen1RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.layer_norm_epsilon,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            rotary_pos_emb_list: list[chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            encoder_hidden_states: Optional[chex.Array] = None,\n            encoder_attention_mask: Optional[chex.Array] = None,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        Args:\n            self: Refer to the class instance itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                previous layer\n            rotary_pos_emb_list: list[chex.Array]: Pass in the frequency\n                information\n            attention_mask: chex.Array: Mask out the attention weights\n                for padding tokens\n            position_ids: chex.Array: Determine the position of each\n                token in the sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Control whether the dropout is applied\n                or not\n            init_cache: bool: Initialize the cache in the attention\n                layer\n            output_attentions: bool: Return the attention weights\n            fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n\n        Returns:\n            A tuple of two items\n        \"\"\"\n        # hidden_states: chex.Array\n        # rotary_pos_emb_list: list[chex.Array]\n        # attention_mask: chex.Array\n        # position_ids: chex.Array\n        # causal_mask: chex.Array\n        # deterministic: bool = True\n        # init_cache: bool = False\n        # output_attentions: bool = False\n        # encoder_hidden_states: Optional[chex.Array] = None\n        # encoder_attention_mask: Optional[chex.Array] = None\n        # fcm_mask = None\n\n        attn_outputs = self.attn(\n            self.ln_1(hidden_states),\n            rotary_pos_emb_list,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            encoder_attention_mask,\n            encoder_hidden_states,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.ln_2(hidden_states)\n\n        if self.config.use_scan_mlp:\n            feed_forward_input = einops.rearrange(\n                feed_forward_input,\n                '... (b s) d -&gt; ... b s d',\n                b=self.config.scan_mlp_chunk_size\n            )\n\n            def mlp_forward(mlp, carry, x):\n                return None, mlp(x, deterministic)\n\n            scan_axis = feed_forward_input.ndim - 3\n\n            _, feed_forward_hidden_states = nn.scan(\n                mlp_forward,\n                variable_broadcast=\"params\",\n                split_rngs={\"params\": False, \"dropout\": True},\n                in_axes=scan_axis,\n                out_axes=scan_axis,\n            )(self.mlp, None, feed_forward_input)\n            feed_forward_hidden_states = einops.rearrange(\n                feed_forward_hidden_states,\n                '... b s d -&gt; ... (b s) d'\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Block.__call__","title":"<code>__call__(hidden_states, rotary_pos_emb_list, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, encoder_hidden_states=None, encoder_attention_mask=None, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>rotary_pos_emb_list</code> <code>list[Array]</code> <p>list[chex.Array]: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <p>:param : Control the dropout in the self attention layer</p> <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        rotary_pos_emb_list: list[chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        encoder_hidden_states: Optional[chex.Array] = None,\n        encoder_attention_mask: Optional[chex.Array] = None,\n        fcm_mask: Optional[jnp.ndarray] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    Args:\n        self: Refer to the class instance itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            previous layer\n        rotary_pos_emb_list: list[chex.Array]: Pass in the frequency\n            information\n        attention_mask: chex.Array: Mask out the attention weights\n            for padding tokens\n        position_ids: chex.Array: Determine the position of each\n            token in the sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Control whether the dropout is applied\n            or not\n        init_cache: bool: Initialize the cache in the attention\n            layer\n        output_attentions: bool: Return the attention weights\n        fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n\n    Returns:\n        A tuple of two items\n    \"\"\"\n    # hidden_states: chex.Array\n    # rotary_pos_emb_list: list[chex.Array]\n    # attention_mask: chex.Array\n    # position_ids: chex.Array\n    # causal_mask: chex.Array\n    # deterministic: bool = True\n    # init_cache: bool = False\n    # output_attentions: bool = False\n    # encoder_hidden_states: Optional[chex.Array] = None\n    # encoder_attention_mask: Optional[chex.Array] = None\n    # fcm_mask = None\n\n    attn_outputs = self.attn(\n        self.ln_1(hidden_states),\n        rotary_pos_emb_list,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        encoder_attention_mask,\n        encoder_hidden_states,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n    hidden_states = hidden_states + attn_output\n\n    feed_forward_input = self.ln_2(hidden_states)\n\n    if self.config.use_scan_mlp:\n        feed_forward_input = einops.rearrange(\n            feed_forward_input,\n            '... (b s) d -&gt; ... b s d',\n            b=self.config.scan_mlp_chunk_size\n        )\n\n        def mlp_forward(mlp, carry, x):\n            return None, mlp(x, deterministic)\n\n        scan_axis = feed_forward_input.ndim - 3\n\n        _, feed_forward_hidden_states = nn.scan(\n            mlp_forward,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False, \"dropout\": True},\n            in_axes=scan_axis,\n            out_axes=scan_axis,\n        )(self.mlp, None, feed_forward_input)\n        feed_forward_hidden_states = einops.rearrange(\n            feed_forward_hidden_states,\n            '... b s d -&gt; ... (b s) d'\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n\n    return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1BlockCollection","title":"<code>FlaxQwen1BlockCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1BlockCollection(nn.Module):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxQwen1Block(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            rotary_pos_emb_list: list[chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model\n         in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module,\n        and return all outputs that are computed by this module.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the input tensor to the\n                encoder\n            rotary_pos_emb_list: chex.Array: Pass in the frequency of\n                each token\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Determine whether the model is in\n                training or evaluation mode\n            init_cache: bool: Initialize the cache for each layer\n            output_attentions: bool: Determine whether to output the\n                attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states of each layer\n            return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n\n        Returns:\n            A tuple of 3 values\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                rotary_pos_emb_list=rotary_pos_emb_list,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1BlockCollection.__call__","title":"<code>__call__(hidden_states, rotary_pos_emb_list, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model  in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>rotary_pos_emb_list</code> <code>list[Array]</code> <p>chex.Array: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <p>:param : Determine whether to use the forgetful causal mask</p> <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        rotary_pos_emb_list: list[chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model\n     in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module,\n    and return all outputs that are computed by this module.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the input tensor to the\n            encoder\n        rotary_pos_emb_list: chex.Array: Pass in the frequency of\n            each token\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Determine whether the model is in\n            training or evaluation mode\n        init_cache: bool: Initialize the cache for each layer\n        output_attentions: bool: Determine whether to output the\n            attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states of each layer\n        return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n\n    Returns:\n        A tuple of 3 values\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            rotary_pos_emb_list=rotary_pos_emb_list,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += (layer_outputs[1],)\n\n    outputs = (hidden_states, all_hidden_states, all_attentions)\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1ForCausalLMModule","title":"<code>FlaxQwen1ForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1ForCausalLMModule(nn.Module):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.transformer = FlaxQwen1Module(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass the input token ids to the model\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the input sequence\n            deterministic: bool: Control whether the model is trained or\n                not\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states\n            return_dict: bool: Return a dictionary of the outputs or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the word that we want to predict\n            None]]: Pass in the extra embedding\n\n        Returns:\n            The logits and the hidden states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"wte\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1ForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass the input token ids to the model\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the input sequence\n        deterministic: bool: Control whether the model is trained or\n            not\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states\n        return_dict: bool: Return a dictionary of the outputs or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the word that we want to predict\n        None]]: Pass in the extra embedding\n\n    Returns:\n        The logits and the hidden states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.transformer(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"wte\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1ForSequenceClassificationModule","title":"<code>FlaxQwen1ForSequenceClassificationModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1ForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        Args:\n            self: Access variables that belong to the class\n\n        Returns:\n            A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxQwen1Module(self.config, dtype=self.dtype)\n        self.classifier = Linear(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n            &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        Args:\n            self: Refer to the class instance\n            input_ids: chex.Array: Pass the input to the model\n            attention_mask: chex.Array: Specify which tokens are masked\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Control whether the model is run in\n                deterministic or stochastic mode\n            init_cache: bool: Initialize the cache for the transformer\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                h\n            return_dict: bool: Return a dictionary of outputs\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of a new word\n            None]]: Pass the extra embedding to the model\n\n        Returns:\n            A tuple of logits and hidden_states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1ForSequenceClassificationModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in all the inputs to the model and returns all outputs from it. The call function can be called directly on an instance of a class, or by using parentheses after an instance:     &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class     &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to call</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Specify which tokens are masked</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is run in deterministic or stochastic mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the transformer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all h</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of outputs</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of a new word</p> <code>None</code> <code>None]]</code> <p>Pass the extra embedding to the model</p> required <p>Returns:</p> Type Description <p>A tuple of logits and hidden_states</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module.\n    It takes in all the inputs to the model and returns all outputs from it.\n    The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n        &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n        &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n    Args:\n        self: Refer to the class instance\n        input_ids: chex.Array: Pass the input to the model\n        attention_mask: chex.Array: Specify which tokens are masked\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Control whether the model is run in\n            deterministic or stochastic mode\n        init_cache: bool: Initialize the cache for the transformer\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            h\n        return_dict: bool: Return a dictionary of outputs\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of a new word\n        None]]: Pass the extra embedding to the model\n\n    Returns:\n        A tuple of logits and hidden_states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n    prediction = self.classifier(hidden_states)\n    if return_dict:\n        return FlaxSequenceClassifierOutput(\n            logits=prediction,\n            hidden_states=hidden_states\n        )\n    else:\n        return prediction,\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1ForSequenceClassificationModule.setup","title":"<code>setup()</code>","text":"<p>The setup function is called once at the beginning of training. It initializes the model and optimizer, and sets up any other state that needs to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <p>Returns:</p> Type Description <p>A tuple of the model and the classifier</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def setup(self):\n    \"\"\"The setup function is called once at the beginning of training.\n    It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n    Args:\n        self: Access variables that belong to the class\n\n    Returns:\n        A tuple of the model and the classifier\n    \"\"\"\n    self.model = FlaxQwen1Module(self.config, dtype=self.dtype)\n    self.classifier = Linear(\n        self.num_classes,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        use_bias=False,\n        kernel_init=jax.nn.initializers.normal(\n            stddev=self.config.initializer_range),\n        precision=self.precision,\n    )\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1MLP","title":"<code>FlaxQwen1MLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1MLP(nn.Module):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.w1 = Linear(\n            config.intermediate_size // 2,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=not self.config.no_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.w2 = Linear(\n            config.intermediate_size // 2,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=not self.config.no_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.c_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=not self.config.no_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        x = self.c_proj(jax.nn.silu(self.w2(x)) * self.w1(x))\n        return x\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1MLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    x = self.c_proj(jax.nn.silu(self.w2(x)) * self.w1(x))\n    return x\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Module","title":"<code>FlaxQwen1Module</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1Module(nn.Module):\n    config: Qwen1Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.wte = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range\n            ),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.drop = flax.linen.Dropout(rate=self.config.emb_dropout_prob)\n        self.h = FlaxQwen1BlockCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.ln_f = Qwen1RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.layer_norm_epsilon,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        config = self.config\n        if config.rotary_pct == 1.0:\n            self.rotary_ndims = None\n        else:\n            assert config.rotary_pct &lt; 1\n            self.rotary_ndims = int(\n                config.kv_channels * config.rotary_pct\n            )\n        self.causal_mask = make_causal_mask(\n            jnp.ones(\n                (1, getattr(config, \"c_max_position_embeddings\", config.seq_length)),\n                dtype=\"bool\"),\n            dtype=\"bool\"\n        )\n        self.rope_cache = compute_qwen1_rope(\n            dim=self.rotary_ndims if self.rotary_ndims is not None else config.kv_channels,\n            base=self.config.rotary_emb_base,\n            seqlen=getattr(config, \"freq_max_position_embeddings\", config.seq_length)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input token ids\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in a sequence\n            deterministic: bool: Control whether dropout is applied or\n                not\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attentions or not\n            output_hidden_states: bool: Determine whether to return\n                hidden states\n            return_dict: bool: Return a dictionary of the output or not\n            extra_embedding: Optional[Union[jnp.ndarray, None]]: Pass in\n                the embedding of the\n\n        Returns:\n            A tuple of:\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.wte(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length, _ = inputs_embeds.shape\n        kv_seq_len = sequence_length\n\n        if self.h.blocks[0].attn.has_variable(\"cache\", \"cached_key\"):\n            cache_index = self.h.blocks[0].attn.get_variable(\n                \"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32)\n            )\n            kv_seq_len += cache_index\n\n        # if deterministic or not self.config.use_dynamic_ntk:\n        #     ntk_alpha_list = [1.0]\n        # elif kv_seq_len != inputs_embeds.shape[1]:\n        #     ntk_alpha_list = self.rotary_emb._ntk_alpha_cached_list\n        # else:\n        #     ntk_alpha_list = []\n        #     if attention_mask is not None and kv_seq_len &gt; self.seq_length:\n        #         true_seq_lens = jnp.sum(attention_mask.reshape(batch_size, 1, 1, -1) == 0, axis=-1, dtype=jnp.float32)\n        #         for i in range(inputs_embeds.shape[0]):\n        #             true_seq_len = true_seq_lens[i].item()\n        #             ntk_alpha = self.get_ntk_alpha(true_seq_len)\n        #             ntk_alpha_list.append(ntk_alpha)\n        #     else:\n        #         ntk_alpha = self.get_ntk_alpha(kv_seq_len)\n        #         ntk_alpha_list.append(ntk_alpha)\n        # self.rotary_emb.set_ntk_alpha_cached_list(ntk_alpha_list)\n        # rotary_pos_emb_list = []\n        assert sequence_length &lt;= self.config.seq_length, \"Maximum Position Embedding Reached !\"\n        inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n        hidden_states = self.drop(\n            inputs_embeds, deterministic=deterministic\n        )\n\n        outputs = self.h(\n            hidden_states=hidden_states,\n            rotary_pos_emb_list=[self.rope_cache],\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.ln_f(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1Module.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids and returns the output of the model. The call function also has optional arguments that can be used to control the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when calling a Flax model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input token ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether dropout is applied or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attentions or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the output or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray, None]]: Pass in the embedding of the</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of:</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n    and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n    the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n    calling a Flax model.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input token ids\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in a sequence\n        deterministic: bool: Control whether dropout is applied or\n            not\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attentions or not\n        output_hidden_states: bool: Determine whether to return\n            hidden states\n        return_dict: bool: Return a dictionary of the output or not\n        extra_embedding: Optional[Union[jnp.ndarray, None]]: Pass in\n            the embedding of the\n\n    Returns:\n        A tuple of:\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.wte(input_ids.astype(\"i4\"))\n\n    batch_size, sequence_length, _ = inputs_embeds.shape\n    kv_seq_len = sequence_length\n\n    if self.h.blocks[0].attn.has_variable(\"cache\", \"cached_key\"):\n        cache_index = self.h.blocks[0].attn.get_variable(\n            \"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32)\n        )\n        kv_seq_len += cache_index\n\n    # if deterministic or not self.config.use_dynamic_ntk:\n    #     ntk_alpha_list = [1.0]\n    # elif kv_seq_len != inputs_embeds.shape[1]:\n    #     ntk_alpha_list = self.rotary_emb._ntk_alpha_cached_list\n    # else:\n    #     ntk_alpha_list = []\n    #     if attention_mask is not None and kv_seq_len &gt; self.seq_length:\n    #         true_seq_lens = jnp.sum(attention_mask.reshape(batch_size, 1, 1, -1) == 0, axis=-1, dtype=jnp.float32)\n    #         for i in range(inputs_embeds.shape[0]):\n    #             true_seq_len = true_seq_lens[i].item()\n    #             ntk_alpha = self.get_ntk_alpha(true_seq_len)\n    #             ntk_alpha_list.append(ntk_alpha)\n    #     else:\n    #         ntk_alpha = self.get_ntk_alpha(kv_seq_len)\n    #         ntk_alpha_list.append(ntk_alpha)\n    # self.rotary_emb.set_ntk_alpha_cached_list(ntk_alpha_list)\n    # rotary_pos_emb_list = []\n    assert sequence_length &lt;= self.config.seq_length, \"Maximum Position Embedding Reached !\"\n    inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n    hidden_states = self.drop(\n        inputs_embeds, deterministic=deterministic\n    )\n\n    outputs = self.h(\n        hidden_states=hidden_states,\n        rotary_pos_emb_list=[self.rope_cache],\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        causal_mask=self.causal_mask,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.ln_f(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(v for v in outputs if v is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel","title":"<code>FlaxQwen1PreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>class FlaxQwen1PreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = Qwen1Config\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: Qwen1Config,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: Qwen1Config: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the input\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of h in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    # def init_rope(self, batch_size, max_length):\n    #     \"\"\"\n    #     The init_rope function is used to initialize the rope for a given batch size and sequence length.\n    #     The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    #\n    #     :param self: Access the module\n    #     :param batch_size: Define the batch size of the input tensors\n    #     :param max_length: Set the length of the input sequence\n    #     \"\"\"\n    #     input_ids = jnp.ones((batch_size, max_length))\n    #     attention_mask = jnp.ones_like(input_ids)\n    #     position_ids = jnp.broadcast_to(jnp.arange(\n    #         jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n    #\n    #     init_variables = self.module.init(\n    #         jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    #     )\n    #     return init_variables[\"rope_cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            # past_rope_cache: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input\n            position_ids: chex.Array: Create the positional embeddings\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass in the past key values from a\n                previous call to __call__\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all h\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.seq_length, \"Maximum Position Embedding Reached !\"\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n        mutable = False\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n\n        # if past_rope_cache is not None:\n        #     inputs[\"rope_cache\"] = past_rope_cache\n        # elif self.config.init_rope_cache_auto:\n        #     inputs[\"rope_cache\"] = self.init_rope(batch_size=batch_size, max_length=sequence_length)\n        # else:\n        #     raise ValueError(\n        #         \"if you are setting `init_rope_cache_auto=False` you should pass `rope_cache` beside param\"\n        #     )\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n        # if return_dict:\n        #     outputs[\"past_rope_cache\"] = unfreeze(rope_cache[\"rope_cache\"])\n        # else:\n        #     outputs = outputs, unfreeze(rope_cache[\"rope_cache\"])\n        return outputs\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": self.init_cache(batch_size, max_length),\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n            # \"past_rope_cache\": self.init_rope(batch_size=batch_size, max_length=max_length)\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        # model_kwargs[\"past_rope_cache\"] = model_outputs.past_rope_cache\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=True, extra_embedding=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all h</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        # past_rope_cache: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input\n        position_ids: chex.Array: Create the positional embeddings\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass in the past key values from a\n            previous call to __call__\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all h\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.seq_length, \"Maximum Position Embedding Reached !\"\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    if self.config.bits is not None:\n        rngs['params'] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n    mutable = False\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n\n    # if past_rope_cache is not None:\n    #     inputs[\"rope_cache\"] = past_rope_cache\n    # elif self.config.init_rope_cache_auto:\n    #     inputs[\"rope_cache\"] = self.init_rope(batch_size=batch_size, max_length=sequence_length)\n    # else:\n    #     raise ValueError(\n    #         \"if you are setting `init_rope_cache_auto=False` you should pass `rope_cache` beside param\"\n    #     )\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n    # if return_dict:\n    #     outputs[\"past_rope_cache\"] = unfreeze(rope_cache[\"rope_cache\"])\n    # else:\n    #     outputs = outputs, unfreeze(rope_cache[\"rope_cache\"])\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>Qwen1Config</code> <p>Qwen1Config: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of h in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def __init__(\n        self,\n        config: Qwen1Config,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: Qwen1Config: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the input\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of h in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-qwen1-modelling_qwen1_flax/#src.python.easydel.modules.qwen1.modelling_qwen1_flax.FlaxQwen1PreTrainedModel.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/qwen1/modelling_qwen1_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = jax.lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": self.init_cache(batch_size, max_length),\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n        # \"past_rope_cache\": self.init_rope(batch_size=batch_size, max_length=max_length)\n    }\n</code></pre>"},{"location":"generated-modules-qwen1-qwen1_configuration/","title":"modules.qwen1.qwen1_configuration","text":""},{"location":"generated-modules-qwen1-qwen1_configuration/#src.python.easydel.modules.qwen1.qwen1_configuration.Qwen1Config","title":"<code>Qwen1Config</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/qwen1/qwen1_configuration.py</code> <pre><code>class Qwen1Config(EasyDeLPretrainedConfig):\n    model_type: str = \"qwen\"\n\n    def __init__(\n            self,\n            vocab_size=151936,\n            hidden_size=4096,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            emb_dropout_prob=0.0,\n            attn_dropout_prob=0.0,\n            layer_norm_epsilon=1e-6,\n            initializer_range=0.02,\n            seq_length=8192,\n            scale_attn_weights=True,\n            use_cache=True,\n            kv_channels=128,\n            rotary_pct=1.0,\n            rotary_emb_base=10000,\n            use_dynamic_ntk=True,\n            use_logn_attn=True,\n            intermediate_size=22016,\n            no_bias=True,\n            tie_word_embeddings=False,\n            softmax_in_fp32=False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            scan_layers: bool = True,\n            init_rope_cache_auto: bool = False,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.seq_length = seq_length\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.scale_attn_weights = scale_attn_weights\n        self.no_bias = no_bias\n        self.kv_channels = kv_channels\n        self.use_dynamic_ntk = use_dynamic_ntk\n        self.use_logn_attn = use_logn_attn\n        self.rotary_emb_base = rotary_emb_base\n        self.rotary_pct = rotary_pct\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.softmax_in_fp32 = softmax_in_fp32\n        self.initializer_range = initializer_range\n        self.use_cache = use_cache\n        self.scan_layers = scan_layers\n        self.emb_dropout_prob = emb_dropout_prob\n        self.attn_dropout_prob = attn_dropout_prob\n        self.init_rope_cache_auto = init_rope_cache_auto\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/wte/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/c_attn/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/c_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\")), \"tp\"),\n            (\"mlp/c_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"ln_1/kernel\", PartitionSpec(None)),\n            (\"ln_2/kernel\", PartitionSpec(None)),\n\n            (\"model/ln_f/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/wte/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/c_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"ln_1/kernel\", PartitionSpec(None)),\n            (\"ln_2/kernel\", PartitionSpec(None)),\n\n            (\"model/ln_f/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec(None)),\n\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            scan_layers: bool = True,\n            init_rope_cache_auto: bool = False,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or not\n            scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n            init_rope_cache_auto: bool: Whether to use the\n                rope_cache_auto in model\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n            scan_layers: bool: Determine whether to use scan layers or\n                not\n\n        Returns:\n            The following:\n        \"\"\"\n        self.scan_layers = scan_layers\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.init_rope_cache_auto = init_rope_cache_auto\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return \"params\", \"dropout\", \"fcm\"\n</code></pre>"},{"location":"generated-modules-qwen1-qwen1_configuration/#src.python.easydel.modules.qwen1.qwen1_configuration.Qwen1Config.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_scan_mlp=False, scan_mlp_chunk_size=1024, bits=None, scan_layers=True, init_rope_cache_auto=False, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Set the chunk size for scan_mlp</p> <code>1024</code> <code>init_rope_cache_auto</code> <code>bool</code> <p>bool: Whether to use the rope_cache_auto in model</p> <code>False</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use scan layers or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The following:</p> Source code in <code>src/python/easydel/modules/qwen1/qwen1_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        scan_layers: bool = True,\n        init_rope_cache_auto: bool = False,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or not\n        scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n        init_rope_cache_auto: bool: Whether to use the\n            rope_cache_auto in model\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n        scan_layers: bool: Determine whether to use scan layers or\n            not\n\n    Returns:\n        The following:\n    \"\"\"\n    self.scan_layers = scan_layers\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n    self.init_rope_cache_auto = init_rope_cache_auto\n</code></pre>"},{"location":"generated-modules-qwen1-qwen1_configuration/#src.python.easydel.modules.qwen1.qwen1_configuration.Qwen1Config.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/qwen1/qwen1_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/wte/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/c_attn/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/c_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\")), \"tp\"),\n        (\"mlp/c_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"ln_1/kernel\", PartitionSpec(None)),\n        (\"ln_2/kernel\", PartitionSpec(None)),\n\n        (\"model/ln_f/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/wte/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/c_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"ln_1/kernel\", PartitionSpec(None)),\n        (\"ln_2/kernel\", PartitionSpec(None)),\n\n        (\"model/ln_f/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\".*\", PartitionSpec(None)),\n\n    )\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/","title":"modules.qwen2.modelling_qwen_flax","text":""},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Attention","title":"<code>FlaxQwen2Attention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2Attention(BaseJAXAttentionModule):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxQwen2Embedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n        self.resid_dropout = flax.linen.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(\n            attn_output, deterministic=deterministic)\n        outputs = (\n            attn_output, attentions.attention_weights\n        ) if output_attentions else (\n            attn_output,\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Attention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    attn_output = self.resid_dropout(\n        attn_output, deterministic=deterministic)\n    outputs = (\n        attn_output, attentions.attention_weights\n    ) if output_attentions else (\n        attn_output,\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Attention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Block","title":"<code>FlaxQwen2Block</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2Block(nn.Module):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxQwen2Attention\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = nn_partitioning.remat(\n                FlaxQwen2Attention, static_argnums=(1, 3, 4, 6, 7, 8, 9),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxQwen2MLP\n\n        if self.config.gradient_checkpointing != \"\":\n            mlp_block = nn_partitioning.remat(\n                FlaxQwen2MLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = Qwen2RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.post_attention_layernorm = Qwen2RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        Args:\n            self: Refer to the class instance itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency information\n            attention_mask: chex.Array: Mask out the attention weights\n                for padding tokens\n            position_ids: chex.Array: Determine the position of each\n                token in the sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Control whether the dropout is applied\n                or not\n            init_cache: bool: Initialize the cache in the attention\n                layer\n            output_attentions: bool: Return the attention weights\n            fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n\n        Returns:\n            A tuple of two items\n        \"\"\"\n        attn_outputs = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n        if self.config.use_scan_mlp:\n            feed_forward_hidden_states = block_wise_ffn(\n                self.mlp,\n                feed_forward_input,\n                self.config.scan_mlp_chunk_size,\n                deterministic\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Block.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <p>:param : Control the dropout in the self attention layer</p> <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask: Optional[jnp.ndarray] = None,\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    Args:\n        self: Refer to the class instance itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency information\n        attention_mask: chex.Array: Mask out the attention weights\n            for padding tokens\n        position_ids: chex.Array: Determine the position of each\n            token in the sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Control whether the dropout is applied\n            or not\n        init_cache: bool: Initialize the cache in the attention\n            layer\n        output_attentions: bool: Return the attention weights\n        fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n\n    Returns:\n        A tuple of two items\n    \"\"\"\n    attn_outputs = self.self_attn(\n        self.input_layernorm(hidden_states),\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n    hidden_states = hidden_states + attn_output\n\n    feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n    if self.config.use_scan_mlp:\n        feed_forward_hidden_states = block_wise_ffn(\n            self.mlp,\n            feed_forward_input,\n            self.config.scan_mlp_chunk_size,\n            deterministic\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n\n    return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2BlockCollection","title":"<code>FlaxQwen2BlockCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2BlockCollection(nn.Module):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxQwen2Block(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model\n         in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module,\n        and return all outputs that are computed by this module.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the input tensor to the\n                encoder\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency of each token\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Determine whether the model is in\n                training or evaluation mode\n            init_cache: bool: Initialize the cache for each layer\n            output_attentions: bool: Determine whether to output the\n                attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states of each layer\n            return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n\n        Returns:\n            A tuple of 3 values\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2BlockCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model  in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <p>:param : Determine whether to use the forgetful causal mask</p> <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model\n     in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module,\n    and return all outputs that are computed by this module.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the input tensor to the\n            encoder\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency of each token\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Determine whether the model is in\n            training or evaluation mode\n        init_cache: bool: Initialize the cache for each layer\n        output_attentions: bool: Determine whether to output the\n            attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states of each layer\n        return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n\n    Returns:\n        A tuple of 3 values\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += (layer_outputs[1],)\n\n    outputs = (hidden_states, all_hidden_states, all_attentions)\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForCausalLM","title":"<code>FlaxQwen2ForCausalLM</code>","text":"<p>               Bases: <code>FlaxQwen2PreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2ForCausalLM(FlaxQwen2PreTrainedModel):\n    module_class = FlaxQwen2ForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForCausalLMModule","title":"<code>FlaxQwen2ForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2ForCausalLMModule(nn.Module):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxQwen2Module(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass the input token ids to the model\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the input sequence\n            deterministic: bool: Control whether the model is trained or\n                not\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states\n            return_dict: bool: Return a dictionary of the outputs or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the word that we want to predict\n            None]]: Pass in the extra embedding\n\n        Returns:\n            The logits and the hidden states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass the input token ids to the model\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the input sequence\n        deterministic: bool: Control whether the model is trained or\n            not\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states\n        return_dict: bool: Return a dictionary of the outputs or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the word that we want to predict\n        None]]: Pass in the extra embedding\n\n    Returns:\n        The logits and the hidden states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        lm_logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForSequenceClassificationModule","title":"<code>FlaxQwen2ForSequenceClassificationModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2ForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        Args:\n            self: Access variables that belong to the class\n\n        Returns:\n            A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxQwen2Module(self.config, dtype=self.dtype)\n        self.classifier = Linear(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n            &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        Args:\n            self: Refer to the class instance\n            input_ids: chex.Array: Pass the input to the model\n            attention_mask: chex.Array: Specify which tokens are masked\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Control whether the model is run in\n                deterministic or stochastic mode\n            init_cache: bool: Initialize the cache for the transformer\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of outputs\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of a new word\n            None]]: Pass the extra embedding to the model\n\n        Returns:\n            A tuple of logits and hidden_states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForSequenceClassificationModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in all the inputs to the model and returns all outputs from it. The call function can be called directly on an instance of a class, or by using parentheses after an instance:     &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class     &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to call</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Specify which tokens are masked</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is run in deterministic or stochastic mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the transformer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of outputs</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of a new word</p> <code>None</code> <code>None]]</code> <p>Pass the extra embedding to the model</p> required <p>Returns:</p> Type Description <p>A tuple of logits and hidden_states</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module.\n    It takes in all the inputs to the model and returns all outputs from it.\n    The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n        &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n        &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n    Args:\n        self: Refer to the class instance\n        input_ids: chex.Array: Pass the input to the model\n        attention_mask: chex.Array: Specify which tokens are masked\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Control whether the model is run in\n            deterministic or stochastic mode\n        init_cache: bool: Initialize the cache for the transformer\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of outputs\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of a new word\n        None]]: Pass the extra embedding to the model\n\n    Returns:\n        A tuple of logits and hidden_states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n    prediction = self.classifier(hidden_states)\n    if return_dict:\n        return FlaxSequenceClassifierOutput(\n            logits=prediction,\n            hidden_states=hidden_states\n        )\n    else:\n        return prediction,\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2ForSequenceClassificationModule.setup","title":"<code>setup()</code>","text":"<p>The setup function is called once at the beginning of training. It initializes the model and optimizer, and sets up any other state that needs to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <p>Returns:</p> Type Description <p>A tuple of the model and the classifier</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def setup(self):\n    \"\"\"The setup function is called once at the beginning of training.\n    It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n    Args:\n        self: Access variables that belong to the class\n\n    Returns:\n        A tuple of the model and the classifier\n    \"\"\"\n    self.model = FlaxQwen2Module(self.config, dtype=self.dtype)\n    self.classifier = Linear(\n        self.num_classes,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        use_bias=False,\n        kernel_init=jax.nn.initializers.normal(\n            stddev=self.config.initializer_range),\n        precision=self.precision,\n    )\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2MLP","title":"<code>FlaxQwen2MLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2MLP(nn.Module):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.gate_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.dropout = flax.linen.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        x = self.dropout(x, deterministic=deterministic)\n        return x\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2MLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n    x = self.dropout(x, deterministic=deterministic)\n    return x\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Module","title":"<code>FlaxQwen2Module</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2Module(nn.Module):\n    config: Qwen2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range\n            ),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.dropout = flax.linen.Dropout(rate=self.config.embd_pdrop)\n        self.layers = FlaxQwen2BlockCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = Qwen2RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        config = self.config\n        self.causal_mask = make_causal_mask(\n            jnp.ones(\n                (1, getattr(config, \"c_max_position_embeddings\", config.max_position_embeddings)), dtype=\"bool\"\n            ), dtype=\"bool\"\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if config.rope_scaling is not None:\n            scaling_type = config.rope_scaling[\"type\"]\n            scaling_factor = config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=(\n                getattr(self.config, \"freq_max_position_embeddings\", self.config.max_position_embeddings)\n            ),\n            dim=config.hidden_size // config.num_attention_heads,\n            base=config.rope_theta,\n            **initial_rope_kwargs\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input token ids\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Indicate the position of each\n                token in a sequence\n            deterministic: bool: Control whether dropout is applied or\n                not\n            inputs_embeds: chex.Array: Pass in the embeddings of the\n                input tokens\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attentions or not\n            output_hidden_states: bool: Determine whether to return\n                hidden states\n            return_dict: bool: Return a dictionary of the output or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the\n            None]]: Pass in the extra embedding\n\n        Returns:\n            A tuple of:\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length, _ = inputs_embeds.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n        inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n        hidden_states = self.dropout(\n            inputs_embeds, deterministic=deterministic)\n\n        outputs = self.layers(\n            hidden_states=hidden_states,\n            freq_cis=self.freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2Module.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, inputs_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids and returns the output of the model. The call function also has optional arguments that can be used to control the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when calling a Flax model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input token ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether dropout is applied or not</p> <code>True</code> <code>inputs_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attentions or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the output or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>A tuple of:</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        inputs_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n    and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n    the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n    calling a Flax model.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input token ids\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Indicate the position of each\n            token in a sequence\n        deterministic: bool: Control whether dropout is applied or\n            not\n        inputs_embeds: chex.Array: Pass in the embeddings of the\n            input tokens\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attentions or not\n        output_hidden_states: bool: Determine whether to return\n            hidden states\n        return_dict: bool: Return a dictionary of the output or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the\n        None]]: Pass in the extra embedding\n\n    Returns:\n        A tuple of:\n    \"\"\"\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n    batch_size, sequence_length, _ = inputs_embeds.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n    inputs_embeds = inputs_embeds + extra_embedding if extra_embedding is not None else inputs_embeds\n    hidden_states = self.dropout(\n        inputs_embeds, deterministic=deterministic)\n\n    outputs = self.layers(\n        hidden_states=hidden_states,\n        freq_cis=self.freq_cis,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        causal_mask=self.causal_mask,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(v for v in outputs if v is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2PreTrainedModel","title":"<code>FlaxQwen2PreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>class FlaxQwen2PreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = Qwen2Config\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: Qwen2Config,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: Qwen2Config: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the input\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of layers in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input\n            position_ids: chex.Array: Create the positional embeddings\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass in the past key values from a\n                previous call to __call__\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all layers\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2PreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, extra_embedding=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input\n        position_ids: chex.Array: Create the positional embeddings\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass in the past key values from a\n            previous call to __call__\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all layers\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    if self.config.bits is not None:\n        rngs['params'] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2PreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>Qwen2Config</code> <p>Qwen2Config: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of layers in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def __init__(\n        self,\n        config: Qwen2Config,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: Qwen2Config: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the input\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of layers in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2PreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-qwen2-modelling_qwen_flax/#src.python.easydel.modules.qwen2.modelling_qwen_flax.FlaxQwen2PreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/qwen2/modelling_qwen_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-qwen2-qwen_configuration/","title":"modules.qwen2.qwen_configuration","text":""},{"location":"generated-modules-qwen2-qwen_configuration/#src.python.easydel.modules.qwen2.qwen_configuration.Qwen2Config","title":"<code>Qwen2Config</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/qwen2/qwen_configuration.py</code> <pre><code>class Qwen2Config(EasyDeLPretrainedConfig):\n    model_type: str = \"qwen2\"\n\n    def __init__(\n            self,\n            vocab_size=151936,\n            hidden_size=4096,\n            intermediate_size=22016,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=32,\n            hidden_act=\"silu\",\n            max_position_embeddings=32768,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            use_sliding_window=False,\n            sliding_window=4096,\n            max_window_layers=28,\n            attention_dropout=0.0,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = 0.0,\n            fcm_max_ratio: float = 0.0,\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            scan_layers: bool = True,\n            rope_scaling: Optional[Mapping[str, str | float]] = None,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.use_sliding_window = use_sliding_window\n        self.sliding_window = sliding_window\n        self.max_window_layers = max_window_layers\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.rope_scaling = rope_scaling\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.scan_layers = scan_layers\n        self.embd_pdrop = embd_pdrop\n        self.number_rep_kv = number_rep_kv\n        self.resid_pdrop = resid_pdrop\n        self.attention_dropout = attention_dropout\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            use_scan_mlp=use_scan_mlp,\n            scan_mlp_chunk_size=scan_mlp_chunk_size,\n            bits=bits,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attention_dropout: float = 0.0,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = 0.0,\n            fcm_max_ratio: float = 0.0,\n            use_scan_mlp: bool = False,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            bits: Optional[int] = None,\n            rope_theta: float = 10000.,\n            hidden_act: str = \"silu\",\n            scan_layers: bool = True,\n            rope_scaling: Optional[Mapping[str, str | float]] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            resid_pdrop: float: Set the dropout rate for residual\n                connections\n            embd_pdrop: float: Set the probability of dropping an\n                embedding\n            attention_dropout: float: Set the probability of dropping\n                out the attention layer\n            tie_word_embeddings: bool: Tie the word embeddings to the\n                decoder\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            fcm_min_ratio: float: Control the minimum ratio of the\n                number of chunks to be used in flash-based computation\n            fcm_max_ratio: float: Set the maximum ratio of the number of\n                input tokens to output tokens\n            use_scan_mlp: bool: Determine whether to use the scan_mlp\n                function or not\n            scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n            number_rep_kv: int: Determine how many times the key and\n                value vectors are repeated\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n            rope_theta: float : rope_theta for compute rope\n            hidden_act: str : hidden_act for mlp\n            scan_layers: bool: Determine whether to use scan layers or\n                not\n\n        Returns:\n            The following:\n        \"\"\"\n        self.scan_layers = scan_layers\n        self.embd_pdrop = embd_pdrop\n        self.number_rep_kv = number_rep_kv\n        self.resid_pdrop = resid_pdrop\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n\n        self.use_scan_mlp = use_scan_mlp\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return \"params\", \"dropout\", \"fcm\"\n</code></pre>"},{"location":"generated-modules-qwen2-qwen_configuration/#src.python.easydel.modules.qwen2.qwen_configuration.Qwen2Config.add_jax_args","title":"<code>add_jax_args(resid_pdrop=0.0, embd_pdrop=0.0, attention_dropout=0.0, tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', fcm_min_ratio=0.0, fcm_max_ratio=0.0, use_scan_mlp=False, scan_mlp_chunk_size=1024, number_rep_kv=1, bits=None, rope_theta=10000.0, hidden_act='silu', scan_layers=True, rope_scaling=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>resid_pdrop</code> <code>float</code> <p>float: Set the dropout rate for residual connections</p> <code>0.0</code> <code>embd_pdrop</code> <code>float</code> <p>float: Set the probability of dropping an embedding</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>float: Set the probability of dropping out the attention layer</p> <code>0.0</code> <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>fcm_min_ratio</code> <code>float</code> <p>float: Control the minimum ratio of the number of chunks to be used in flash-based computation</p> <code>0.0</code> <code>fcm_max_ratio</code> <code>float</code> <p>float: Set the maximum ratio of the number of input tokens to output tokens</p> <code>0.0</code> <code>use_scan_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Set the chunk size for scan_mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Determine how many times the key and value vectors are repeated</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> <code>rope_theta</code> <code>float</code> <p>float : rope_theta for compute rope</p> <code>10000.0</code> <code>hidden_act</code> <code>str</code> <p>str : hidden_act for mlp</p> <code>'silu'</code> <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use scan layers or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The following:</p> Source code in <code>src/python/easydel/modules/qwen2/qwen_configuration.py</code> <pre><code>def add_jax_args(\n        self,\n        resid_pdrop: float = 0.0,\n        embd_pdrop: float = 0.0,\n        attention_dropout: float = 0.0,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        fcm_min_ratio: float = 0.0,\n        fcm_max_ratio: float = 0.0,\n        use_scan_mlp: bool = False,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        bits: Optional[int] = None,\n        rope_theta: float = 10000.,\n        hidden_act: str = \"silu\",\n        scan_layers: bool = True,\n        rope_scaling: Optional[Mapping[str, str | float]] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        resid_pdrop: float: Set the dropout rate for residual\n            connections\n        embd_pdrop: float: Set the probability of dropping an\n            embedding\n        attention_dropout: float: Set the probability of dropping\n            out the attention layer\n        tie_word_embeddings: bool: Tie the word embeddings to the\n            decoder\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        fcm_min_ratio: float: Control the minimum ratio of the\n            number of chunks to be used in flash-based computation\n        fcm_max_ratio: float: Set the maximum ratio of the number of\n            input tokens to output tokens\n        use_scan_mlp: bool: Determine whether to use the scan_mlp\n            function or not\n        scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n        number_rep_kv: int: Determine how many times the key and\n            value vectors are repeated\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n        rope_theta: float : rope_theta for compute rope\n        hidden_act: str : hidden_act for mlp\n        scan_layers: bool: Determine whether to use scan layers or\n            not\n\n    Returns:\n        The following:\n    \"\"\"\n    self.scan_layers = scan_layers\n    self.embd_pdrop = embd_pdrop\n    self.number_rep_kv = number_rep_kv\n    self.resid_pdrop = resid_pdrop\n    self.rope_theta = rope_theta\n    self.rope_scaling = rope_scaling\n    self.attention_dropout = attention_dropout\n    self.hidden_act = hidden_act\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.fcm_min_ratio = fcm_min_ratio\n    self.fcm_max_ratio = fcm_max_ratio\n\n    self.use_scan_mlp = use_scan_mlp\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-qwen2-qwen_configuration/#src.python.easydel.modules.qwen2.qwen_configuration.Qwen2Config.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/qwen2/qwen_configuration.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-qwen2_moe-configuration_qwen2_moe/","title":"modules.qwen2_moe.configuration_qwen2_moe","text":""},{"location":"generated-modules-qwen2_moe-configuration_qwen2_moe/#src.python.easydel.modules.qwen2_moe.configuration_qwen2_moe.Qwen2MoeConfig","title":"<code>Qwen2MoeConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/configuration_qwen2_moe.py</code> <pre><code>class Qwen2MoeConfig(EasyDeLPretrainedConfig):\n    model_type: str = \"qwen2_moe\"\n\n    def __init__(\n            self,\n            vocab_size=151936,\n            hidden_size=2048,\n            intermediate_size=5632,\n            num_hidden_layers=24,\n            num_attention_heads=16,\n            num_key_value_heads=16,\n            hidden_act=\"silu\",\n            max_position_embeddings=32768,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            use_sliding_window=False,\n            sliding_window=4096,\n            max_window_layers=28,\n            attention_dropout=0.0,\n            decoder_sparse_step=1,\n            moe_intermediate_size=1408,\n            shared_expert_intermediate_size=5632,\n            num_experts_per_tok=4,\n            num_experts=60,\n            norm_topk_prob=False,\n            output_router_logits=False,\n            router_aux_loss_coef=0.001,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.use_sliding_window = use_sliding_window\n        self.sliding_window = sliding_window\n        self.max_window_layers = max_window_layers\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.attention_dropout = attention_dropout\n\n        # MoE arguments\n        self.decoder_sparse_step = decoder_sparse_step\n        self.moe_intermediate_size = moe_intermediate_size\n        self.shared_expert_intermediate_size = shared_expert_intermediate_size\n        self.num_experts_per_tok = num_experts_per_tok\n        self.num_experts = num_experts\n        self.norm_topk_prob = norm_topk_prob\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        Args:\n            fully_sharded_data_parallel: bool: Determine whether to\n                partition the model fully or not\n\n        Returns:\n            A list of tuples\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"shared_expert_gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"shared_expert_gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n        Args:\n            self: Refer to the current object\n            gradient_checkpointing: str: Control the amount of memory\n                used by jax\n            bits: Optional[int]: Determine the number of bits used in\n                the quantization\n\n        Returns:\n            The following:\n        \"\"\"\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return \"params\", \"dropout\"\n</code></pre>"},{"location":"generated-modules-qwen2_moe-configuration_qwen2_moe/#src.python.easydel.modules.qwen2_moe.configuration_qwen2_moe.Qwen2MoeConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', bits=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> <p>Returns:</p> Type Description <p>The following:</p> Source code in <code>src/python/easydel/modules/qwen2_moe/configuration_qwen2_moe.py</code> <pre><code>def add_jax_args(\n        self,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        bits: Optional[int] = None,\n        **kwargs,\n):\n    \"\"\"The add_jax_args function adds the following arguments to the Transformer class:\n\n    Args:\n        self: Refer to the current object\n        gradient_checkpointing: str: Control the amount of memory\n            used by jax\n        bits: Optional[int]: Determine the number of bits used in\n            the quantization\n\n    Returns:\n        The following:\n    \"\"\"\n    self.gradient_checkpointing = gradient_checkpointing\n    self.bits = bits\n</code></pre>"},{"location":"generated-modules-qwen2_moe-configuration_qwen2_moe/#src.python.easydel.modules.qwen2_moe.configuration_qwen2_moe.Qwen2MoeConfig.get_partition_rules","title":"<code>get_partition_rules(fully_sharded_data_parallel=True)</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>src/python/easydel/modules/qwen2_moe/configuration_qwen2_moe.py</code> <pre><code>def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n    \"\"\"The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    Args:\n        fully_sharded_data_parallel: bool: Determine whether to\n            partition the model fully or not\n\n    Returns:\n        A list of tuples\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n        (\"up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"shared_expert_gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec(None)),\n    ) if not fully_sharded_data_parallel else (\n\n        (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n        (\"gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"shared_expert_gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n        (\"gate/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n        (\"input_layernorm/kernel\", PartitionSpec(None)),\n        (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n        (\"model/norm/kernel\", PartitionSpec(None)),\n        (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n        (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n    )\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/","title":"modules.qwen2_moe.modeling_qwen2_moe_flax","text":""},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeAttention","title":"<code>FlaxQwen2MoeAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeAttention(BaseJAXAttentionModule):\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.num_key_value_groups = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=True,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxQwen2MoeEmbedding(self.dtype)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n        self.resid_dropout = flax.linen.Dropout(rate=config.attention_dropout)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query, key and value tensors\n            sequence_length: Reshape the query, key and value tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query, key and value\n        \"\"\"\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n        )\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n\n        if self.config.use_sharding_constraint:\n            query_states = with_sharding_constraint(\n                query_states,\n                jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n            )\n            key_states = with_sharding_constraint(key_states,\n                                                  jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n            value_states = with_sharding_constraint(value_states,\n                                                    jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=attention_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(\n            attn_output, deterministic=deterministic)\n        outputs = (\n            attn_output, attentions.attention_weights\n        ) if output_attentions else (\n            attn_output,\n        )\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                 query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n\n    if self.config.use_sharding_constraint:\n        query_states = with_sharding_constraint(\n            query_states,\n            jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        )\n        key_states = with_sharding_constraint(key_states,\n                                              jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n        value_states = with_sharding_constraint(value_states,\n                                                jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n    attention_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=attention_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n\n    attn_output = self.resid_dropout(\n        attn_output, deterministic=deterministic)\n    outputs = (\n        attn_output, attentions.attention_weights\n    ) if output_attentions else (\n        attn_output,\n    )\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query, key and value tensors\n        sequence_length: Reshape the query, key and value tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query, key and value\n    \"\"\"\n    query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n    query, key = self.rotary(\n        position_ids=position_ids, query=query, key=key, freq_cis=freq_cis\n    )\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeBlock","title":"<code>FlaxQwen2MoeBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeBlock(nn.Module):\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxQwen2MoeAttention\n        if self.config.gradient_checkpointing != \"\":\n            attn_block = nn_partitioning.remat(\n                FlaxQwen2MoeAttention, static_argnums=(1, 3, 4, 6, 7, 8, 9),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxQwen2MoeSparseMoeBlock if self.config.num_experts &gt; 0 else FlaxQwen2MoeMLP\n\n        if self.config.gradient_checkpointing != \"\":\n            mlp_block = nn_partitioning.remat(\n                mlp_block, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = Qwen2MoeRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.post_attention_layernorm = Qwen2MoeRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: Optional[bool] = False,\n            output_hidden_states: Optional[bool] = False,\n            output_router_logits: Optional[bool] = None,\n            return_dict: bool = True,\n            segment_ids: Optional[chex.Array] = None,\n            fcm_mask: Optional[jnp.ndarray] = None,\n\n    ):\n        \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        Args:\n            self: Refer to the class instance itself\n            hidden_states: chex.Array: Pass in the hidden state of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency information\n            attention_mask: chex.Array: Mask out the attention weights\n                for padding tokens\n            position_ids: chex.Array: Determine the position of each\n                token in the sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Control whether the dropout is applied\n                or not\n            init_cache: bool: Initialize the cache in the attention\n                layer\n            output_attentions: bool: Return the attention weights\n            fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n\n        Returns:\n            A tuple of two items\n        \"\"\"\n        attn_outputs = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            segment_ids,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n        mlp_out = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n        if self.config.num_experts &gt; 0:\n            feed_forward_hidden_states, router_logits = mlp_out\n        else:\n            feed_forward_hidden_states = mlp_out\n            router_logits = None\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:] + (router_logits,)\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeBlock.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, output_router_logits=None, return_dict=True, segment_ids=None, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <p>:param : Control the dropout in the self attention layer</p> <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        output_router_logits: Optional[bool] = None,\n        return_dict: bool = True,\n        segment_ids: Optional[chex.Array] = None,\n        fcm_mask: Optional[jnp.ndarray] = None,\n\n):\n    \"\"\"The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    Args:\n        self: Refer to the class instance itself\n        hidden_states: chex.Array: Pass in the hidden state of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency information\n        attention_mask: chex.Array: Mask out the attention weights\n            for padding tokens\n        position_ids: chex.Array: Determine the position of each\n            token in the sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Control whether the dropout is applied\n            or not\n        init_cache: bool: Initialize the cache in the attention\n            layer\n        output_attentions: bool: Return the attention weights\n        fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n\n    Returns:\n        A tuple of two items\n    \"\"\"\n    attn_outputs = self.self_attn(\n        self.input_layernorm(hidden_states),\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        segment_ids,\n        deterministic,\n        init_cache,\n        output_attentions,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n    hidden_states = hidden_states + attn_output\n\n    feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n    mlp_out = self.mlp(\n        feed_forward_input,\n        deterministic,\n    )\n\n    if self.config.num_experts &gt; 0:\n        feed_forward_hidden_states, router_logits = mlp_out\n    else:\n        feed_forward_hidden_states = mlp_out\n        router_logits = None\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n\n    return (hidden_states,) + attn_outputs[1:] + (router_logits,)\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeBlockCollection","title":"<code>FlaxQwen2MoeBlockCollection</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeBlockCollection(nn.Module):\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxQwen2MoeBlock(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: Optional[bool] = False,\n            output_hidden_states: Optional[bool] = False,\n            output_router_logits: Optional[bool] = None,\n            return_dict: bool = True,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model\n         in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module,\n        and return all outputs that are computed by this module.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the input tensor to the\n                encoder\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency of each token\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Specify the position of each token\n                in a sequence\n            causal_mask: chex.Array: Mask the attention weights\n            deterministic: bool: Determine whether the model is in\n                training or evaluation mode\n            init_cache: bool: Initialize the cache for each layer\n            output_attentions: bool: Determine whether to output the\n                attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states of each layer\n            return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n\n        Returns:\n            A tuple of 3 values\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        all_router_logits = () if output_router_logits else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += layer_outputs[1],\n            if output_router_logits:\n                all_router_logits += layer_outputs[-1],\n\n        outputs = (hidden_states, all_hidden_states, all_attentions, all_router_logits)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeBlockCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, output_router_logits=None, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model  in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <p>:param : Determine whether to use the forgetful causal mask</p> <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        output_router_logits: Optional[bool] = None,\n        return_dict: bool = True,\n):\n    \"\"\"The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model\n     in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module,\n    and return all outputs that are computed by this module.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the input tensor to the\n            encoder\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency of each token\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Specify the position of each token\n            in a sequence\n        causal_mask: chex.Array: Mask the attention weights\n        deterministic: bool: Determine whether the model is in\n            training or evaluation mode\n        init_cache: bool: Initialize the cache for each layer\n        output_attentions: bool: Determine whether to output the\n            attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states of each layer\n        return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n\n    Returns:\n        A tuple of 3 values\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n    all_router_logits = () if output_router_logits else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += layer_outputs[1],\n        if output_router_logits:\n            all_router_logits += layer_outputs[-1],\n\n    outputs = (hidden_states, all_hidden_states, all_attentions, all_router_logits)\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForCausalLM","title":"<code>FlaxQwen2MoeForCausalLM</code>","text":"<p>               Bases: <code>FlaxQwen2MoePreTrainedModel</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeForCausalLM(FlaxQwen2MoePreTrainedModel):\n    module_class = FlaxQwen2MoeForCausalLMModule\n\n    def set_input_embeddings(self, value):\n        self.module.model.embed_tokens = value\n\n    def get_input_embeddings(self):\n        return self.module.model.embed_tokens\n\n    def set_decoder(self, decoder):\n        self.module.model = decoder\n\n    def get_decoder(self):\n        return self.module.model\n\n    def get_output_embeddings(self):\n        return self.module.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.module.lm_head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        Args:\n            self: Access variables that belong to the class\n            input_ids: Pass in the input tokens\n            max_length: Set the length of the sequence to be generated\n            attention_mask: Optional[chex.Array]: Mask the attention\n                weights\n\n        Returns:\n            A dictionary of the past_key_values, attention_mask and\n            position ids\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and</p> <p>position ids</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    Args:\n        self: Access variables that belong to the class\n        input_ids: Pass in the input tokens\n        max_length: Set the length of the sequence to be generated\n        attention_mask: Optional[chex.Array]: Mask the attention\n            weights\n\n    Returns:\n        A dictionary of the past_key_values, attention_mask and\n        position ids\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones(\n        (batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(\n            extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                        None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForCausalLMModule","title":"<code>FlaxQwen2MoeForCausalLMModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeForCausalLMModule(nn.Module):\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxQwen2MoeModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = Linear(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        Args:\n            self: Refer to the object itself\n            input_ids: chex.Array: Pass the input token ids to the model\n            attention_mask: chex.Array: Mask out the padding tokens\n            position_ids: chex.Array: Specify the position of each token\n                in the input sequence\n            deterministic: bool: Control whether the model is trained or\n                not\n            init_cache: bool: Initialize the cache for the decoder\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Determine whether to return the\n                hidden states\n            return_dict: bool: Return a dictionary of the outputs or not\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of the word that we want to predict\n            None]]: Pass in the extra embedding\n\n        Returns:\n            The logits and the hidden states\n        \"\"\"\n        if output_router_logits is None:\n            output_router_logits = self.config.output_router_logits\n        if output_hidden_states is None:\n            output_hidden_states = self.config.output_hidden_states\n        if output_attentions is None:\n            output_attentions = self.config.output_attentions\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_router_logits=output_router_logits,\n            init_cache=init_cache,\n            deterministic=deterministic,\n            return_dict=True,\n        )\n        hidden_states = outputs.last_hidden_state\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n            shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n            logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            logits = self.lm_head(hidden_states)\n\n        logits = logits.astype(jnp.float32)\n        batch_size, seq_length, hd = logits.shape\n        aux_loss = None\n        if output_router_logits and outputs.router_logits is not None:\n            aux_loss = auxiliary_load_balancing_loss_func(\n                gate_logits=tuple([logit.reshape(batch_size * seq_length, -1) for logit in outputs.router_logits]),\n                num_experts=self.config.num_experts,\n                top_k=self.config.num_experts_per_tok,\n                attention_mask=attention_mask\n            )\n            aux_loss = aux_loss * self.config.router_aux_loss_coef\n        if not return_dict:\n            outputs = (logits,) + tuple(\n                v\n                for v in [\n                    aux_loss,\n                    outputs.hidden_states,\n                    outputs.attentions,\n                    outputs.router_logits\n                ]\n                if v is not None\n            )\n            return outputs\n\n        return MoeCausalLMOutput(\n            aux_loss=aux_loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            router_logits=outputs.router_logits,\n        )\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>bool: Return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>bool: Determine whether to return the hidden states</p> <code>None</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    Args:\n        self: Refer to the object itself\n        input_ids: chex.Array: Pass the input token ids to the model\n        attention_mask: chex.Array: Mask out the padding tokens\n        position_ids: chex.Array: Specify the position of each token\n            in the input sequence\n        deterministic: bool: Control whether the model is trained or\n            not\n        init_cache: bool: Initialize the cache for the decoder\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Determine whether to return the\n            hidden states\n        return_dict: bool: Return a dictionary of the outputs or not\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of the word that we want to predict\n        None]]: Pass in the extra embedding\n\n    Returns:\n        The logits and the hidden states\n    \"\"\"\n    if output_router_logits is None:\n        output_router_logits = self.config.output_router_logits\n    if output_hidden_states is None:\n        output_hidden_states = self.config.output_hidden_states\n    if output_attentions is None:\n        output_attentions = self.config.output_attentions\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        output_router_logits=output_router_logits,\n        init_cache=init_cache,\n        deterministic=deterministic,\n        return_dict=True,\n    )\n    hidden_states = outputs.last_hidden_state\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"]\n        shared_kernel = fjformer.linen.linen.control_quantization(shared_kernel, self.param_dtype).T\n        logits = self.lm_head.apply(\n            {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        logits = self.lm_head(hidden_states)\n\n    logits = logits.astype(jnp.float32)\n    batch_size, seq_length, hd = logits.shape\n    aux_loss = None\n    if output_router_logits and outputs.router_logits is not None:\n        aux_loss = auxiliary_load_balancing_loss_func(\n            gate_logits=tuple([logit.reshape(batch_size * seq_length, -1) for logit in outputs.router_logits]),\n            num_experts=self.config.num_experts,\n            top_k=self.config.num_experts_per_tok,\n            attention_mask=attention_mask\n        )\n        aux_loss = aux_loss * self.config.router_aux_loss_coef\n    if not return_dict:\n        outputs = (logits,) + tuple(\n            v\n            for v in [\n                aux_loss,\n                outputs.hidden_states,\n                outputs.attentions,\n                outputs.router_logits\n            ]\n            if v is not None\n        )\n        return outputs\n\n    return MoeCausalLMOutput(\n        aux_loss=aux_loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n        router_logits=outputs.router_logits,\n    )\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForSequenceClassificationModule","title":"<code>FlaxQwen2MoeForSequenceClassificationModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        Args:\n            self: Access variables that belong to the class\n\n        Returns:\n            A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxQwen2MoeModule(self.config, dtype=self.dtype)\n        self.classifier = Linear(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n            &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        Args:\n            self: Refer to the class instance\n            input_ids: chex.Array: Pass the input to the model\n            attention_mask: chex.Array: Specify which tokens are masked\n            position_ids: chex.Array: Specify the position of each token\n                in the sequence\n            deterministic: bool: Control whether the model is run in\n                deterministic or stochastic mode\n            init_cache: bool: Initialize the cache for the transformer\n            output_attentions: bool: Return the attention weights\n            output_hidden_states: bool: Return the hidden states of all\n                layers\n            return_dict: bool: Return a dictionary of outputs\n            extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n                embedding of a new word\n            None]]: Pass the extra embedding to the model\n\n        Returns:\n            A tuple of logits and hidden_states\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForSequenceClassificationModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in all the inputs to the model and returns all outputs from it. The call function can be called directly on an instance of a class, or by using parentheses after an instance:     &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class     &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to call</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Specify which tokens are masked</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is run in deterministic or stochastic mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the transformer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of outputs</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of a new word</p> <code>None</code> <code>None]]</code> <p>Pass the extra embedding to the model</p> required <p>Returns:</p> Type Description <p>A tuple of logits and hidden_states</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"The __call__ function is the main function of a Flax module.\n    It takes in all the inputs to the model and returns all outputs from it.\n    The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n        &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n        &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n    Args:\n        self: Refer to the class instance\n        input_ids: chex.Array: Pass the input to the model\n        attention_mask: chex.Array: Specify which tokens are masked\n        position_ids: chex.Array: Specify the position of each token\n            in the sequence\n        deterministic: bool: Control whether the model is run in\n            deterministic or stochastic mode\n        init_cache: bool: Initialize the cache for the transformer\n        output_attentions: bool: Return the attention weights\n        output_hidden_states: bool: Return the hidden states of all\n            layers\n        return_dict: bool: Return a dictionary of outputs\n        extra_embedding: Optional[Union[jnp.ndarray: Pass in the\n            embedding of a new word\n        None]]: Pass the extra embedding to the model\n\n    Returns:\n        A tuple of logits and hidden_states\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n    prediction = self.classifier(hidden_states)\n    if return_dict:\n        return FlaxSequenceClassifierOutput(\n            logits=prediction,\n            hidden_states=hidden_states\n        )\n    else:\n        return prediction,\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeForSequenceClassificationModule.setup","title":"<code>setup()</code>","text":"<p>The setup function is called once at the beginning of training. It initializes the model and optimizer, and sets up any other state that needs to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <p>Returns:</p> Type Description <p>A tuple of the model and the classifier</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def setup(self):\n    \"\"\"The setup function is called once at the beginning of training.\n    It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n    Args:\n        self: Access variables that belong to the class\n\n    Returns:\n        A tuple of the model and the classifier\n    \"\"\"\n    self.model = FlaxQwen2MoeModule(self.config, dtype=self.dtype)\n    self.classifier = Linear(\n        self.num_classes,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        use_bias=False,\n        kernel_init=jax.nn.initializers.normal(\n            stddev=self.config.initializer_range),\n        precision=self.precision,\n    )\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeMLP","title":"<code>FlaxQwen2MoeMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeMLP(nn.Module):\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n    intermediate_size: Optional[int] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n        intermediate_size = self.intermediate_size if self.intermediate_size is not None else config.moe_intermediate_size\n        self.gate_proj = Linear(\n            intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range\n            ),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = Linear(\n            intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor that is the result of applying a dropout function\n            to x\n        \"\"\"\n        x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        return x\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function</p> <code>ndarray</code> <p>to x</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor that is the result of applying a dropout function\n        to x\n    \"\"\"\n    x = self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n    return x\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoePreTrainedModel","title":"<code>FlaxQwen2MoePreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoePreTrainedModel(EasyDeLFlaxPretrainedModel):\n    config_class = Qwen2MoeConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: Qwen2MoeConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n        Args:\n            self: Refer to the object itself\n            config: Qwen2MoeConfig: Pass the configuration to the module\n            input_shape: Tuple: Specify the shape of the input to the\n                model\n            seed: int: Set the seed for random number generation\n            dtype: jnp.dtype: Specify the data type of the input\n            _do_init: bool: Control whether the module is initialized or\n                not\n            **kwargs: Pass in any additional parameters that the\n                module_class might need\n        :param : Specify the number of layers in the network\n\n        Returns:\n            The super() of the class\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        \"\"\"The init_weights function is used to initialize the weights of a model.\n\n        Args:\n            self: Access variables that belong to the class\n            rng: jax.random.PRNGKey: Initialize the weights of the model\n            input_shape: Tuple: Specify the shape of the input tensor\n            params: FrozenDict: Pass in the parameters of a pre-trained\n                model\n\n        Returns:\n            A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        Args:\n            self: Access the module\n            batch_size: Define the batch size of the input tensors\n            max_length: Set the length of the input sequence\n\n        Returns:\n            A dictionary with the following keys:\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        Args:\n            self: Represent the instance of the class\n            input_ids: chex.Array: Pass in the input tokens\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input\n            position_ids: chex.Array: Create the positional embeddings\n            params: dict: Pass in the parameters of the model\n            past_key_values: dict: Pass in the past key values from a\n                previous call to __call__\n            dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n                is applied in a random way\n            train: bool: Determine whether to use dropout or not\n            output_attentions: Optional[bool]: Determine whether to\n                return the attention weights\n            output_hidden_states: Optional[bool]: Return the hidden\n                states of all layers\n            return_dict: Optional[bool]: Determine whether to return a\n                dictionary or not\n            extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n                the embedding for the input_ids\n            add_params_field: bool: Add the params field to the inputs\n                dictionary\n\n        Returns:\n            A tuple of the following:\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        output_router_logits = output_router_logits if output_router_logits is not None else self.config.output_router_logits\n\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params\n        } if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            output_router_logits,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoePreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, output_router_logits=None, return_dict=None, extra_embedding=None, add_params_field=False, **kwargs)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False,\n        **kwargs\n):\n    \"\"\"The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    Args:\n        self: Represent the instance of the class\n        input_ids: chex.Array: Pass in the input tokens\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input\n        position_ids: chex.Array: Create the positional embeddings\n        params: dict: Pass in the parameters of the model\n        past_key_values: dict: Pass in the past key values from a\n            previous call to __call__\n        dropout_rng: jax.random.PRNGKey: Make sure that the dropout\n            is applied in a random way\n        train: bool: Determine whether to use dropout or not\n        output_attentions: Optional[bool]: Determine whether to\n            return the attention weights\n        output_hidden_states: Optional[bool]: Return the hidden\n            states of all layers\n        return_dict: Optional[bool]: Determine whether to return a\n            dictionary or not\n        extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in\n            the embedding for the input_ids\n        add_params_field: bool: Add the params field to the inputs\n            dictionary\n\n    Returns:\n        A tuple of the following:\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    output_router_logits = output_router_logits if output_router_logits is not None else self.config.output_router_logits\n\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\n                \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                        None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    if self.config.bits is not None:\n        rngs['params'] = jax.random.key(0)\n\n    inputs = {\n        \"params\": params or self.params\n    } if add_params_field else params or self.params\n\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        output_router_logits,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + \\\n                  (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoePreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>Qwen2MoeConfig</code> <p>Qwen2MoeConfig: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <p>:param : Specify the number of layers in the network</p> <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def __init__(\n        self,\n        config: Qwen2MoeConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n    Args:\n        self: Refer to the object itself\n        config: Qwen2MoeConfig: Pass the configuration to the module\n        input_shape: Tuple: Specify the shape of the input to the\n            model\n        seed: int: Set the seed for random number generation\n        dtype: jnp.dtype: Specify the data type of the input\n        _do_init: bool: Control whether the module is initialized or\n            not\n        **kwargs: Pass in any additional parameters that the\n            module_class might need\n    :param : Specify the number of layers in the network\n\n    Returns:\n        The super() of the class\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoePreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n    \"\"\"The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n    the model, which would be very slow.\n\n    Args:\n        self: Access the module\n        batch_size: Define the batch size of the input tensors\n        max_length: Set the length of the input sequence\n\n    Returns:\n        A dictionary with the following keys:\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(\n        jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoePreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n    \"\"\"The init_weights function is used to initialize the weights of a model.\n\n    Args:\n        self: Access variables that belong to the class\n        rng: jax.random.PRNGKey: Initialize the weights of the model\n        input_shape: Tuple: Specify the shape of the input tensor\n        params: FrozenDict: Pass in the parameters of a pre-trained\n            model\n\n    Returns:\n        A frozendict of parameters\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(\n            input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(\n            rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"generated-modules-qwen2_moe-modeling_qwen2_moe_flax/#src.python.easydel.modules.qwen2_moe.modeling_qwen2_moe_flax.FlaxQwen2MoeSparseMoeBlock","title":"<code>FlaxQwen2MoeSparseMoeBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>This implementation is strictly equivalent to standard MoE with full capacity (no dropped tokens). It's faster since it formulates MoE operations in terms of block-sparse operations to accomodate imbalanced assignments of tokens to experts, whereas standard MoE either (1) drop tokens at the cost of reduced performance or (2) set capacity factor to number of experts and thus waste computation and memory on padding.</p> Source code in <code>src/python/easydel/modules/qwen2_moe/modeling_qwen2_moe_flax.py</code> <pre><code>class FlaxQwen2MoeSparseMoeBlock(nn.Module):\n    \"\"\"This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n    config: Qwen2MoeConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[\n        Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -&gt; None:\n        self.gate = Linear(\n            self.config.num_experts,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n        )\n\n        self.experts = FlaxQwen2MoeBlocKSparesTop2MLPCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n        self.shared_expert = FlaxQwen2MoeMLP(\n            config=self.config,\n            intermediate_size=self.config.shared_expert_intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.shared_expert_gate = Linear(\n            1,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            e: bool = False  # Ignored\n    ) -&gt; Tuple[chex.Array, chex.Array]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n\n        router_logits = self.gate(hidden_states).astype(\n            jnp.promote_types(self.dtype, jnp.float32)\n        )\n\n        routing_weights = jax.nn.softmax(\n            router_logits.astype(\n                jnp.promote_types(self.dtype, jnp.float32)\n            ), axis=-1\n        )\n\n        routing_weights, selected_experts = jax.lax.top_k(\n            routing_weights,\n            k=self.config.num_experts_per_tok\n        )\n\n        if self.config.norm_topk_prob:\n            routing_weights /= routing_weights.sum(axis=-1, keepdims=True)\n        final_hidden_state = self.experts(\n            selected_experts=selected_experts,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            hidden_dim=hidden_dim,\n            hidden_states=hidden_states,\n            routing_weights=routing_weights\n        )\n        shared_expert_output = self.shared_expert(hidden_states)\n        shared_expert_output = jax.nn.sigmoid(\n            self.shared_expert_gate(hidden_states)\n        ) * shared_expert_output\n        final_hidden_state = final_hidden_state + shared_expert_output\n\n        return (\n            final_hidden_state,\n            router_logits\n        )\n</code></pre>"},{"location":"generated-modules-roberta-modelling_roberta_flax/","title":"modules.roberta.modelling_roberta_flax","text":""},{"location":"generated-modules-roberta-roberta_configuration/","title":"modules.roberta.roberta_configuration","text":""},{"location":"generated-modules-rwkv-modelling_rwkv_flax/","title":"modules.rwkv.modelling_rwkv_flax","text":""},{"location":"generated-modules-rwkv-rwkv_configuration/","title":"modules.rwkv.rwkv_configuration","text":""},{"location":"generated-modules-rwkv-rwkv_configuration/#src.python.easydel.modules.rwkv.rwkv_configuration.RwkvConfig","title":"<code>RwkvConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> <p>RWKV configuration.</p> Source code in <code>src/python/easydel/modules/rwkv/rwkv_configuration.py</code> <pre><code>class RwkvConfig(EasyDeLPretrainedConfig):\n    \"\"\"RWKV configuration.\"\"\"\n\n    model_type: str = \"rwkv\"\n    attribute_map = {\"max_position_embeddings\": \"context_length\"}\n\n    def __init__(\n            self,\n            vocab_size=50277,\n            context_length=1024,\n            hidden_size=4096,\n            num_hidden_layers=32,\n            attention_hidden_size=None,\n            intermediate_size=None,\n            layer_norm_epsilon=1e-5,\n            bos_token_id=0,\n            eos_token_id=0,\n            rescale_every=6,\n            tie_word_embeddings=False,\n            use_cache=True,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ) -&gt; None:\n\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        self.vocab_size = vocab_size\n        self.context_length = context_length\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.attention_hidden_size = attention_hidden_size if attention_hidden_size is not None else hidden_size\n        self.intermediate_size = intermediate_size if intermediate_size is not None else 4 * hidden_size\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.rescale_every = rescale_every\n        self.use_cache = use_cache\n\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            bits=bits,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        return (\n            (\".*\", PartitionSpec((\"sp\", \"fsdp\"))),\n        ) if fully_sharded_data_parallel else (\n            (\".*\", PartitionSpec((\"sp\", \"fsdp\"))),\n        )\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/","title":"modules.stablelm.modelling_stablelm_flax","text":""},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmAttention","title":"<code>FlaxStableLmAttention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>class FlaxStableLmAttention(BaseJAXAttentionModule):\n    config: StableLmConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config: StableLmConfig = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.partial_rotary_factor = config.partial_rotary_factor\n\n        if self.num_key_value_groups == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = Linear(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.use_qkv_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.use_qkv_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = Linear(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.use_qkv_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary_emb_dim = int(self.config.partial_rotary_factor * self.head_dim)\n        self.attention_performer = AttentionModule(\n            use_sharding_constraint=self.config.use_sharding_constraint,\n            block_k_major=self.config.block_k_major,\n            block_b=self.config.block_b,\n            block_q=self.config.block_q,\n            block_k=self.config.block_k,\n            block_q_major_dkv=self.config.block_q_major_dkv,\n            block_k_major_dkv=self.config.block_k_major_dkv,\n            block_k_major_dq=self.config.block_k_major_dq,\n            block_k_dkv=self.config.block_k_dkv,\n            block_q_dkv=self.config.block_q_dkv,\n            block_q_dq=self.config.block_q_dq,\n            block_k_dq=self.config.block_k_dq,\n            num_attention_heads=self.config.num_attention_heads,\n            attention_dropout=self.config.attention_dropout,\n            head_dims=self.head_dim,\n            attention_partition_spec=self.config.attention_partition_spec,\n            shard_attention_computation=self.config.shard_attention_computation,\n            precision=self.precision,\n            force_float32_tpu=True,\n            attn_mechanism=self.config.attn_mechanism,\n            dtype=self.dtype,\n            bias_partition_spec=self.config.bias_partition_spec,\n            key_partition_spec=self.config.key_partition_spec,\n            query_partition_spec=self.config.query_partition_spec,\n            generation_query_partition_spec=self.config.generation_query_partition_spec,\n            generation_bias_partition_spec=self.config.generation_bias_partition_spec,\n            generation_attention_partition_spec=self.config.generation_attention_partition_spec,\n            value_partition_spec=self.config.value_partition_spec,\n            scan_ring_attention=self.config.scan_ring_attention,\n            mesh=self.config.jax_mesh(),\n            sm_scale=1 / math.sqrt(self.head_dim),\n            axis_name=self.config.attention_axis_name,\n            backward_pass_impl=self.config.flash_attention_backward_pass_impl\n        )\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @staticmethod\n    def _transpose_sequence_head(query, key, value):\n        \"\"\"The _transpose_sequence_head function transposes the query, key and value matrices.\n\n        Args:\n            query: Get the attention weights for each of the heads\n            key: Determine the number of heads\n            value: Store the values of the input\n\n        Returns:\n            The transpose of the query, key and value matrices\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        Args:\n            self: Access variables that belong to the class\n            batch_size: Reshape the query_states, key and value tensors\n            sequence_length: Reshape the query_states, key and value\n                tensors\n            query: Calculate the attention weights\n            key: Calculate the attention\n            value: Compute the attention weights\n            freq_cis: Calculate the frequency of each word in the\n                vocabulary\n            position_ids: Identify the position of each token in the\n                sequence\n\n        Returns:\n            A tuple of 3 tensors: query_states, key and value\n        \"\"\"\n        query = query.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_attention_heads,\n            self.head_dim\n        )\n        key = key.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n        value = value.reshape(\n            batch_size,\n            sequence_length,\n            self.config.num_key_value_heads,\n            self.head_dim\n        )\n\n        query, key, value = self._transpose_sequence_head(query, key, value)\n\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        query_rot, query_pass = (\n            query[..., : self.rotary_emb_dim],\n            query[..., self.rotary_emb_dim:],\n        )\n        key_rot, key_pass = (\n            key[..., : self.rotary_emb_dim],\n            key[..., self.rotary_emb_dim:],\n        )\n\n        key_rot = apply_rotary_pos_emb(key_rot, sin, cos)\n        query_rot = apply_rotary_pos_emb(query_rot, sin, cos)\n\n        query = jnp.concatenate((query_rot, query_pass), axis=-1)\n        key = jnp.concatenate((key_rot, key_pass), axis=-1)\n\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._transpose_sequence_head(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            segment_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        Args:\n            self: Access variables that belong to the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n                frequency coefficients for each position\n            attention_mask: chex.Array: Mask out certain tokens in the\n                input sequence\n            position_ids: chex.Array: Determine the position of each\n                token in a sequence\n            causal_mask: chex.Array: Mask out the future tokens in the\n                decoder\n            deterministic: bool: Determine whether to use dropout or not\n            init_cache: bool: Initialize the cache\n            output_attentions: bool: Determine whether to return the\n                attention weights or not\n            fcm_mask: Mask out the attention weights between the input\n                and output tokens\n        :param : Determine if the attention is causal or not\n\n        Returns:\n            A tuple of two arrays\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        query_states = query_states.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_states = key_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_states = value_states.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_states, key_states, value_states = self.apply_rotary(\n            query=query_states,\n            key=key_states,\n            value=value_states,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask,\n                (0, 0, mask_shift, 0),\n                (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states,\n                value_states,\n                query_states,\n                attention_mask\n            )\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        use_qkv_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(\n                self.dtype).min).astype(self.dtype),\n        )\n\n        query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n        attentions = self.attention_performer.__call__(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            bias=use_qkv_bias,\n            attention_mask=attention_mask,\n            causal=True,\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            query_sequence_length=query_length,\n            key_value_sequence_length=key_length,\n            uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n            segment_ids=segment_ids,\n            causal_mask=causal_mask\n        )\n\n\n        attn_output = self._merge_heads(attentions.attention_outputs)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        attn_output = self.o_proj(attn_output)\n        outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n        return outputs\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, segment_ids=None, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Tuple[Array, Array]</code> <p>Tuple[chex.Array, chex.Array],: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <p>:param : Determine if the attention is causal or not</p> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: Tuple[chex.Array, chex.Array],\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        segment_ids: Optional[chex.Array] = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n    \"\"\"The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    Args:\n        self: Access variables that belong to the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        freq_cis: Tuple[chex.Array, chex.Array],: Pass in the\n            frequency coefficients for each position\n        attention_mask: chex.Array: Mask out certain tokens in the\n            input sequence\n        position_ids: chex.Array: Determine the position of each\n            token in a sequence\n        causal_mask: chex.Array: Mask out the future tokens in the\n            decoder\n        deterministic: bool: Determine whether to use dropout or not\n        init_cache: bool: Initialize the cache\n        output_attentions: bool: Determine whether to return the\n            attention weights or not\n        fcm_mask: Mask out the attention weights between the input\n            and output tokens\n    :param : Determine if the attention is causal or not\n\n    Returns:\n        A tuple of two arrays\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_states, key_states, value_states = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    query_states = query_states.reshape(\n        batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_states = key_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_states = value_states.reshape(\n        batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_states, key_states, value_states = self.apply_rotary(\n        query=query_states,\n        key=key_states,\n        value=value_states,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with num_key_value_groups = {self.num_key_value_groups}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_states.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_states.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask,\n            (0, 0, mask_shift, 0),\n            (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(\n        causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(\n        attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n    dropout_rng = None\n\n    if not deterministic and self.config.attention_dropout &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_states, value_states, attention_mask = self._concatenate_to_cache(\n            key_states,\n            value_states,\n            query_states,\n            attention_mask\n        )\n    # if self.config.use_sharding_constraint:\n    #     query_states = with_sharding_constraint(\n    #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n    #     )\n    #     key_states = with_sharding_constraint(\n    #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    #     value_states = with_sharding_constraint(\n    #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n    #     )\n    use_qkv_bias = lax.select(\n        attention_mask &gt; 0,\n        jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n        jnp.full(attention_mask.shape, jnp.finfo(\n            self.dtype).min).astype(self.dtype),\n    )\n\n    query_length, key_length = query_states.shape[1], key_states.shape[1]\n\n    attentions = self.attention_performer.__call__(\n        query_states=query_states,\n        key_states=key_states,\n        value_states=value_states,\n        bias=use_qkv_bias,\n        attention_mask=attention_mask,\n        causal=True,\n        dropout_rng=dropout_rng,\n        deterministic=deterministic,\n        query_sequence_length=query_length,\n        key_value_sequence_length=key_length,\n        uses_cache=self.has_variable(\"cache\", \"cached_key\") or init_cache,\n        segment_ids=segment_ids,\n        causal_mask=causal_mask\n    )\n\n\n    attn_output = self._merge_heads(attentions.attention_outputs)\n    if self.config.shard_attention_computation:\n        attn_output = with_sharding_constraint(\n            attn_output, PartitionSpec(\n                (\"dp\", \"fsdp\"),\n                \"sp\" if attn_output.shape[1] != 1 else None,\n                \"tp\"\n            )\n        )\n    attn_output = self.o_proj(attn_output)\n    outputs = (attn_output, attentions.attention_weights) if output_attentions else (attn_output,)\n    return outputs\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query_states, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query_states, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query_states, key and value</p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    Args:\n        self: Access variables that belong to the class\n        batch_size: Reshape the query_states, key and value tensors\n        sequence_length: Reshape the query_states, key and value\n            tensors\n        query: Calculate the attention weights\n        key: Calculate the attention\n        value: Compute the attention weights\n        freq_cis: Calculate the frequency of each word in the\n            vocabulary\n        position_ids: Identify the position of each token in the\n            sequence\n\n    Returns:\n        A tuple of 3 tensors: query_states, key and value\n    \"\"\"\n    query = query.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_attention_heads,\n        self.head_dim\n    )\n    key = key.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n    value = value.reshape(\n        batch_size,\n        sequence_length,\n        self.config.num_key_value_heads,\n        self.head_dim\n    )\n\n    query, key, value = self._transpose_sequence_head(query, key, value)\n\n    sin, cos = freq_cis\n\n    sin = sin[position_ids][:, None, :, :]\n    cos = cos[position_ids][:, None, :, :]\n\n    query_rot, query_pass = (\n        query[..., : self.rotary_emb_dim],\n        query[..., self.rotary_emb_dim:],\n    )\n    key_rot, key_pass = (\n        key[..., : self.rotary_emb_dim],\n        key[..., self.rotary_emb_dim:],\n    )\n\n    key_rot = apply_rotary_pos_emb(key_rot, sin, cos)\n    query_rot = apply_rotary_pos_emb(query_rot, sin, cos)\n\n    query = jnp.concatenate((query_rot, query_pass), axis=-1)\n    key = jnp.concatenate((key_rot, key_pass), axis=-1)\n\n    key = repeat_kv_bnsh(key, self.num_key_value_groups)\n    value = repeat_kv_bnsh(value, self.num_key_value_groups)\n    return self._transpose_sequence_head(query, key, value)\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmMLP","title":"<code>FlaxStableLmMLP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>class FlaxStableLmMLP(nn.Module):\n    config: StableLmConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        self.gate_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = Linear(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = Linear(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            x: jnp.ndarray: Pass in the input to the layer\n            deterministic: bool: Determine whether to use dropout #\n                Ignored\n\n        Returns:\n            A tensor that is the result of function to x\n        \"\"\"\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout # Ignored</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of function to x</p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        x: jnp.ndarray: Pass in the input to the layer\n        deterministic: bool: Determine whether to use dropout #\n            Ignored\n\n    Returns:\n        A tensor that is the result of function to x\n    \"\"\"\n    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n</code></pre>"},{"location":"generated-modules-stablelm-modelling_stablelm_flax/#src.python.easydel.modules.stablelm.modelling_stablelm_flax.FlaxStableLmPreTrainedModel","title":"<code>FlaxStableLmPreTrainedModel</code>","text":"<p>               Bases: <code>EasyDeLFlaxPretrainedModel</code></p> <p>StableLm pre-trained model.</p> Source code in <code>src/python/easydel/modules/stablelm/modelling_stablelm_flax.py</code> <pre><code>class FlaxStableLmPreTrainedModel(EasyDeLFlaxPretrainedModel):\n    \"\"\"StableLm pre-trained model.\"\"\"\n    module_class = None\n    config_class = StableLmConfig\n    base_model_prefix = \"model\"\n\n    def __init__(\n            self,\n            config: StableLmConfig,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            input_shape=(1, 1),\n            seed: int = 42,\n            _do_init: bool = False\n    ) -&gt; None:\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision\n        )\n        super().__init__(\n            config=config,\n            module=module,\n            input_shape=input_shape,\n            _do_init=_do_init,\n            seed=seed\n        )\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(rngs, input_ids, attention_mask)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False,\n            **kwargs\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, \"Maximum Position Embedding Reached !\"\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            input_ids=input_ids,\n            inputs_embeds=None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            extra_embedding=extra_embedding,\n            deterministic=not train,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            init_cache=False,\n            return_dict=return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-stablelm-stablelm_configuration/","title":"modules.stablelm.stablelm_configuration","text":""},{"location":"generated-modules-stablelm-stablelm_configuration/#src.python.easydel.modules.stablelm.stablelm_configuration.StableLmConfig","title":"<code>StableLmConfig</code>","text":"<p>               Bases: <code>EasyDeLPretrainedConfig</code></p> <p>Phi configuration.</p> Source code in <code>src/python/easydel/modules/stablelm/stablelm_configuration.py</code> <pre><code>class StableLmConfig(EasyDeLPretrainedConfig):\n    \"\"\"Phi configuration.\"\"\"\n\n    model_type: str = \"stablelm\"\n\n    def __init__(\n            self,\n            vocab_size=50304,\n            intermediate_size=6912,\n            hidden_size=2560,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=32,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096,\n            initializer_range=0.02,\n            layer_norm_eps=1.0e-5,\n            use_cache=True,\n            tie_word_embeddings=False,\n            rope_theta=10_000,\n            rope_scaling=None,\n            use_qkv_bias=False,\n            hidden_dropout=0.0,\n            attention_dropout=0.0,\n            partial_rotary_factor=0.25,\n            bos_token_id=0,\n            eos_token_id=0,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ) -&gt; None:\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.use_qkv_bias = use_qkv_bias\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.partial_rotary_factor = partial_rotary_factor\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            bits=bits,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n    def get_partition_rules(self, fully_sharded_data_parallel: bool = True):\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\".*\", PartitionSpec(None)),\n        ) if not fully_sharded_data_parallel else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"sp\", \"fsdp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\".*\", PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n</code></pre>"},{"location":"generated-modules-t5-modelling_t5_flax/","title":"modules.t5.modelling_t5_flax","text":"<p>Flax T5 model.</p>"},{"location":"generated-modules-t5-modelling_t5_flax/#src.python.easydel.modules.t5.modelling_t5_flax.FlaxT5Attention","title":"<code>FlaxT5Attention</code>","text":"<p>               Bases: <code>BaseJAXAttentionModule</code></p> Source code in <code>src/python/easydel/modules/t5/modelling_t5_flax.py</code> <pre><code>class FlaxT5Attention(BaseJAXAttentionModule):\n    config: T5Config\n    has_relative_attention_bias: bool = False\n    causal: bool = False\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.relative_attention_num_buckets = self.config.relative_attention_num_buckets\n        self.relative_attention_max_distance = self.config.relative_attention_max_distance\n        self.d_model = self.config.d_model\n        self.key_value_proj_dim = self.config.d_kv\n        self.n_heads = self.config.num_heads\n        self.dropout = self.config.dropout_rate\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        q_init_std = self.config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        kv_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n        o_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n\n        self.q = Linear(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(q_init_std),\n            dtype=self.dtype,\n        )\n        self.k = Linear(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.v = Linear(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.o = Linear(\n            self.d_model,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(o_init_std),\n            dtype=self.dtype,\n        )\n\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embed(\n                self.relative_attention_num_buckets,\n                self.n_heads,\n                embedding_init=jax.nn.initializers.normal(kv_init_std),\n                dtype=self.dtype,\n            )\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position &gt; 0) * num_buckets\n            relative_position = jnp.abs(relative_position)\n        else:\n            relative_position = -jnp.clip(relative_position, a_max=0)\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position &lt; max_exact\n\n        relative_position_if_large = max_exact + (\n                jnp.log(relative_position / max_exact) / jnp.log(max_distance / max_exact) * (num_buckets - max_exact)\n        )\n        relative_position_if_large = jnp.clip(relative_position_if_large, a_max=num_buckets - 1)\n\n        relative_buckets += jnp.where(is_small, relative_position, relative_position_if_large)\n\n        return relative_buckets.astype(\"i4\")\n\n    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = jnp.arange(query_length, dtype=\"i4\")[:, None]\n        memory_position = jnp.arange(key_length, dtype=\"i4\")[None, :]\n\n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,\n            bidirectional=(not self.causal),\n            num_buckets=self.relative_attention_num_buckets,\n            max_distance=self.relative_attention_max_distance,\n        )\n\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.transpose((2, 0, 1))[None, :, :, :]\n        return values\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.n_heads, self.key_value_proj_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.inner_dim,))\n\n    def _create_position_bias(\n            self, key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n    ):\n        cache_is_filled = self.causal and self.has_variable(\"cache\", \"cached_key\") and (not init_cache)\n        key_length = key_states.shape[1]\n        query_length = key_length if cache_is_filled else query_states.shape[1]\n\n        if self.has_relative_attention_bias:\n            position_bias = self.compute_bias(query_length, key_length)\n        elif attention_mask is not None:\n            position_bias = jnp.zeros_like(attention_mask)\n        else:\n            position_bias = jnp.zeros((1, self.n_heads, query_length, key_length), dtype=self.dtype)\n\n        # if key and values are already calculated, only the last query position bias should be taken\n        if cache_is_filled:\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            position_bias = jax.lax.dynamic_slice(\n                position_bias,\n                (0, 0, causal_attention_mask_shift, 0),\n                (1, self.n_heads, seq_length, max_decoder_length),\n            )\n        return position_bias\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            key_value_states=None,\n            position_bias=None,\n            use_cache=False,\n            output_attentions=False,\n            deterministic=True,\n            init_cache=False,\n    ):\n\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # q, k, v projections\n        query_states = self.q(hidden_states)  # (batch_size, n_heads, seq_length, dim_per_head)\n        key_states = self.k(hidden_states) if key_value_states is None else self.k(key_value_states)\n        value_states = self.v(hidden_states) if key_value_states is None else self.v(key_value_states)\n\n        # reshape to (batch_size, seq_length, n_heads, head_dim)\n        query_states = self._split_heads(query_states)\n        key_states = self._split_heads(key_states)\n        value_states = self._split_heads(value_states)\n        # if self.config.use_sharding_constraint:\n        #     query_states = with_sharding_constraint(\n        #         query_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, \"tp\", None)\n        #     )\n        #     key_states = with_sharding_constraint(\n        #         key_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        #     value_states = with_sharding_constraint(\n        #         value_states, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None)\n        #     )\n        # counter-act scaling in dot_product_attention_weights function\n        query_states *= jnp.sqrt(query_states.shape[-1])\n\n        # for fast decoding causal attention mask should be shifted\n        causal_attention_mask_shift = (\n            self.variables[\"cache\"][\"cache_index\"] if (self.has_variable(\"cache\", \"cached_key\") and self.causal) else 0\n        )\n        # create causal attention_mask; attention_mask has to be defined when model is causal\n        if self.causal:\n            causal_attention_mask = make_causal_mask(attention_mask, dtype=\"bool\")\n\n            # fast decoding for generate requires special attention_mask\n            if self.has_variable(\"cache\", \"cached_key\"):\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_attention_mask = jax.lax.dynamic_slice(\n                    causal_attention_mask,\n                    (0, 0, causal_attention_mask_shift, 0),\n                    (1, 1, seq_length, max_decoder_length),\n                )\n\n            # broadcast causal attention mask &amp; attention mask to fit for merge\n            causal_attention_mask = jnp.broadcast_to(\n                causal_attention_mask, (batch_size,) + causal_attention_mask.shape[1:]\n            )\n            attention_mask = jnp.broadcast_to(\n                jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_attention_mask.shape\n            )\n            attention_mask = combine_masks(attention_mask, causal_attention_mask)\n        elif attention_mask is not None:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key_states, value_states, attention_attention_mask = self._concatenate_to_cache(\n                key_states, value_states, query_states, attention_mask\n            )\n\n        # replace masked positions with -10_000\n        if attention_mask is not None:\n            mask_value = jnp.finfo(self.dtype).min\n            attention_mask = jax.lax.select(\n                attention_mask &gt; 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, mask_value).astype(self.dtype),\n            )\n\n        if position_bias is None:\n            # compute position bias (only for first layer)\n            position_bias = self._create_position_bias(\n                key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n            )\n\n            if attention_mask is not None:\n                position_bias = position_bias + attention_mask\n\n        # create dropout rng\n        dropout_rng = None\n        if not deterministic and self.dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # Softmax(QK^T)\n        attn_weights = dot_product_attention_weights(\n            query_states,\n            key_states,\n            bias=position_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.dropout,\n            broadcast_dropout=True,\n            deterministic=deterministic,\n            dtype=self.dtype,\n        )\n\n        attn_weights = with_sharding_constraint(attn_weights, PartitionSpec(\n            (\"dp\", \"fsdp\"), \"sp\" if query_states.shape[1] != 1 else None, None, None\n        ))\n\n        # multiply with value states\n        attn_output = jnp.einsum(\"...hqk,...khd-&gt;...qhd\", attn_weights, value_states)\n\n        # bring back to (batch_size, seq_length, d_model)\n        attn_output = self._merge_heads(attn_output)\n        if self.config.shard_attention_computation:\n            attn_output = with_sharding_constraint(\n                attn_output, PartitionSpec(\n                    (\"dp\", \"fsdp\"),\n                    \"sp\" if attn_output.shape[1] != 1 else None,\n                    \"tp\"\n                )\n            )\n        # apply output matrix\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n\n        return outputs\n</code></pre>"},{"location":"generated-modules-t5-modelling_t5_flax/#src.python.easydel.modules.t5.modelling_t5_flax.FlaxT5Attention.compute_bias","title":"<code>compute_bias(query_length, key_length)</code>","text":"<p>Compute binned relative position bias</p> Source code in <code>src/python/easydel/modules/t5/modelling_t5_flax.py</code> <pre><code>def compute_bias(self, query_length, key_length):\n    \"\"\"Compute binned relative position bias\"\"\"\n    context_position = jnp.arange(query_length, dtype=\"i4\")[:, None]\n    memory_position = jnp.arange(key_length, dtype=\"i4\")[None, :]\n\n    relative_position = memory_position - context_position\n    relative_position_bucket = self._relative_position_bucket(\n        relative_position,\n        bidirectional=(not self.causal),\n        num_buckets=self.relative_attention_num_buckets,\n        max_distance=self.relative_attention_max_distance,\n    )\n\n    values = self.relative_attention_bias(relative_position_bucket)\n    values = values.transpose((2, 0, 1))[None, :, :, :]\n    return values\n</code></pre>"},{"location":"generated-modules-t5-modelling_t5_flax/#src.python.easydel.modules.t5.modelling_t5_flax.shift_tokens_right","title":"<code>shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id)</code>","text":"<p>Shift input ids one token to the right.</p> Source code in <code>src/python/easydel/modules/t5/modelling_t5_flax.py</code> <pre><code>def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -&gt; chex.Array:\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = jnp.zeros_like(input_ids)\n    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n\n    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n    return shifted_input_ids\n</code></pre>"},{"location":"generated-modules-t5-t5_configuration/","title":"modules.t5.t5_configuration","text":""},{"location":"generated-modules-whisper-modelling_whisper_flax/","title":"modules.whisper.modelling_whisper_flax","text":""},{"location":"generated-modules-whisper-whisper_configuration/","title":"modules.whisper.whisper_configuration","text":""},{"location":"generated-reinforcement_learning-core/","title":"reinforcement_learning.core","text":""},{"location":"generated-reinforcement_learning-core/#src.python.easydel.reinforcement_learning.core.add_suffix","title":"<code>add_suffix(input_dict, suffix)</code>","text":"<p>Add suffix to dict keys.</p> Source code in <code>src/python/easydel/reinforcement_learning/core.py</code> <pre><code>def add_suffix(input_dict, suffix):\n    \"\"\"Add suffix to dict keys.\"\"\"\n    return dict((k + suffix, v) for k, v in input_dict.items())\n</code></pre>"},{"location":"generated-reinforcement_learning-core/#src.python.easydel.reinforcement_learning.core.multinomial","title":"<code>multinomial(logits, num_samples, replacement=False)</code>","text":"<p>Implements the <code>torch.multinomial</code> function in JAX.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>array</code> <p>The unnormalized log probabilities of the events.</p> required <code>num_samples</code> <code>int</code> <p>The number of samples to draw.</p> required <code>replacement</code> <code>bool</code> <p>Don't use this ;</p> <code>False</code> <p>Returns:     jnp.array: A matrix of shape (num_samples, batch_size) containing the         sampled indices.</p> Source code in <code>src/python/easydel/reinforcement_learning/core.py</code> <pre><code>def multinomial(logits, num_samples: int, replacement: bool = False):\n    \"\"\"Implements the `torch.multinomial` function in JAX.\n\n    Args:\n        logits (jnp.array): The unnormalized log probabilities of the events.\n        num_samples (int): The number of samples to draw.\n        replacement (bool): Don't use this ;\\\n\n    Returns:\n        jnp.array: A matrix of shape (num_samples, batch_size) containing the\n            sampled indices.\n    \"\"\"\n    logits = jax.nn.log_softmax(logits, axis=-1)\n    if replacement:\n        return jax.random.categorical(logits, num_samples)\n    else:\n        samples = []\n        for _ in range(num_samples):\n            sample = jax.random.categorical(logits, 1)\n            samples.append(sample[0])\n            logits = logits.at[sample[0]].set(-jnp.inf)\n        return jnp.array(samples)\n</code></pre>"},{"location":"generated-reinforcement_learning-models-modelling_casual_language_rl/","title":"reinforcement_learning.models.modelling_casual_language_rl","text":""},{"location":"generated-reinforcement_learning-models-modelling_casual_language_rl/#src.python.easydel.reinforcement_learning.models.modelling_casual_language_rl.ValueHead","title":"<code>ValueHead</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/python/easydel/reinforcement_learning/models/modelling_casual_language_rl.py</code> <pre><code>class ValueHead(nn.Module):\n    summary_dropout_prob: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n    kernel_init: Callable = nn.initializers.orthogonal()\n\n    def setup(self):\n        \"\"\"The setup function is called by the model's constructor.\n        It initializes all the layers in your model, and assigns them to member variables.\n        The setup function should be used for any initialization that needs to happen before running forward().\n        This includes things like loading weights from a file, or setting up an optimizer.\n\n        Args:\n            self: Represent the instance of the class\n        \"\"\"\n        self.dropout = flax.linen.Dropout(self.summary_dropout_prob)\n\n        self.summary = Linear(\n            1,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=self.kernel_init,\n            use_bias=False\n        )\n\n    def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n        \"\"\"The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        Args:\n            self: Represent the instance of the class\n            hidden_states: chex.Array: Pass the hidden states of the\n                previous layer\n            deterministic: bool: Determine whether to use dropout\n\n        Returns:\n            A tensor of shape (batch_size, num_classes)\n        \"\"\"\n        return self.summary(self.dropout(hidden_states, deterministic=deterministic))\n</code></pre>"},{"location":"generated-reinforcement_learning-models-modelling_casual_language_rl/#src.python.easydel.reinforcement_learning.models.modelling_casual_language_rl.ValueHead.__call__","title":"<code>__call__(hidden_states, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <p>A tensor of shape (batch_size, num_classes)</p> Source code in <code>src/python/easydel/reinforcement_learning/models/modelling_casual_language_rl.py</code> <pre><code>def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n    \"\"\"The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    Args:\n        self: Represent the instance of the class\n        hidden_states: chex.Array: Pass the hidden states of the\n            previous layer\n        deterministic: bool: Determine whether to use dropout\n\n    Returns:\n        A tensor of shape (batch_size, num_classes)\n    \"\"\"\n    return self.summary(self.dropout(hidden_states, deterministic=deterministic))\n</code></pre>"},{"location":"generated-reinforcement_learning-models-modelling_casual_language_rl/#src.python.easydel.reinforcement_learning.models.modelling_casual_language_rl.ValueHead.setup","title":"<code>setup()</code>","text":"<p>The setup function is called by the model's constructor. It initializes all the layers in your model, and assigns them to member variables. The setup function should be used for any initialization that needs to happen before running forward(). This includes things like loading weights from a file, or setting up an optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required Source code in <code>src/python/easydel/reinforcement_learning/models/modelling_casual_language_rl.py</code> <pre><code>def setup(self):\n    \"\"\"The setup function is called by the model's constructor.\n    It initializes all the layers in your model, and assigns them to member variables.\n    The setup function should be used for any initialization that needs to happen before running forward().\n    This includes things like loading weights from a file, or setting up an optimizer.\n\n    Args:\n        self: Represent the instance of the class\n    \"\"\"\n    self.dropout = flax.linen.Dropout(self.summary_dropout_prob)\n\n    self.summary = Linear(\n        1,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        precision=self.precision,\n        kernel_init=self.kernel_init,\n        use_bias=False\n    )\n</code></pre>"},{"location":"generated-reinforcement_learning-trainer-partitioner_config/","title":"reinforcement_learning.trainer.partitioner_config","text":""},{"location":"generated-reinforcement_learning-trainer-ppo_config/","title":"reinforcement_learning.trainer.ppo_config","text":""},{"location":"generated-reinforcement_learning-trainer-ppo_config/#src.python.easydel.reinforcement_learning.trainer.ppo_config.PPOConfig","title":"<code>PPOConfig</code>","text":"Source code in <code>src/python/easydel/reinforcement_learning/trainer/ppo_config.py</code> <pre><code>class PPOConfig:\n    def __init__(\n            self,\n            exp_name: str = os.path.basename(sys.argv[0])[: -len(\".py\")],\n            seed: int = 0,\n            task_name: Optional[str] = None,\n            model_name: Optional[str] = None,\n            query_dataset: Optional[str] = None,\n            reward_model: Optional[str] = None,\n            remove_unused_columns: bool = True,\n            tracker_kwargs: Optional[dict] = None,\n            accelerator_kwargs: Optional[dict] = None,\n            project_kwargs: Optional[dict] = None,\n            tracker_project_name: str = \"trl\",\n            push_to_hub_if_best_kwargs: Optional[dict] = None,\n            steps: int = 20000,\n            learning_rate: float = 1e-5,\n            adap_kl_ctrl: bool = True,\n            init_kl_coef: Optional[float] = 0.2,\n            kl_penalty: Literal[\"kl\", \"abs\", \"mse\", \"full\"] = \"kl\",\n            target: Optional[float] = 6,\n            horizon: Optional[float] = 10000,\n            gamma: float = 1,\n            lam: float = 0.95,\n            cliprange: float = 0.2,\n            cliprange_value: float = 0.2,\n            vf_coef: float = 0.1,\n            batch_size: int = 256,\n            gradient_accumulation_steps: int = 1,\n            ppo_epochs: int = 4,\n            max_grad_norm: Optional[float] = None,\n            target_kl: float = 1,\n            compare_steps: int = 1,\n            ratio_threshold: float = 10.0,\n            use_score_scaling: bool = False,\n            use_score_norm: bool = False,\n            score_clip: Optional[float] = None,\n            whiten_rewards: bool = False,\n            is_encoder_decoder: Optional[bool] = None,\n            warmup_steps: Optional[int] = 0,\n            learning_rate_end: float = 1e-5,\n            extra_optimizer_kwargs: dict | None = None,\n            weight_decay: Optional[float] = 0.01,\n    ):\n        \"\"\"Configuration class for PPOTrainer\n\n        Args:\n            exp_name: str : the name of this experiment (by default is\n                the file name without the extension name)\n            seed: int :Seed value for random generations\n            task_name: Optional[str] : Name of task to use - used only\n                for tracking purposes\n            model_name: Optional[str] :Name of model to use - used only\n                for tracking purposes\n            query_dataset: Optional[str] :Name of dataset to query -\n                used only for tracking purposes\n            reward_model: Optional[str] :The reward model to use - used\n                only for tracking purposes\n            remove_unused_columns: bool : Remove unused columns from the\n                dataset if `datasets.Dataset` is used\n            tracker_kwargs: Optional[dict] : Keyword arguments for the\n                tracker\n            accelerator_kwargs: Optional[dict] :Keyword arguments for\n                the accelerator\n            project_kwargs: Optional[dict] : Keyword arguments for the\n                accelerator project config (e.g. `logging_dir`)\n            tracker_project_name: str :Name of project to use for\n                tracking\n            push_to_hub_if_best_kwargs: Optional[dict] :Keyword\n                arguments for pushing model to the hub during training\n            steps: int : Number of training steps\n            learning_rate: float :Adam learning rate\n            adap_kl_ctrl: bool :Use adaptive KL control, otherwise\n                linear\n            init_kl_coef: Optional[float] : Initial KL penalty\n                coefficient (used for adaptive and linear control)\n            kl_penalty: Literal[\"kl\", \"abs\", \"mse\", \"full\"] : kl penalty\n                options: 'kl': model_logp - ref_logp,\n            target: Optional[float] :Target KL value for adaptive KL\n                control\n            horizon: Optional[float] :Horizon for adaptive KL control\n            gamma: float :Gamma parameter for advantage calculation\n            lam: float : Lambda parameter for advantage calculation\n            cliprange: float : Range for clipping in PPO policy gradient\n                loss\n            cliprange_value: float : Range for clipping values in loss\n                calculation\n            vf_coef: float : Scaling factor for value loss\n            batch_size: int :Number of samples per optimisation step\n            gradient_accumulation_steps: int :The number of gradient\n                accumulation steps\n            ppo_epochs: int : Number of optimisation epochs per batch of\n                samples\n            max_grad_norm: Optional[float] :Maximum gradient norm for\n                gradient clipping\n            target_kl: float :Stop early if we exceed this value by over\n                50%\n            compare_steps: int : Number of steps between comparison of\n                the current reward with the best seen so far\n            ratio_threshold: float :Skip mini-batches with high PPO\n                ratios that can cause loss spikes\n            use_score_scaling: bool : Use score scaling\n            use_score_norm: bool : Use score normalization. Only\n                applicable if use_score_scaling is True\n            score_clip: Optional[float] :Score clipping\n            whiten_rewards: bool :Whiten the rewards before compute\n                advantages\n            is_encoder_decoder: Optional[bool] :TO BE FILLED In RUNTIME:\n                Whether the model is an encoder-decoder model\n            warmup_steps: Optional[int]:\n            learning_rate_end: float :\n            extra_optimizer_kwargs: dict | None :\n            weight_decay: Optional[float] : Weight decay is Optimizer\n                Weight decay :\\\n        (e.g. pretrained_model_name_or_path).\n        'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution\n        \"\"\"\n\n        tracker_kwargs = tracker_kwargs if tracker_kwargs is not None else {}\n        accelerator_kwargs = accelerator_kwargs if accelerator_kwargs is not None else {}\n        project_kwargs = project_kwargs if project_kwargs is not None else {}\n        push_to_hub_if_best_kwargs = push_to_hub_if_best_kwargs if push_to_hub_if_best_kwargs is not None else {}\n        self.exp_name = exp_name\n        self.seed = seed\n        self.task_name = task_name\n        self.model_name = model_name\n        self.query_dataset = query_dataset\n        self.reward_model = reward_model\n        self.remove_unused_columns = remove_unused_columns\n        self.tracker_kwargs = tracker_kwargs\n        self.accelerator_kwargs = accelerator_kwargs\n        self.project_kwargs = project_kwargs\n        self.tracker_project_name = tracker_project_name\n        self.push_to_hub_if_best_kwargs = push_to_hub_if_best_kwargs\n        self.steps = steps\n        self.learning_rate = learning_rate\n        self.adap_kl_ctrl = adap_kl_ctrl\n        self.init_kl_coef = init_kl_coef\n        self.kl_penalty = kl_penalty\n        self.target = target\n        self.horizon = horizon\n        self.gamma = gamma\n        self.lam = lam\n        self.cliprange = cliprange\n        self.cliprange_value = cliprange_value\n        self.vf_coef = vf_coef\n        self.batch_size = batch_size\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.ppo_epochs = ppo_epochs\n        self.max_grad_norm = max_grad_norm\n        self.target_kl = target_kl\n        self.compare_steps = compare_steps\n        self.ratio_threshold = ratio_threshold\n        self.use_score_scaling = use_score_scaling\n        self.use_score_norm = use_score_norm\n        self.score_clip = score_clip\n        self.whiten_rewards = whiten_rewards\n        self.is_encoder_decoder = is_encoder_decoder\n        self.warmup_steps = warmup_steps\n        self.learning_rate_end = learning_rate_end\n        self.extra_optimizer_kwargs = extra_optimizer_kwargs\n        self.weight_decay = weight_decay\n        self.total_ppo_epochs = int(np.ceil(self.steps / (self.batch_size * self.gradient_accumulation_steps)))\n        assert self.kl_penalty in [\"kl\", \"abs\", \"mse\", \"full\"]\n\n    def to_dict(self):\n        output_dict = {}\n        for key, value in self.__dict__.items():\n            output_dict[key] = value\n        return flatten_dict(output_dict)\n</code></pre>"},{"location":"generated-reinforcement_learning-trainer-ppo_config/#src.python.easydel.reinforcement_learning.trainer.ppo_config.PPOConfig.__init__","title":"<code>__init__(exp_name=os.path.basename(sys.argv[0])[:-len('.py')], seed=0, task_name=None, model_name=None, query_dataset=None, reward_model=None, remove_unused_columns=True, tracker_kwargs=None, accelerator_kwargs=None, project_kwargs=None, tracker_project_name='trl', push_to_hub_if_best_kwargs=None, steps=20000, learning_rate=1e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, gradient_accumulation_steps=1, ppo_epochs=4, max_grad_norm=None, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, warmup_steps=0, learning_rate_end=1e-05, extra_optimizer_kwargs=None, weight_decay=0.01)</code>","text":"<p>Configuration class for PPOTrainer</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>str : the name of this experiment (by default is the file name without the extension name)</p> <code>basename(argv[0])[:-len('.py')]</code> <code>seed</code> <code>int</code> <p>int :Seed value for random generations</p> <code>0</code> <code>task_name</code> <code>Optional[str]</code> <p>Optional[str] : Name of task to use - used only for tracking purposes</p> <code>None</code> <code>model_name</code> <code>Optional[str]</code> <p>Optional[str] :Name of model to use - used only for tracking purposes</p> <code>None</code> <code>query_dataset</code> <code>Optional[str]</code> <p>Optional[str] :Name of dataset to query - used only for tracking purposes</p> <code>None</code> <code>reward_model</code> <code>Optional[str]</code> <p>Optional[str] :The reward model to use - used only for tracking purposes</p> <code>None</code> <code>remove_unused_columns</code> <code>bool</code> <p>bool : Remove unused columns from the dataset if <code>datasets.Dataset</code> is used</p> <code>True</code> <code>tracker_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict] : Keyword arguments for the tracker</p> <code>None</code> <code>accelerator_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict] :Keyword arguments for the accelerator</p> <code>None</code> <code>project_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict] : Keyword arguments for the accelerator project config (e.g. <code>logging_dir</code>)</p> <code>None</code> <code>tracker_project_name</code> <code>str</code> <p>str :Name of project to use for tracking</p> <code>'trl'</code> <code>push_to_hub_if_best_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict] :Keyword arguments for pushing model to the hub during training</p> <code>None</code> <code>steps</code> <code>int</code> <p>int : Number of training steps</p> <code>20000</code> <code>learning_rate</code> <code>float</code> <p>float :Adam learning rate</p> <code>1e-05</code> <code>adap_kl_ctrl</code> <code>bool</code> <p>bool :Use adaptive KL control, otherwise linear</p> <code>True</code> <code>init_kl_coef</code> <code>Optional[float]</code> <p>Optional[float] : Initial KL penalty coefficient (used for adaptive and linear control)</p> <code>0.2</code> <code>kl_penalty</code> <code>Literal['kl', 'abs', 'mse', 'full']</code> <p>Literal[\"kl\", \"abs\", \"mse\", \"full\"] : kl penalty options: 'kl': model_logp - ref_logp,</p> <code>'kl'</code> <code>target</code> <code>Optional[float]</code> <p>Optional[float] :Target KL value for adaptive KL control</p> <code>6</code> <code>horizon</code> <code>Optional[float]</code> <p>Optional[float] :Horizon for adaptive KL control</p> <code>10000</code> <code>gamma</code> <code>float</code> <p>float :Gamma parameter for advantage calculation</p> <code>1</code> <code>lam</code> <code>float</code> <p>float : Lambda parameter for advantage calculation</p> <code>0.95</code> <code>cliprange</code> <code>float</code> <p>float : Range for clipping in PPO policy gradient loss</p> <code>0.2</code> <code>cliprange_value</code> <code>float</code> <p>float : Range for clipping values in loss calculation</p> <code>0.2</code> <code>vf_coef</code> <code>float</code> <p>float : Scaling factor for value loss</p> <code>0.1</code> <code>batch_size</code> <code>int</code> <p>int :Number of samples per optimisation step</p> <code>256</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int :The number of gradient accumulation steps</p> <code>1</code> <code>ppo_epochs</code> <code>int</code> <p>int : Number of optimisation epochs per batch of samples</p> <code>4</code> <code>max_grad_norm</code> <code>Optional[float]</code> <p>Optional[float] :Maximum gradient norm for gradient clipping</p> <code>None</code> <code>target_kl</code> <code>float</code> <p>float :Stop early if we exceed this value by over 50%</p> <code>1</code> <code>compare_steps</code> <code>int</code> <p>int : Number of steps between comparison of the current reward with the best seen so far</p> <code>1</code> <code>ratio_threshold</code> <code>float</code> <p>float :Skip mini-batches with high PPO ratios that can cause loss spikes</p> <code>10.0</code> <code>use_score_scaling</code> <code>bool</code> <p>bool : Use score scaling</p> <code>False</code> <code>use_score_norm</code> <code>bool</code> <p>bool : Use score normalization. Only applicable if use_score_scaling is True</p> <code>False</code> <code>score_clip</code> <code>Optional[float]</code> <p>Optional[float] :Score clipping</p> <code>None</code> <code>whiten_rewards</code> <code>bool</code> <p>bool :Whiten the rewards before compute advantages</p> <code>False</code> <code>is_encoder_decoder</code> <code>Optional[bool]</code> <p>Optional[bool] :TO BE FILLED In RUNTIME: Whether the model is an encoder-decoder model</p> <code>None</code> <code>warmup_steps</code> <code>Optional[int]</code> <p>Optional[int]:</p> <code>0</code> <code>learning_rate_end</code> <code>float</code> <p>float :</p> <code>1e-05</code> <code>extra_optimizer_kwargs</code> <code>dict | None</code> <p>dict | None :</p> <code>None</code> <code>weight_decay</code> <code>Optional[float]</code> <p>Optional[float] : Weight decay is Optimizer Weight decay :        (e.g. pretrained_model_name_or_path).</p> <code>0.01</code> <p>'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution</p> Source code in <code>src/python/easydel/reinforcement_learning/trainer/ppo_config.py</code> <pre><code>def __init__(\n        self,\n        exp_name: str = os.path.basename(sys.argv[0])[: -len(\".py\")],\n        seed: int = 0,\n        task_name: Optional[str] = None,\n        model_name: Optional[str] = None,\n        query_dataset: Optional[str] = None,\n        reward_model: Optional[str] = None,\n        remove_unused_columns: bool = True,\n        tracker_kwargs: Optional[dict] = None,\n        accelerator_kwargs: Optional[dict] = None,\n        project_kwargs: Optional[dict] = None,\n        tracker_project_name: str = \"trl\",\n        push_to_hub_if_best_kwargs: Optional[dict] = None,\n        steps: int = 20000,\n        learning_rate: float = 1e-5,\n        adap_kl_ctrl: bool = True,\n        init_kl_coef: Optional[float] = 0.2,\n        kl_penalty: Literal[\"kl\", \"abs\", \"mse\", \"full\"] = \"kl\",\n        target: Optional[float] = 6,\n        horizon: Optional[float] = 10000,\n        gamma: float = 1,\n        lam: float = 0.95,\n        cliprange: float = 0.2,\n        cliprange_value: float = 0.2,\n        vf_coef: float = 0.1,\n        batch_size: int = 256,\n        gradient_accumulation_steps: int = 1,\n        ppo_epochs: int = 4,\n        max_grad_norm: Optional[float] = None,\n        target_kl: float = 1,\n        compare_steps: int = 1,\n        ratio_threshold: float = 10.0,\n        use_score_scaling: bool = False,\n        use_score_norm: bool = False,\n        score_clip: Optional[float] = None,\n        whiten_rewards: bool = False,\n        is_encoder_decoder: Optional[bool] = None,\n        warmup_steps: Optional[int] = 0,\n        learning_rate_end: float = 1e-5,\n        extra_optimizer_kwargs: dict | None = None,\n        weight_decay: Optional[float] = 0.01,\n):\n    \"\"\"Configuration class for PPOTrainer\n\n    Args:\n        exp_name: str : the name of this experiment (by default is\n            the file name without the extension name)\n        seed: int :Seed value for random generations\n        task_name: Optional[str] : Name of task to use - used only\n            for tracking purposes\n        model_name: Optional[str] :Name of model to use - used only\n            for tracking purposes\n        query_dataset: Optional[str] :Name of dataset to query -\n            used only for tracking purposes\n        reward_model: Optional[str] :The reward model to use - used\n            only for tracking purposes\n        remove_unused_columns: bool : Remove unused columns from the\n            dataset if `datasets.Dataset` is used\n        tracker_kwargs: Optional[dict] : Keyword arguments for the\n            tracker\n        accelerator_kwargs: Optional[dict] :Keyword arguments for\n            the accelerator\n        project_kwargs: Optional[dict] : Keyword arguments for the\n            accelerator project config (e.g. `logging_dir`)\n        tracker_project_name: str :Name of project to use for\n            tracking\n        push_to_hub_if_best_kwargs: Optional[dict] :Keyword\n            arguments for pushing model to the hub during training\n        steps: int : Number of training steps\n        learning_rate: float :Adam learning rate\n        adap_kl_ctrl: bool :Use adaptive KL control, otherwise\n            linear\n        init_kl_coef: Optional[float] : Initial KL penalty\n            coefficient (used for adaptive and linear control)\n        kl_penalty: Literal[\"kl\", \"abs\", \"mse\", \"full\"] : kl penalty\n            options: 'kl': model_logp - ref_logp,\n        target: Optional[float] :Target KL value for adaptive KL\n            control\n        horizon: Optional[float] :Horizon for adaptive KL control\n        gamma: float :Gamma parameter for advantage calculation\n        lam: float : Lambda parameter for advantage calculation\n        cliprange: float : Range for clipping in PPO policy gradient\n            loss\n        cliprange_value: float : Range for clipping values in loss\n            calculation\n        vf_coef: float : Scaling factor for value loss\n        batch_size: int :Number of samples per optimisation step\n        gradient_accumulation_steps: int :The number of gradient\n            accumulation steps\n        ppo_epochs: int : Number of optimisation epochs per batch of\n            samples\n        max_grad_norm: Optional[float] :Maximum gradient norm for\n            gradient clipping\n        target_kl: float :Stop early if we exceed this value by over\n            50%\n        compare_steps: int : Number of steps between comparison of\n            the current reward with the best seen so far\n        ratio_threshold: float :Skip mini-batches with high PPO\n            ratios that can cause loss spikes\n        use_score_scaling: bool : Use score scaling\n        use_score_norm: bool : Use score normalization. Only\n            applicable if use_score_scaling is True\n        score_clip: Optional[float] :Score clipping\n        whiten_rewards: bool :Whiten the rewards before compute\n            advantages\n        is_encoder_decoder: Optional[bool] :TO BE FILLED In RUNTIME:\n            Whether the model is an encoder-decoder model\n        warmup_steps: Optional[int]:\n        learning_rate_end: float :\n        extra_optimizer_kwargs: dict | None :\n        weight_decay: Optional[float] : Weight decay is Optimizer\n            Weight decay :\\\n    (e.g. pretrained_model_name_or_path).\n    'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution\n    \"\"\"\n\n    tracker_kwargs = tracker_kwargs if tracker_kwargs is not None else {}\n    accelerator_kwargs = accelerator_kwargs if accelerator_kwargs is not None else {}\n    project_kwargs = project_kwargs if project_kwargs is not None else {}\n    push_to_hub_if_best_kwargs = push_to_hub_if_best_kwargs if push_to_hub_if_best_kwargs is not None else {}\n    self.exp_name = exp_name\n    self.seed = seed\n    self.task_name = task_name\n    self.model_name = model_name\n    self.query_dataset = query_dataset\n    self.reward_model = reward_model\n    self.remove_unused_columns = remove_unused_columns\n    self.tracker_kwargs = tracker_kwargs\n    self.accelerator_kwargs = accelerator_kwargs\n    self.project_kwargs = project_kwargs\n    self.tracker_project_name = tracker_project_name\n    self.push_to_hub_if_best_kwargs = push_to_hub_if_best_kwargs\n    self.steps = steps\n    self.learning_rate = learning_rate\n    self.adap_kl_ctrl = adap_kl_ctrl\n    self.init_kl_coef = init_kl_coef\n    self.kl_penalty = kl_penalty\n    self.target = target\n    self.horizon = horizon\n    self.gamma = gamma\n    self.lam = lam\n    self.cliprange = cliprange\n    self.cliprange_value = cliprange_value\n    self.vf_coef = vf_coef\n    self.batch_size = batch_size\n    self.gradient_accumulation_steps = gradient_accumulation_steps\n    self.ppo_epochs = ppo_epochs\n    self.max_grad_norm = max_grad_norm\n    self.target_kl = target_kl\n    self.compare_steps = compare_steps\n    self.ratio_threshold = ratio_threshold\n    self.use_score_scaling = use_score_scaling\n    self.use_score_norm = use_score_norm\n    self.score_clip = score_clip\n    self.whiten_rewards = whiten_rewards\n    self.is_encoder_decoder = is_encoder_decoder\n    self.warmup_steps = warmup_steps\n    self.learning_rate_end = learning_rate_end\n    self.extra_optimizer_kwargs = extra_optimizer_kwargs\n    self.weight_decay = weight_decay\n    self.total_ppo_epochs = int(np.ceil(self.steps / (self.batch_size * self.gradient_accumulation_steps)))\n    assert self.kl_penalty in [\"kl\", \"abs\", \"mse\", \"full\"]\n</code></pre>"},{"location":"generated-reinforcement_learning-trainer-ppo_trainer/","title":"reinforcement_learning.trainer.ppo_trainer","text":""},{"location":"generated-reinforcement_learning-trainer-training_configs/","title":"reinforcement_learning.trainer.training_configs","text":""},{"location":"generated-reinforcement_learning-trainer-training_configs/#src.python.easydel.reinforcement_learning.trainer.training_configs.RewardConfig","title":"<code>RewardConfig</code>  <code>dataclass</code>","text":"Source code in <code>src/python/easydel/reinforcement_learning/trainer/training_configs.py</code> <pre><code>@dataclass\nclass RewardConfig:\n    max_length: Optional[int] = None\n    \"\"\"\n    The maximum length of the sequences in the batch. This argument is \n    required if you want to use the default data collator.\n    \"\"\"\n    gradient_checkpointing: Optional[bool] = True\n    \"\"\"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\"\"\n    gradient_checkpointing_kwargs: Optional[dict] = None\n    \"\"\"Keyword arguments to pass to the gradient checkpointing function.\"\"\"\n</code></pre>"},{"location":"generated-reinforcement_learning-trainer-training_configs/#src.python.easydel.reinforcement_learning.trainer.training_configs.RewardConfig.gradient_checkpointing","title":"<code>gradient_checkpointing: Optional[bool] = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If True, use gradient checkpointing to save memory at the expense of slower backward pass.</p>"},{"location":"generated-reinforcement_learning-trainer-training_configs/#src.python.easydel.reinforcement_learning.trainer.training_configs.RewardConfig.gradient_checkpointing_kwargs","title":"<code>gradient_checkpointing_kwargs: Optional[dict] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Keyword arguments to pass to the gradient checkpointing function.</p>"},{"location":"generated-reinforcement_learning-trainer-training_configs/#src.python.easydel.reinforcement_learning.trainer.training_configs.RewardConfig.max_length","title":"<code>max_length: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The maximum length of the sequences in the batch. This argument is  required if you want to use the default data collator.</p>"},{"location":"generated-reinforcement_learning-trainer-utils/","title":"reinforcement_learning.trainer.utils","text":""},{"location":"generated-reinforcement_learning-utils-collectors/","title":"reinforcement_learning.utils.collectors","text":""},{"location":"generated-reinforcement_learning-utils-collectors/#src.python.easydel.reinforcement_learning.utils.collectors.DPODataCollatorWithPadding","title":"<code>DPODataCollatorWithPadding</code>  <code>dataclass</code>","text":"<p>DPO DataCollator class that pads the tokenized inputs to the maximum length of the batch.</p> <p>Parameters:</p> Name Type Description Default <code>pad_token_id</code> <code>int</code> <p>int: The tokenizers pad_token_id.</p> <code>0</code> <code>label_pad_token_id</code> <code>int</code> <p>int: The label used for masking.</p> <code>-100</code> <code>is_encoder_decoder</code> <code>Optional[bool]</code> <p>Optional[bool]: Whether you model has an encoder_decoder architecture</p> <code>False</code> Source code in <code>src/python/easydel/reinforcement_learning/utils/collectors.py</code> <pre><code>@dataclass\nclass DPODataCollatorWithPadding:\n    r\"\"\"DPO DataCollator class that pads the tokenized inputs to the maximum length of the batch.\n\n    Args:\n        pad_token_id: int: The tokenizers pad_token_id.\n        label_pad_token_id: int: The label used for masking.\n        is_encoder_decoder: Optional[bool]: Whether you model has an\n            encoder_decoder architecture\n    \"\"\"\n\n    pad_token_id: int = 0\n    label_pad_token_id: int = -100\n    is_encoder_decoder: Optional[bool] = False\n\n    def __call__(self, features: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        padded_batch = {}\n        for k in features[0].keys():\n            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n                if self.is_encoder_decoder:\n                    to_pad = [jnp.array(ex[k], dtype=\"i4\") for ex in features]\n\n                    if (k.startswith(\"prompt\")) and (k.endswith(\"input_ids\")):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    elif (k.startswith(\"chosen\")) or (k.startswith(\"rejected\")) or (\"decoder\" in k):\n                        padding_value = self.label_pad_token_id\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value).astype(\"i4\")\n                else:\n                    if \"prompt\" in k:\n                        to_pad = [jnp.array(ex[k][::-1], dtype=\"i4\") for ex in features]\n                    else:\n                        to_pad = [jnp.array(ex[k], dtype=\"i4\") for ex in features]\n                    if k.endswith(\"_input_ids\"):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_labels\"):\n                        padding_value = self.label_pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value).astype(\"i4\")\n                    if \"prompt\" in k:\n                        padded_batch[k] = jnp.flip(padded_batch[k], axis=[1])\n            elif k.endswith(\"_logps\"):\n                padded_batch[k] = jnp.array([ex[k] for ex in features])\n            else:\n                padded_batch[k] = [ex[k] for ex in features]\n        return padded_batch\n</code></pre>"},{"location":"generated-serve-client/","title":"serve.client","text":""},{"location":"generated-serve-configuration/","title":"serve.configuration","text":""},{"location":"generated-serve-configuration/#src.python.easydel.serve.configuration.EasyDeLServeEngineConfig","title":"<code>EasyDeLServeEngineConfig</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>str: Set the host address of the server</p> required <code>port</code> <p>int: Specify the port number that the server will run on</p> required <code>batch_size</code> <p>int: Set the batch size of the model</p> required <code>max_sequence_length</code> <p>int: Set the maximum length of the text that can be generated</p> required <code>max_new_tokens</code> <p>int: Determine how many tokens can be added to the vocabulary</p> required <code>max_compile_tokens</code> <p>int: Set the maximum number of tokens that can be streamed at a time</p> required <code>generation_ps</code> <p>jax.sharding.PartitionSpec : PartitionSpec to use for sharding data</p> required <code>temperature</code> <p>float: Control the randomness of the output</p> required <code>top_p</code> <p>float: Control the diversity of the text generated</p> required <code>top_k</code> <p>int: Limit the number of tokens that can be generated</p> required <code>logging</code> <p>bool: Print out the progress of the server</p> required <code>mesh_axes_names</code> <p>Sequence[str]: Specify the names of the axes in the mesh tensor</p> required <code>mesh_axes_shape</code> <p>Sequence[int]: Specify the shape of the mesh</p> required <code>dtype</code> <p>str: Specify the data type of the model</p> required <code>use_prefix_tokenizer</code> <p>bool: Determine if the tokenizer should be used to generate tokens</p> required <code>pre_compile</code> <p>bool: Pre-compile the model</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/python/easydel/serve/configuration.py</code> <pre><code>class EasyDeLServeEngineConfig(NamedTuple):\n    \"\"\"\n    Args:\n        host: str: Set the host address of the server\n        port: int: Specify the port number that the server will run on\n        batch_size: int: Set the batch size of the model\n        max_sequence_length: int: Set the maximum length of the text\n            that can be generated\n        max_new_tokens: int: Determine how many tokens can be added to\n            the vocabulary\n        max_compile_tokens: int: Set the maximum number of tokens that\n            can be streamed at a time\n        generation_ps: jax.sharding.PartitionSpec : PartitionSpec to use\n            for sharding data\n        temperature: float: Control the randomness of the output\n        top_p: float: Control the diversity of the text generated\n        top_k: int: Limit the number of tokens that can be generated\n        logging: bool: Print out the progress of the server\n        mesh_axes_names: Sequence[str]: Specify the names of the axes in\n            the mesh tensor\n        mesh_axes_shape: Sequence[int]: Specify the shape of the mesh\n        dtype: str: Specify the data type of the model\n        use_prefix_tokenizer: bool: Determine if the tokenizer should be\n            used to generate tokens\n        pre_compile: bool: Pre-compile the model\n\n    Returns:\n        Nothing\n    \"\"\"\n    host: str = \"0.0.0.0\"\n    port: int = 2059\n\n    batch_size: int = 1\n    max_sequence_length: int = 4096\n    max_new_tokens: int = 4096\n    max_compile_tokens: int = 64\n    temperature: float = 0.1\n    top_p: float = 0.95\n    top_k: int = 50\n    repetition_penalty: float = 1.2\n    greedy: bool = False\n\n    logging: bool = True\n\n    mesh_axes_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\")\n    mesh_axes_shape: Sequence[int] = (1, -1, 1, 1)\n    generation_ps: PartitionSpec = PartitionSpec(\"dp\", \"fsdp\")\n    dtype: str = \"fp16\"\n\n    eos_token_id: Optional[int] = None\n    pad_token_id: Optional[int] = None\n    bos_token_id: Optional[int] = None\n\n    use_prefix_tokenizer: bool = True\n    pre_compile: bool = True\n\n    verbose: bool = True\n\n    use_mxn_break_point: bool = True\n</code></pre>"},{"location":"generated-serve-serve/","title":"serve.serve","text":""},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine","title":"<code>EasyDeLServeEngine</code>","text":"Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>class EasyDeLServeEngine:\n    def __init__(\n            self,\n            llm: EasyDeLFlaxPretrainedModel,\n            params: Union[FrozenDict, dict],\n            tokenizer: PreTrainedTokenizerBase,\n            prefix_tokenizer: PreTrainedTokenizerBase,\n            greedy_generate_function: Callable,\n            non_greedy_generate_function: Callable,\n            serve_config: EasyDeLServeEngineConfig,\n    ):\n        self.llm = llm\n        self.params = params\n        self.tokenizer = tokenizer\n        self.prefix_tokenizer = prefix_tokenizer\n        self.greedy_generate_function = greedy_generate_function\n        self.non_greedy_generate_function = non_greedy_generate_function\n        self.serve_config = serve_config\n        if serve_config.pre_compile:\n            self.compile(verbose=serve_config.verbose)\n\n    def get_generation_function(self, greedy: bool):\n        return self.greedy_generate_function if greedy else self.non_greedy_generate_function\n\n    def conversation_template(self, conversation: List[Dict]) -&gt; str:\n        \"\"\"The conversation_template function takes a list of ConversationItem objects and returns a string.\n        where system message, user message, and assistant message are the content fields of the ConversationItem objects.\n        If there is no system message in the conversation, then it will be omitted from the template.\n\n        Args:\n            self: Refer to the current instance of a class\n            conversation: List[ConversationItem]: Pass in the\n                conversation items\n\n        Returns:\n            A string that is a concatenation of the messages in the\n            conversation\n        \"\"\"\n        return self.tokenizer.apply_chat_template(\n            conversation=conversation,\n            add_generation_prompt=True,\n            tokenize=False\n        )\n\n    async def generate(self, socket):\n        data = json.loads(await socket.recv())\n        prompt = self.conversation_template(data[\"conversation\"])\n        max_new_tokens = data.get(\"max_new_tokens\", None) or self.serve_config.max_new_tokens\n        greedy = data.get(\"greedy\", None) or self.serve_config.greedy\n        start = time.time()\n        send_data = {}\n        prl_res = 0\n        for response, num_token_generated in self.sample(\n                string=prompt,\n                max_new_tokens=max_new_tokens,\n                greedy=greedy,\n\n        ):\n            generation_duration = time.time() - start\n            tokens_pre_second = num_token_generated / generation_duration\n\n            send_data = {\n                \"response\": response[prl_res:],\n                \"num_token_generated\": num_token_generated,\n                \"greedy\": greedy,\n                \"model_prompt\": prompt,\n                \"generation_duration\": generation_duration,\n                \"tokens_pre_second\": tokens_pre_second,\n                \"done\": False\n            }\n            prl_res += len(response)\n            await socket.send(json.dumps(send_data))\n\n        send_data[\"done\"] = True\n        send_data[\"response\"] = \"\"\n        await socket.send(json.dumps(send_data))\n\n    async def handle_client(self, socket: websocket.WebSocket, path: str):\n        try:\n            logger.info(\"connection open\")\n            if path == \"/stream/v1/conversation\":\n                await self.generate(socket)\n            elif path == \"/\":\n                await socket.send(json.dumps({\"status\": \"EasyDeLServeEngine server is Running...\"}))  # noqa\n            else:\n                await socket.send(json.dumps({\"error\": f\"invalid path {path}\"}))  # noqa\n        except websockets.ConnectionClosed:\n            logger.info(\"connection closed\")\n        except Exception as e:\n            logger.warning(f\"Error: {e}\")\n\n    @staticmethod\n    def create_shard_and_gather_functions(\n            parameters: dict,\n            partition_rules: Tuple[Tuple[str, PartitionSpec]],\n            dtype: Union[jax.numpy.dtype, str] = \"fp16\"\n    ):\n\n        \"\"\"The create_shard_and_gather_functions function takes in a dictionary of parameters,\n        a tuple of partition rules, and an optional dtype. It then matches the partition rules to the\n        parameters and creates shard functions for each parameter. The shard functions are used to\n        split up a parameter into shards (or partitions) that can be stored on different devices.\n        The gather function is used to combine all the shards back together again.\n\n        Args:\n            parameters: dict: Specify the parameters of the model\n            partition_rules: Tuple[Tuple[str,  PartitionSpec]]: Specify\n                which parameters to partition\n            dtype: jax.numpy.dtype | str: Specify the data type of the\n                parameters\n\n        Returns:\n            A tuple of three elements:\n        \"\"\"\n        partition_specs = match_partition_rules(partition_rules, parameters)\n        shard_fns, gather_fns = make_shard_and_gather_fns(\n            partition_specs=partition_specs,\n            dtype_specs=get_dtype(dtype)\n        )\n        return shard_fns, gather_fns, partition_specs\n\n    @staticmethod\n    def shard_parameters(\n            mesh: Mesh,\n            params: Union[FrozenDict, dict],\n            partition_rules: Tuple[Tuple[str, PartitionSpec]],\n            serve_config: EasyDeLServeEngineConfig,\n    ):\n\n        \"\"\"The shard_parameters function takes a set of parameters and partitions them according to the partition_rules.\n\n        Args:\n            mesh: Mesh: Create a mesh object that is used to shard the\n                parameters\n            params: FrozenDict | dict: Pass in the parameters of the\n                model\n            partition_rules: Tuple[Tuple[str, PartitionSpec]]: Specify\n                the partitioning rules for each parameter\n            serve_config: EasyDeLServeEngineConfig: Specify the dtype of the\n                parameters\n        :param : Create a mesh of devices\n\n        Returns:\n            sharded parameters\n        \"\"\"\n\n        partition_specs = match_partition_rules(params=params, rules=partition_rules)\n        shard_fns, _ = make_shard_and_gather_fns(partition_specs, get_dtype(serve_config.dtype))\n\n        with mesh:\n            params = jax.tree_map(\n                lambda func, param: func(param), shard_fns, params\n            )\n\n        return params\n\n    @staticmethod\n    def create_generation_functions_and_tokenizers(\n            model: EasyDeLFlaxPretrainedModel,\n            tokenizer: PreTrainedTokenizerBase,\n            serve_config: EasyDeLServeEngineConfig,\n            partition_specs: dict[str, PartitionSpec]\n    ) -&gt; LLMBaseReq:\n        \"\"\"The create_generation_functions_and_tokenizers function is used to create the functions that will be used for\n        generation. It also creates a tokenizer object that can be used to encode and decode text. The function takes in\n        a model, a tokenizer, an EasyDeLServeEngineConfig object (which contains all the parameters needed for generation), and\n        partition_specs which are specifications about how data should be partitioned across devices.\n\n        Args:\n            model: EasyDeLFlaxPretrainedModel: Create the model and\n                tokenizer\n            tokenizer: PreTrainedTokenizerBase: Create a tokenizer\n                object\n            serve_config: EasyDeLServeEngineConfig: Create the generation\n                function\n            partition_specs: dict[str, PartitionSpec]: Specify the\n                sharding of the model parameters\n\n        Returns:\n            An LLMBaseReq object\n        \"\"\"\n        if tokenizer.pad_token is None:\n            logging.info(\n                \"Tokenizer does not contain padding token setting padding token to eos token for open end generation\")\n            tokenizer.pad_token = tokenizer.eos_token\n\n        try:\n            tokenizer.padding_side = \"left\"\n            tokenizer.truncation_side = \"left\"\n            prefix_tokenizer = copy.deepcopy(tokenizer)\n            tokenizer.padding_side = \"right\"\n            tokenizer.truncation_side = \"right\"\n            tokenizer = copy.deepcopy(tokenizer)\n\n        except:\n            warnings.warn(\n                f\"The class Model of Tokenizer {type(tokenizer)} do not support deepcopy option \"\n            )\n            if serve_config.use_prefix_tokenizer:\n                tokenizer.padding_side = \"left\"\n                tokenizer.truncation_side = \"left\"\n            else:\n                tokenizer.padding_side = \"right\"\n                tokenizer.truncation_side = \"right\"\n            prefix_tokenizer = tokenizer\n\n        @functools.partial(\n            pjit,\n            in_shardings=(partition_specs, PartitionSpec(), PartitionSpec()),\n            out_shardings=(PartitionSpec())\n        )\n        def greedy_generate_function(\n                parameters,\n                input_ids,\n                attention_mask\n        ):\n            input_ids = with_sharding_constraint(input_ids, serve_config.generation_ps)\n            attention_mask = with_sharding_constraint(attention_mask, serve_config.generation_ps)\n            predict = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                params=parameters,\n                generation_config=GenerationConfig(\n                    max_new_tokens=serve_config.max_compile_tokens,\n\n                    eos_token_id=serve_config.eos_token_id or tokenizer.eos_token_id,\n                    pad_token_id=serve_config.pad_token_id or tokenizer.pad_token_id,\n                    bos_token_id=serve_config.bos_token_id or tokenizer.bos_token_id,\n\n                    do_sample=False,\n                    num_beams=1,\n                )\n            ).sequences[:, input_ids.shape[1]:]\n            return predict\n\n        @functools.partial(\n            pjit,\n            in_shardings=(partition_specs, PartitionSpec(), PartitionSpec()),\n            out_shardings=(PartitionSpec())\n        )\n        def non_greedy_generate_function(\n                parameters,\n                input_ids,\n                attention_mask\n        ):\n            input_ids = with_sharding_constraint(input_ids, serve_config.generation_ps)\n            attention_mask = with_sharding_constraint(attention_mask, serve_config.generation_ps)\n            predict = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                params=parameters,\n                generation_config=GenerationConfig(\n                    max_new_tokens=serve_config.max_compile_tokens,\n\n                    eos_token_id=serve_config.eos_token_id or tokenizer.eos_token_id,\n                    pad_token_id=serve_config.pad_token_id or tokenizer.pad_token_id,\n                    bos_token_id=serve_config.bos_token_id or tokenizer.bos_token_id,\n\n                    temperature=serve_config.temperature,\n                    repetition_penalty=serve_config.repetition_penalty,\n                    do_sample=True,\n                    num_beams=1,\n                    top_p=serve_config.top_p,\n                    top_k=serve_config.top_k,\n                )\n            ).sequences[:, input_ids.shape[1]:]\n            return predict\n\n        return LLMBaseReq(\n            greedy_generate_function=greedy_generate_function,\n            non_greedy_generate_function=non_greedy_generate_function,\n            tokenizer=tokenizer,\n            prefix_tokenizer=prefix_tokenizer\n        )\n\n    @classmethod\n    def from_parameters(\n            cls,\n            llm: EasyDeLFlaxPretrainedModel,\n            params: dict,\n            tokenizer: PreTrainedTokenizerBase,\n            serve_config: EasyDeLServeEngineConfig,\n            partition_rules: Tuple[Tuple[str, PartitionSpec]],\n            shard_parameters: bool = True,\n    ):\n\n        \"\"\"The from_parameters function is the main entry point for creating a model that can be served.\n        It takes in a pretrained model, parameters, tokenizer and serve_config as input and returns an object of type\n        EasyServe.\n\n        Args:\n            cls: Create a new instance of the class\n            llm: EasyDeLFlaxPretrainedModel: Pass the model to the class\n            params: dict: Pass the parameters of the model\n            tokenizer: PreTrainedTokenizerBase: Create the tokenizer and\n                prefix_tokenizer\n            serve_config: EasyDeLServeEngineConfig: Configure the model for\n                serving\n            partition_rules: Tuple[Tuple[str, PartitionSpec]]: Partition\n                the parameters of the model\n            shard_parameters: bool: Specify whether the parameters\n                should be sharded or not\n        :param : Shard the parameters of the model\n\n        Returns:\n            A EasyServe object\n        \"\"\"\n        shard_fns, gather_fns, partition_specs = cls.create_shard_and_gather_functions(\n            parameters=params,\n            partition_rules=partition_rules,\n            dtype=serve_config.dtype\n        )\n        llm_base_req = cls.create_generation_functions_and_tokenizers(\n            model=llm,\n            tokenizer=tokenizer,\n            partition_specs=partition_specs,\n            serve_config=serve_config\n        )\n\n        if shard_parameters:\n            params = cls.shard_parameters(\n                params=params,\n                partition_rules=partition_rules,\n                serve_config=serve_config,\n                mesh=llm.config.jax_mesh()\n            )\n\n        return cls(\n            llm=llm,\n            serve_config=serve_config,\n            tokenizer=llm_base_req.tokenizer,\n            prefix_tokenizer=llm_base_req.prefix_tokenizer,\n            params=params,\n            greedy_generate_function=llm_base_req.greedy_generate_function,\n            non_greedy_generate_function=llm_base_req.non_greedy_generate_function,\n        )\n\n    def sample(\n            self,\n            string: str,\n            *,\n            greedy: bool = False,\n            max_new_tokens: int = None,\n            **kwargs\n    ):\n        \"\"\"The process function is the main function of a model. It takes in an input string and returns a list of strings\n        that are generated from that input string. The process function can be called multiple times with different inputs,\n        and each time it will return a new set of outputs based on those inputs.\n\n        Args:\n            self: Access the class attributes\n            string: str: Pass the string that we want to generate\n            greedy: bool: Determine whether to use the greedy or non-\n                greedy version of the generate function\n            max_new_tokens: int: Set the number of tokens to generate\n            **kwargs: Pass any additional parameters to the process\n                function\n\n        Returns:\n            A generator that yields the predicted text and the number of\n            tokens generated\n        \"\"\"\n        with self.llm.config.jax_mesh():\n            fixed_pad = self.serve_config.max_sequence_length - self.serve_config.max_compile_tokens\n            tokens = self.prefix_tokenizer(\n                string,\n                max_length=fixed_pad,\n                padding=\"max_length\",\n                return_tensors=\"jax\"\n            ) if self.serve_config.use_prefix_tokenizer else self.tokenizer(\n                string,\n                return_tensors=\"jax\"\n            )\n\n            input_ids = tokens.input_ids\n            attention_mask = tokens.attention_mask\n            num_generated_tokens = 0\n\n            for _ in range(\n                    (max_new_tokens or self.serve_config.max_new_tokens) // self.serve_config.max_compile_tokens):\n\n                predicted_token = self.get_generation_function(greedy=greedy)(\n                    self.params,\n                    input_ids,\n                    attention_mask\n                )\n\n                num_generated_tokens += predicted_token.shape[-1]\n                plus_attn_mask = jnp.ones(\n                    (len(attention_mask), self.serve_config.max_compile_tokens),\n                    dtype=\"i4\"\n                )\n\n                input_ids = jnp.concatenate(\n                    (input_ids, predicted_token), dtype=\"i4\",\n                    axis=-1\n                )[:, -fixed_pad:]\n\n                attention_mask = jnp.concatenate(\n                    (attention_mask, plus_attn_mask), dtype=\"i4\",\n                    axis=-1\n                )[:, -fixed_pad:]\n\n                returns = (\n                    self.tokenizer.decode(\n                        input_ids[0][-num_generated_tokens:],  # type:ignore\n                        skip_special_tokens=True\n                    ),\n                    num_generated_tokens\n                )\n\n                yield returns\n\n                if self.serve_config.use_mxn_break_point:\n                    if self.serve_config.max_compile_tokens != predicted_token.shape[-1]:\n                        break\n                if (\n                        predicted_token[0][-1] == (self.serve_config.eos_token_id or self.tokenizer.eos_token_id)\n                        or\n                        predicted_token[0][-1] == (self.serve_config.eos_token_id or self.prefix_tokenizer.eos_token_id)\n                ):\n                    break\n\n    def compile(self, verbose: bool = True) -&gt; bool:\n        \"\"\"The compile function is used to compile the model for use in inference.\n        It does this by running through all possible combinations of rules and actions,\n        and compiling them into functions that can be called later on during inference.\n        This allows us to avoid having to recompile the model every time we want to run it,\n        which would be very slow.\n\n        Args:\n            self: Represent the instance of the class\n            verbose: bool: Print out the compiling process\n\n        Returns:\n            True, but what does it do?\n        \"\"\"\n        if self.serve_config.use_prefix_tokenizer:\n            if verbose:\n                logger.info(\"Compiling greedy generate function\")\n            response, tokens = [None] * 2\n            for response, tokens in self.sample(\n                    string=\"\",\n                    max_new_tokens=self.serve_config.max_compile_tokens,\n                    greedy=True\n            ):\n                ...\n            if verbose:\n                logger.info(\"Compiling non-greedy generate function\")\n            for response, tokens in self.sample(\n                    string=\"\",\n                    max_new_tokens=self.serve_config.max_compile_tokens,\n                    greedy=False\n            ):\n                ...\n\n        else:\n            warnings.warn(\n                \"Skip Compiling the compiling process is useless \"\n                \"when you are not using prefix tokenizer\",\n            )\n        return True\n\n    def __repr__(self):\n\n        \"\"\"The __repr__ function is used to generate a string representation of an object.\n        This function should return a string that can be parsed by the Python interpreter\n        to recreate the object. The __repr__ function is called when you use print() on an\n        object, or when you type its name in the REPL.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            A string representation of the object\n        \"\"\"\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                try:\n                    repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n                except TypeError:\n                    ...\n        return string + \")\"\n\n    def __str__(self):\n\n        \"\"\"The __str__ function is called when you use the print function or when str() is used.\n        It should return a string representation of the object.\n\n        Args:\n            self: Refer to the instance of the class\n\n        Returns:\n            The object's string representation\n        \"\"\"\n        return self.__repr__()\n\n    def fire(self):\n        async def run_engine():\n            async with websockets.serve(self.handle_client, self.serve_config.host, self.serve_config.port) as ws:\n                logger.info(f\"Starting EasyDeL websocket server on {self.serve_config.host}:{self.serve_config.port}\")\n                await ws.wait_closed()\n\n        asyncio.run(run_engine())\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is used to generate a string representation of an object. This function should return a string that can be parsed by the Python interpreter to recreate the object. The repr function is called when you use print() on an object, or when you type its name in the REPL.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>A string representation of the object</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"The __repr__ function is used to generate a string representation of an object.\n    This function should return a string that can be parsed by the Python interpreter\n    to recreate the object. The __repr__ function is called when you use print() on an\n    object, or when you type its name in the REPL.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        A string representation of the object\n    \"\"\"\n    string = f\"{self.__class__.__name__}(\\n\"\n    for k, v in self.__dict__.items():\n        if not k.startswith(\"_\"):\n            try:\n                repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 500 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n            except TypeError:\n                ...\n    return string + \")\"\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you use the print function or when str() is used. It should return a string representation of the object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>The object's string representation</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>def __str__(self):\n\n    \"\"\"The __str__ function is called when you use the print function or when str() is used.\n    It should return a string representation of the object.\n\n    Args:\n        self: Refer to the instance of the class\n\n    Returns:\n        The object's string representation\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.compile","title":"<code>compile(verbose=True)</code>","text":"<p>The compile function is used to compile the model for use in inference. It does this by running through all possible combinations of rules and actions, and compiling them into functions that can be called later on during inference. This allows us to avoid having to recompile the model every time we want to run it, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>verbose</code> <code>bool</code> <p>bool: Print out the compiling process</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True, but what does it do?</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>def compile(self, verbose: bool = True) -&gt; bool:\n    \"\"\"The compile function is used to compile the model for use in inference.\n    It does this by running through all possible combinations of rules and actions,\n    and compiling them into functions that can be called later on during inference.\n    This allows us to avoid having to recompile the model every time we want to run it,\n    which would be very slow.\n\n    Args:\n        self: Represent the instance of the class\n        verbose: bool: Print out the compiling process\n\n    Returns:\n        True, but what does it do?\n    \"\"\"\n    if self.serve_config.use_prefix_tokenizer:\n        if verbose:\n            logger.info(\"Compiling greedy generate function\")\n        response, tokens = [None] * 2\n        for response, tokens in self.sample(\n                string=\"\",\n                max_new_tokens=self.serve_config.max_compile_tokens,\n                greedy=True\n        ):\n            ...\n        if verbose:\n            logger.info(\"Compiling non-greedy generate function\")\n        for response, tokens in self.sample(\n                string=\"\",\n                max_new_tokens=self.serve_config.max_compile_tokens,\n                greedy=False\n        ):\n            ...\n\n    else:\n        warnings.warn(\n            \"Skip Compiling the compiling process is useless \"\n            \"when you are not using prefix tokenizer\",\n        )\n    return True\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.conversation_template","title":"<code>conversation_template(conversation)</code>","text":"<p>The conversation_template function takes a list of ConversationItem objects and returns a string. where system message, user message, and assistant message are the content fields of the ConversationItem objects. If there is no system message in the conversation, then it will be omitted from the template.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current instance of a class</p> required <code>conversation</code> <code>List[Dict]</code> <p>List[ConversationItem]: Pass in the conversation items</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that is a concatenation of the messages in the</p> <code>str</code> <p>conversation</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>def conversation_template(self, conversation: List[Dict]) -&gt; str:\n    \"\"\"The conversation_template function takes a list of ConversationItem objects and returns a string.\n    where system message, user message, and assistant message are the content fields of the ConversationItem objects.\n    If there is no system message in the conversation, then it will be omitted from the template.\n\n    Args:\n        self: Refer to the current instance of a class\n        conversation: List[ConversationItem]: Pass in the\n            conversation items\n\n    Returns:\n        A string that is a concatenation of the messages in the\n        conversation\n    \"\"\"\n    return self.tokenizer.apply_chat_template(\n        conversation=conversation,\n        add_generation_prompt=True,\n        tokenize=False\n    )\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.create_generation_functions_and_tokenizers","title":"<code>create_generation_functions_and_tokenizers(model, tokenizer, serve_config, partition_specs)</code>  <code>staticmethod</code>","text":"<p>The create_generation_functions_and_tokenizers function is used to create the functions that will be used for generation. It also creates a tokenizer object that can be used to encode and decode text. The function takes in a model, a tokenizer, an EasyDeLServeEngineConfig object (which contains all the parameters needed for generation), and partition_specs which are specifications about how data should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>EasyDeLFlaxPretrainedModel</code> <p>EasyDeLFlaxPretrainedModel: Create the model and tokenizer</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>PreTrainedTokenizerBase: Create a tokenizer object</p> required <code>serve_config</code> <code>EasyDeLServeEngineConfig</code> <p>EasyDeLServeEngineConfig: Create the generation function</p> required <code>partition_specs</code> <code>dict[str, PartitionSpec]</code> <p>dict[str, PartitionSpec]: Specify the sharding of the model parameters</p> required <p>Returns:</p> Type Description <code>LLMBaseReq</code> <p>An LLMBaseReq object</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>@staticmethod\ndef create_generation_functions_and_tokenizers(\n        model: EasyDeLFlaxPretrainedModel,\n        tokenizer: PreTrainedTokenizerBase,\n        serve_config: EasyDeLServeEngineConfig,\n        partition_specs: dict[str, PartitionSpec]\n) -&gt; LLMBaseReq:\n    \"\"\"The create_generation_functions_and_tokenizers function is used to create the functions that will be used for\n    generation. It also creates a tokenizer object that can be used to encode and decode text. The function takes in\n    a model, a tokenizer, an EasyDeLServeEngineConfig object (which contains all the parameters needed for generation), and\n    partition_specs which are specifications about how data should be partitioned across devices.\n\n    Args:\n        model: EasyDeLFlaxPretrainedModel: Create the model and\n            tokenizer\n        tokenizer: PreTrainedTokenizerBase: Create a tokenizer\n            object\n        serve_config: EasyDeLServeEngineConfig: Create the generation\n            function\n        partition_specs: dict[str, PartitionSpec]: Specify the\n            sharding of the model parameters\n\n    Returns:\n        An LLMBaseReq object\n    \"\"\"\n    if tokenizer.pad_token is None:\n        logging.info(\n            \"Tokenizer does not contain padding token setting padding token to eos token for open end generation\")\n        tokenizer.pad_token = tokenizer.eos_token\n\n    try:\n        tokenizer.padding_side = \"left\"\n        tokenizer.truncation_side = \"left\"\n        prefix_tokenizer = copy.deepcopy(tokenizer)\n        tokenizer.padding_side = \"right\"\n        tokenizer.truncation_side = \"right\"\n        tokenizer = copy.deepcopy(tokenizer)\n\n    except:\n        warnings.warn(\n            f\"The class Model of Tokenizer {type(tokenizer)} do not support deepcopy option \"\n        )\n        if serve_config.use_prefix_tokenizer:\n            tokenizer.padding_side = \"left\"\n            tokenizer.truncation_side = \"left\"\n        else:\n            tokenizer.padding_side = \"right\"\n            tokenizer.truncation_side = \"right\"\n        prefix_tokenizer = tokenizer\n\n    @functools.partial(\n        pjit,\n        in_shardings=(partition_specs, PartitionSpec(), PartitionSpec()),\n        out_shardings=(PartitionSpec())\n    )\n    def greedy_generate_function(\n            parameters,\n            input_ids,\n            attention_mask\n    ):\n        input_ids = with_sharding_constraint(input_ids, serve_config.generation_ps)\n        attention_mask = with_sharding_constraint(attention_mask, serve_config.generation_ps)\n        predict = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            params=parameters,\n            generation_config=GenerationConfig(\n                max_new_tokens=serve_config.max_compile_tokens,\n\n                eos_token_id=serve_config.eos_token_id or tokenizer.eos_token_id,\n                pad_token_id=serve_config.pad_token_id or tokenizer.pad_token_id,\n                bos_token_id=serve_config.bos_token_id or tokenizer.bos_token_id,\n\n                do_sample=False,\n                num_beams=1,\n            )\n        ).sequences[:, input_ids.shape[1]:]\n        return predict\n\n    @functools.partial(\n        pjit,\n        in_shardings=(partition_specs, PartitionSpec(), PartitionSpec()),\n        out_shardings=(PartitionSpec())\n    )\n    def non_greedy_generate_function(\n            parameters,\n            input_ids,\n            attention_mask\n    ):\n        input_ids = with_sharding_constraint(input_ids, serve_config.generation_ps)\n        attention_mask = with_sharding_constraint(attention_mask, serve_config.generation_ps)\n        predict = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            params=parameters,\n            generation_config=GenerationConfig(\n                max_new_tokens=serve_config.max_compile_tokens,\n\n                eos_token_id=serve_config.eos_token_id or tokenizer.eos_token_id,\n                pad_token_id=serve_config.pad_token_id or tokenizer.pad_token_id,\n                bos_token_id=serve_config.bos_token_id or tokenizer.bos_token_id,\n\n                temperature=serve_config.temperature,\n                repetition_penalty=serve_config.repetition_penalty,\n                do_sample=True,\n                num_beams=1,\n                top_p=serve_config.top_p,\n                top_k=serve_config.top_k,\n            )\n        ).sequences[:, input_ids.shape[1]:]\n        return predict\n\n    return LLMBaseReq(\n        greedy_generate_function=greedy_generate_function,\n        non_greedy_generate_function=non_greedy_generate_function,\n        tokenizer=tokenizer,\n        prefix_tokenizer=prefix_tokenizer\n    )\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.create_shard_and_gather_functions","title":"<code>create_shard_and_gather_functions(parameters, partition_rules, dtype='fp16')</code>  <code>staticmethod</code>","text":"<p>The create_shard_and_gather_functions function takes in a dictionary of parameters, a tuple of partition rules, and an optional dtype. It then matches the partition rules to the parameters and creates shard functions for each parameter. The shard functions are used to split up a parameter into shards (or partitions) that can be stored on different devices. The gather function is used to combine all the shards back together again.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>dict: Specify the parameters of the model</p> required <code>partition_rules</code> <code>Tuple[Tuple[str, PartitionSpec]]</code> <p>Tuple[Tuple[str,  PartitionSpec]]: Specify which parameters to partition</p> required <code>dtype</code> <code>Union[dtype, str]</code> <p>jax.numpy.dtype | str: Specify the data type of the parameters</p> <code>'fp16'</code> <p>Returns:</p> Type Description <p>A tuple of three elements:</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>@staticmethod\ndef create_shard_and_gather_functions(\n        parameters: dict,\n        partition_rules: Tuple[Tuple[str, PartitionSpec]],\n        dtype: Union[jax.numpy.dtype, str] = \"fp16\"\n):\n\n    \"\"\"The create_shard_and_gather_functions function takes in a dictionary of parameters,\n    a tuple of partition rules, and an optional dtype. It then matches the partition rules to the\n    parameters and creates shard functions for each parameter. The shard functions are used to\n    split up a parameter into shards (or partitions) that can be stored on different devices.\n    The gather function is used to combine all the shards back together again.\n\n    Args:\n        parameters: dict: Specify the parameters of the model\n        partition_rules: Tuple[Tuple[str,  PartitionSpec]]: Specify\n            which parameters to partition\n        dtype: jax.numpy.dtype | str: Specify the data type of the\n            parameters\n\n    Returns:\n        A tuple of three elements:\n    \"\"\"\n    partition_specs = match_partition_rules(partition_rules, parameters)\n    shard_fns, gather_fns = make_shard_and_gather_fns(\n        partition_specs=partition_specs,\n        dtype_specs=get_dtype(dtype)\n    )\n    return shard_fns, gather_fns, partition_specs\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.from_parameters","title":"<code>from_parameters(llm, params, tokenizer, serve_config, partition_rules, shard_parameters=True)</code>  <code>classmethod</code>","text":"<p>The from_parameters function is the main entry point for creating a model that can be served. It takes in a pretrained model, parameters, tokenizer and serve_config as input and returns an object of type EasyServe.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create a new instance of the class</p> required <code>llm</code> <code>EasyDeLFlaxPretrainedModel</code> <p>EasyDeLFlaxPretrainedModel: Pass the model to the class</p> required <code>params</code> <code>dict</code> <p>dict: Pass the parameters of the model</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>PreTrainedTokenizerBase: Create the tokenizer and prefix_tokenizer</p> required <code>serve_config</code> <code>EasyDeLServeEngineConfig</code> <p>EasyDeLServeEngineConfig: Configure the model for serving</p> required <code>partition_rules</code> <code>Tuple[Tuple[str, PartitionSpec]]</code> <p>Tuple[Tuple[str, PartitionSpec]]: Partition the parameters of the model</p> required <code>shard_parameters</code> <code>bool</code> <p>bool: Specify whether the parameters should be sharded or not</p> <code>True</code> <p>:param : Shard the parameters of the model</p> <p>Returns:</p> Type Description <p>A EasyServe object</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>@classmethod\ndef from_parameters(\n        cls,\n        llm: EasyDeLFlaxPretrainedModel,\n        params: dict,\n        tokenizer: PreTrainedTokenizerBase,\n        serve_config: EasyDeLServeEngineConfig,\n        partition_rules: Tuple[Tuple[str, PartitionSpec]],\n        shard_parameters: bool = True,\n):\n\n    \"\"\"The from_parameters function is the main entry point for creating a model that can be served.\n    It takes in a pretrained model, parameters, tokenizer and serve_config as input and returns an object of type\n    EasyServe.\n\n    Args:\n        cls: Create a new instance of the class\n        llm: EasyDeLFlaxPretrainedModel: Pass the model to the class\n        params: dict: Pass the parameters of the model\n        tokenizer: PreTrainedTokenizerBase: Create the tokenizer and\n            prefix_tokenizer\n        serve_config: EasyDeLServeEngineConfig: Configure the model for\n            serving\n        partition_rules: Tuple[Tuple[str, PartitionSpec]]: Partition\n            the parameters of the model\n        shard_parameters: bool: Specify whether the parameters\n            should be sharded or not\n    :param : Shard the parameters of the model\n\n    Returns:\n        A EasyServe object\n    \"\"\"\n    shard_fns, gather_fns, partition_specs = cls.create_shard_and_gather_functions(\n        parameters=params,\n        partition_rules=partition_rules,\n        dtype=serve_config.dtype\n    )\n    llm_base_req = cls.create_generation_functions_and_tokenizers(\n        model=llm,\n        tokenizer=tokenizer,\n        partition_specs=partition_specs,\n        serve_config=serve_config\n    )\n\n    if shard_parameters:\n        params = cls.shard_parameters(\n            params=params,\n            partition_rules=partition_rules,\n            serve_config=serve_config,\n            mesh=llm.config.jax_mesh()\n        )\n\n    return cls(\n        llm=llm,\n        serve_config=serve_config,\n        tokenizer=llm_base_req.tokenizer,\n        prefix_tokenizer=llm_base_req.prefix_tokenizer,\n        params=params,\n        greedy_generate_function=llm_base_req.greedy_generate_function,\n        non_greedy_generate_function=llm_base_req.non_greedy_generate_function,\n    )\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.sample","title":"<code>sample(string, *, greedy=False, max_new_tokens=None, **kwargs)</code>","text":"<p>The process function is the main function of a model. It takes in an input string and returns a list of strings that are generated from that input string. The process function can be called multiple times with different inputs, and each time it will return a new set of outputs based on those inputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <code>string</code> <code>str</code> <p>str: Pass the string that we want to generate</p> required <code>greedy</code> <code>bool</code> <p>bool: Determine whether to use the greedy or non- greedy version of the generate function</p> <code>False</code> <code>max_new_tokens</code> <code>int</code> <p>int: Set the number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <p>Pass any additional parameters to the process function</p> <code>{}</code> <p>Returns:</p> Type Description <p>A generator that yields the predicted text and the number of</p> <p>tokens generated</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>def sample(\n        self,\n        string: str,\n        *,\n        greedy: bool = False,\n        max_new_tokens: int = None,\n        **kwargs\n):\n    \"\"\"The process function is the main function of a model. It takes in an input string and returns a list of strings\n    that are generated from that input string. The process function can be called multiple times with different inputs,\n    and each time it will return a new set of outputs based on those inputs.\n\n    Args:\n        self: Access the class attributes\n        string: str: Pass the string that we want to generate\n        greedy: bool: Determine whether to use the greedy or non-\n            greedy version of the generate function\n        max_new_tokens: int: Set the number of tokens to generate\n        **kwargs: Pass any additional parameters to the process\n            function\n\n    Returns:\n        A generator that yields the predicted text and the number of\n        tokens generated\n    \"\"\"\n    with self.llm.config.jax_mesh():\n        fixed_pad = self.serve_config.max_sequence_length - self.serve_config.max_compile_tokens\n        tokens = self.prefix_tokenizer(\n            string,\n            max_length=fixed_pad,\n            padding=\"max_length\",\n            return_tensors=\"jax\"\n        ) if self.serve_config.use_prefix_tokenizer else self.tokenizer(\n            string,\n            return_tensors=\"jax\"\n        )\n\n        input_ids = tokens.input_ids\n        attention_mask = tokens.attention_mask\n        num_generated_tokens = 0\n\n        for _ in range(\n                (max_new_tokens or self.serve_config.max_new_tokens) // self.serve_config.max_compile_tokens):\n\n            predicted_token = self.get_generation_function(greedy=greedy)(\n                self.params,\n                input_ids,\n                attention_mask\n            )\n\n            num_generated_tokens += predicted_token.shape[-1]\n            plus_attn_mask = jnp.ones(\n                (len(attention_mask), self.serve_config.max_compile_tokens),\n                dtype=\"i4\"\n            )\n\n            input_ids = jnp.concatenate(\n                (input_ids, predicted_token), dtype=\"i4\",\n                axis=-1\n            )[:, -fixed_pad:]\n\n            attention_mask = jnp.concatenate(\n                (attention_mask, plus_attn_mask), dtype=\"i4\",\n                axis=-1\n            )[:, -fixed_pad:]\n\n            returns = (\n                self.tokenizer.decode(\n                    input_ids[0][-num_generated_tokens:],  # type:ignore\n                    skip_special_tokens=True\n                ),\n                num_generated_tokens\n            )\n\n            yield returns\n\n            if self.serve_config.use_mxn_break_point:\n                if self.serve_config.max_compile_tokens != predicted_token.shape[-1]:\n                    break\n            if (\n                    predicted_token[0][-1] == (self.serve_config.eos_token_id or self.tokenizer.eos_token_id)\n                    or\n                    predicted_token[0][-1] == (self.serve_config.eos_token_id or self.prefix_tokenizer.eos_token_id)\n            ):\n                break\n</code></pre>"},{"location":"generated-serve-serve/#src.python.easydel.serve.serve.EasyDeLServeEngine.shard_parameters","title":"<code>shard_parameters(mesh, params, partition_rules, serve_config)</code>  <code>staticmethod</code>","text":"<p>The shard_parameters function takes a set of parameters and partitions them according to the partition_rules.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Mesh</code> <p>Mesh: Create a mesh object that is used to shard the parameters</p> required <code>params</code> <code>Union[FrozenDict, dict]</code> <p>FrozenDict | dict: Pass in the parameters of the model</p> required <code>partition_rules</code> <code>Tuple[Tuple[str, PartitionSpec]]</code> <p>Tuple[Tuple[str, PartitionSpec]]: Specify the partitioning rules for each parameter</p> required <code>serve_config</code> <code>EasyDeLServeEngineConfig</code> <p>EasyDeLServeEngineConfig: Specify the dtype of the parameters</p> required <p>:param : Create a mesh of devices</p> <p>Returns:</p> Type Description <p>sharded parameters</p> Source code in <code>src/python/easydel/serve/serve.py</code> <pre><code>@staticmethod\ndef shard_parameters(\n        mesh: Mesh,\n        params: Union[FrozenDict, dict],\n        partition_rules: Tuple[Tuple[str, PartitionSpec]],\n        serve_config: EasyDeLServeEngineConfig,\n):\n\n    \"\"\"The shard_parameters function takes a set of parameters and partitions them according to the partition_rules.\n\n    Args:\n        mesh: Mesh: Create a mesh object that is used to shard the\n            parameters\n        params: FrozenDict | dict: Pass in the parameters of the\n            model\n        partition_rules: Tuple[Tuple[str, PartitionSpec]]: Specify\n            the partitioning rules for each parameter\n        serve_config: EasyDeLServeEngineConfig: Specify the dtype of the\n            parameters\n    :param : Create a mesh of devices\n\n    Returns:\n        sharded parameters\n    \"\"\"\n\n    partition_specs = match_partition_rules(params=params, rules=partition_rules)\n    shard_fns, _ = make_shard_and_gather_fns(partition_specs, get_dtype(serve_config.dtype))\n\n    with mesh:\n        params = jax.tree_map(\n            lambda func, param: func(param), shard_fns, params\n        )\n\n    return params\n</code></pre>"},{"location":"generated-smi-smi/","title":"smi.smi","text":""},{"location":"generated-smi-smi/#src.python.easydel.smi.smi.get_mem","title":"<code>get_mem(dir_prefix='/dev/shm' if sys.platform != 'win32' else '.')</code>","text":"<p>The get_mem function is a wrapper around the go tool pprof command. It takes in an optional argument, dir_prefix, which defaults to /dev/shm. The function then runs the go tool pprof command with arguments -tags and dir_prefix/memory.prof, and returns its stdout as a string.</p> <p>Parameters:</p> Name Type Description Default <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where</p> <code>'/dev/shm' if platform != 'win32' else '.'</code> <p>Returns:</p> Type Description <p>A string of the memory profile</p> Source code in <code>src/python/easydel/smi/smi.py</code> <pre><code>def get_mem(dir_prefix: str = \"/dev/shm\" if sys.platform != \"win32\" else \".\"):\n    \"\"\"The get_mem function is a wrapper around the go tool pprof command.\n    It takes in an optional argument, dir_prefix, which defaults to /dev/shm.\n    The function then runs the go tool pprof command with arguments -tags and dir_prefix/memory.prof,\n    and returns its stdout as a string.\n\n    Args:\n        dir_prefix: str: Specify the directory where\n\n    Returns:\n        A string of the memory profile\n    \"\"\"\n    return subprocess.run(\n        args=['go', 'tool', 'pprof', '-tags', f'{dir_prefix}/memory.prof'],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.DEVNULL,\n    ).stdout.decode('utf-8')\n</code></pre>"},{"location":"generated-smi-smi/#src.python.easydel.smi.smi.initialise_tracking","title":"<code>initialise_tracking(interval=0.5, dir_prefix='/dev/shm' if sys.platform != 'win32' else '.')</code>","text":"<p>The initialise_tracking function starts a daemon thread that periodically saves the current memory profile to disk.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>float: Specify the time interval between each memory profile</p> <code>0.5</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory profile will be saved</p> <code>'/dev/shm' if platform != 'win32' else '.'</code> <p>Returns:</p> Type Description <code>None</code> <p>Nothing, but it starts a thread that</p> Source code in <code>src/python/easydel/smi/smi.py</code> <pre><code>def initialise_tracking(interval: float = 0.5,\n                        dir_prefix: str = \"/dev/shm\" if sys.platform != \"win32\" else \".\") -&gt; None:\n    \"\"\"The initialise_tracking function starts a daemon thread that periodically saves the current memory profile to disk.\n\n    Args:\n        interval: float: Specify the time interval between each memory\n            profile\n        dir_prefix: str: Specify the directory where the memory profile\n            will be saved\n\n    Returns:\n        Nothing, but it starts a thread that\n    \"\"\"\n\n    def inner():\n        while True:\n            jax.profiler.save_device_memory_profile(f'{dir_prefix}/memory.prof.new')\n            os.rename(f'{dir_prefix}/memory.prof.new', f'{dir_prefix}/memory.prof')\n            time.sleep(interval)\n\n    thread = threading.Thread(target=inner, daemon=True)\n    thread.start()\n</code></pre>"},{"location":"generated-smi-smi/#src.python.easydel.smi.smi.run","title":"<code>run(note_book=None, interval=1, dir_prefix='/dev/shm', dpr=True)</code>","text":"<p>The run function is a simple wrapper around the go tool pprof command. It runs the command every interval seconds and prints out its output to stdout. If you are running this in a notebook, it will print to IPython's display instead of stdout.</p> <p>Parameters:</p> Name Type Description Default <code>note_book</code> <p>Determine whether the program is running in a notebook or not</p> <code>None</code> <code>interval</code> <code>float</code> <p>float: Specify the time interval between each refresh</p> <code>1</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory</p> <code>'/dev/shm'</code> <code>dpr</code> <p>Control whether the output is displayed in a notebook or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The output of the pprof command</p> Source code in <code>src/python/easydel/smi/smi.py</code> <pre><code>def run(note_book=None, interval: float = 1, dir_prefix: str = '/dev/shm', dpr=True):\n    \"\"\"The run function is a simple wrapper around the go tool pprof command.\n    It runs the command every interval seconds and prints out its output to stdout.\n    If you are running this in a notebook, it will print to IPython's display instead of stdout.\n\n    Args:\n        note_book: Determine whether the program is running in a\n            notebook or not\n        interval: float: Specify the time interval between each refresh\n        dir_prefix: str: Specify the directory where the memory\n        dpr: Control whether the output is displayed in a notebook or\n            not\n\n    Returns:\n        The output of the pprof command\n    \"\"\"\n    if note_book is None:\n        import os\n\n        def is_notebook():\n            \"\"\"Returns True if the code is being run in a notebook, False otherwise.\"\"\"\n            return os.environ.get(\"IPYTHON\") is not None\n\n        note_book = is_notebook()\n    std = curses.initscr() if not note_book else None\n    try:\n        while True:\n            if not note_book and dpr:\n                std.clear()\n            output = subprocess.run(\n                args=['go', 'tool', 'pprof', '-tags', f'{dir_prefix}/memory.prof'],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.DEVNULL,\n            ).stdout.decode('utf-8')\n            if not note_book and dpr:\n                std.addstr(output)\n                std.refresh()\n            if note_book and dpr:\n                IPython.display.clear_output(True)\n                print(output)\n\n            with open(f'{dir_prefix}/memory.json', 'w') as fin:\n                json.dump({\n                    'log': output\n                }, fin)\n            time.sleep(interval)\n    except KeyboardInterrupt:\n        curses.endwin()\n</code></pre>"},{"location":"generated-trainer-base_trainer/","title":"trainer.base_trainer","text":""},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer","title":"<code>BaseTrainer</code>","text":"Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>class BaseTrainer:\n    def __init__(\n            self,\n            arguments: TrainArguments,\n            dataset_train: Dataset,\n            dataset_eval: Dataset = None,\n            finetune: bool = True,\n            checkpoint_path: Union[str, os.PathLike] = None,\n            _do_init_fns: bool = True\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up all the variables that are needed for training, including:\n        - The timer to keep track of how long each epoch takes.\n        - The dataloaders for both training and evaluation (if provided).\n        - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,\n         it will be created from scratch using the arguments passed in by the user.\n         Note that this function also handles creating a mesh if one was not already specified in arguments\n         or loaded from a checkpoint file (see below).\n          This means that you can pass in either\n\n        Args:\n            self: Represent the instance of the class\n            arguments: TrainArguments: Pass the arguments to the trainer\n            dataset_train: Dataset: Pass the training dataset to the\n                trainer\n            dataset_eval: Dataset: Pass the validation dataset\n            finetune: bool: Load the model from a checkpoint\n            checkpoint_path: Union[str,os.PathLike] : Load the\n                checkpoint path\n            _do_init_fns: bool: Initialize the functions\n\n        Returns:\n            Nothing, it just initializes the class\n        \"\"\"\n        # Loggers\n        self.timer = getattr(self, \"timer\", None)\n        self.wandb_runtime: Optional[Union[\"Run\", \"RunDisabled\"]] = getattr(self, \"wandb_runtime\", None)\n\n        # Data\n        self.dataloader_train = getattr(self, \"dataloader_train\", None)\n        self.dataloader_eval = getattr(self, \"dataloader_eval\", None)\n        self.max_training_steps = getattr(self, \"max_training_steps\", None)\n        self.max_evaluation_steps = getattr(self, \"max_evaluation_steps\", None)\n        self.dataset_train = dataset_train\n        self.dataset_eval = dataset_eval\n\n        # Model Related\n        self.model = getattr(self, \"model\", None)\n        self.config = getattr(self, \"config\", None)\n        self.scheduler = getattr(self, \"scheduler\", None)\n        self.tx = getattr(self, \"tx\", None)\n        self.model_state = getattr(self, \"model_state\", None)\n\n        # LoRA Related\n        self.rapture = arguments.rapture\n        self.lora_parameters = getattr(self, \"lora_parameters\", None)\n        self.lora_model = getattr(self, \"lora_model\", None)\n        self.lora_tx = getattr(self, \"lora_tx\", None)\n        self.lora_opt_state = getattr(self, \"lora_opt_state\", None)\n        self.lora_apply_fn = getattr(self, \"lora_apply_fn\", None)\n\n        # PJit functions\n        self.create_sharded_state_from_params_function = getattr(\n            self,\n            \"create_sharded_state_from_params_function\",\n            None\n        )\n        self.sharded_train_step_function = getattr(self, \"sharded_train_step_function\", None)\n        self.sharded_eval_step_function = getattr(self, \"sharded_eval_step_function\", None)\n        self.initialize_state_function = getattr(self, \"initialize_state_function\", None)\n        self.mesh = getattr(self, \"mesh\", None)\n\n        # Checkpoint Managers\n        self.checkpoint_manager: fjformer.CheckpointManager | None = getattr(self, \"checkpoint_manager\", None)\n\n        # EasyState\n        self.state_shape = getattr(self, \"state_shape\", None)\n        self.state_partition_spec = getattr(self, \"state_partition_spec\", None)\n        self.sharded_state = getattr(self, \"sharded_state\", None)\n\n        # Rest\n\n        self.arguments = arguments\n        self.finetune = finetune\n        self.checkpoint_path = checkpoint_path\n        self.dtype = arguments.dtype\n        self.param_dtype = arguments.param_dtype\n        if self.arguments.track_memory:\n            if not self.arguments.performance_mode:\n                initialise_tracking()\n                self.arguments._stop_capturing_memory = False\n                self._start_capturing_memory().start()\n        if finetune:\n            if checkpoint_path is None:\n                prefix_print(\n                    \"Warning\",\n                    \"In case of using `finetune = True` and Passing `checkpoint_path = None`\"\n                    \" you should pass parameters in train function\"\n                )\n        if _do_init_fns:\n            self.initialize_trainer_utils()\n        else:\n            prefix_print(\n                \"Warning\",\n                \"you have set `_do_init_fns = False` so function will not me initialized you have \"\n                f\"to do in manually (simply with `trainer.initialize_trainer_utils()` )\"\n            )\n\n    def __str__(self):\n        string = f\"{self.__class__.__name__}(\"\n        for key, value in self.__dict__.items():\n            try:\n                string += value.__str__().replace(\"\\n\", \"\\n\\t\")\n            except TypeError:\n                ...\n        string += \")\"\n        return string\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def finish():\n        \"\"\"The finish function is called when the experiment ends.\n        It can be used to save data, upload files, or do any other cleanup tasks.\n\n        Returns:\n            A dictionary of the run's metadata\n        \"\"\"\n        if wandb is not None:\n            wandb.finish()\n\n    def _start_capturing_memory(self, dir_prefix: str = \"/dev/shm\" if sys.platform != \"win32\" else \".\"):\n        def _start():\n            while True:\n                information_queries = {}\n                for key in [\"Used\", \"Usage Percent\"]:\n                    for device, info in get_capacity_matrix(dir_prefix=dir_prefix).items():\n                        information_queries[f\"accelerators/{device.replace('_', ' ')} ({key})\"] = float(\n                            info[key].replace(\"%\", \"\").replace(\"GB\", \"\")\n                        )\n                self.arguments._captured_memory = information_queries\n                if self.arguments.stop_capturing_memory:\n                    break\n                time.sleep(1.5)\n\n        return threading.Thread(target=_start)\n\n    def initialize_trainer_utils(self):\n        \"\"\"The initialize_trainer_utils function is responsible for initializing the following:\n            - wandb_runtime (if you use_wandb is True)\n            - timer object (for logging time taken by various functions)\n            - dataloader objects for training and evaluation data, along with max steps per epoch.\n              The configure_dataloader function accomplishes this task.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A tuple of functions\n        \"\"\"\n        self.wandb_runtime = None\n        if self.arguments.use_wandb:\n            self.wandb_runtime = self.arguments.get_wandb_init()\n        self.timer = Timers(\n            use_wandb=False,\n            tensorboard_writer=self.arguments.get_board()\n        )\n\n        self.timer(\"configure dataloaders\").start()\n        dataset_configurations = self.configure_dataloader()\n        self.dataloader_train = dataset_configurations.dataloader_train\n        self.max_training_steps = dataset_configurations.max_training_steps\n        self.dataloader_eval = dataset_configurations.dataloader_eval\n        self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n        self.timer(\"configure dataloaders\").stop()\n\n        self.timer.log([\"configure dataloaders\"])\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n        model_configurations = self.configure_model()\n        model = model_configurations.model\n        tx = model_configurations.tx\n        scheduler = model_configurations.scheduler\n        config = model_configurations.config\n        self.model = model\n        self.tx = tx\n        self.scheduler = scheduler\n        self.config = config\n        if self.rapture is not None:\n            lora_modules = self.rapture.apply_lora(\n                module=model,\n                parameters=self.arguments.rapture_config.parameters,\n                tx=tx,\n            )\n            self.lora_parameters = lora_modules.lora_parameters\n            self.lora_apply_fn = lora_modules.lora_module.__call__\n            self.lora_opt_state = lora_modules.lora_opt_state\n            self.lora_model = lora_modules.lora_module\n            self.lora_tx = lora_modules.lora_tx\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n        self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n        self.timer(\"configure functions and sharding them\").start()\n        function_configurations = self.configure_functions()\n        self.create_sharded_state_from_params_function = \\\n            function_configurations.create_sharded_state_from_params_function\n        self.sharded_train_step_function = function_configurations.sharded_train_step_function\n        self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n        self.mesh = function_configurations.mesh\n        self.checkpoint_manager = function_configurations.checkpoint_manager\n        self.initialize_state_function = function_configurations.initialize_state_function\n        self.timer(\"configure functions and sharding them\").stop()\n        self.timer.log([\"configure functions and sharding them\"])\n\n    @abstractmethod\n    def create_collate_function(\n            self,\n            max_sequence_length: int,\n            truncation_mode: Literal[\"keep_end\", \"keep_start\"]\n    ) -&gt; Callable:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n        \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n        them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n        on a batch of data, including:\n\n        Args:\n            self: Access the class attributes\n\n        Returns:\n            A TrainerConfigureFunctionFuncOutput object\n        \"\"\"\n        raise NotImplementedError\n\n    def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n        \"\"\"The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n        Args:\n            self: Refer to the class instance itself\n\n        Returns:\n            A TrainerConfigureDataloaderFuncOutput object\n        \"\"\"\n\n        def create_tf_dataset(dataset: Dataset, is_train: bool) -&gt; Iterator[ndarray[Any, Any]]:\n            return (\n                dataset.to_tf_dataset(\n                    collate_fn=self.create_collate_function(\n                        max_sequence_length=self.arguments.max_sequence_length,\n                        truncation_mode=self.arguments.truncation_mode\n                    ),\n                    batch_size=self.arguments.total_batch_size,\n                    drop_remainder=True,\n                    shuffle=not is_train,\n                    num_workers=self.arguments.dataloader_num_workers\n                )\n                .repeat(self.arguments.num_train_epochs if is_train else 1)\n                .prefetch(tf.data.experimental.AUTOTUNE)\n                .as_numpy_iterator()\n            )\n\n        def create_tf_dataset_from_iterable(dataset: IterableDataset, is_train: bool) -&gt; Iterator[ndarray[Any, Any]]:\n            return (\n                tf.data.Dataset.from_generator(\n                    lambda: dataset,\n                    output_signature={\n                        col: tf.TensorSpec(shape=(self.arguments.max_sequence_length,), dtype=tf.int32)\n                        for col in next(iter(dataset)).keys()\n                    }\n                )\n                .repeat(self.arguments.num_train_epochs if is_train else 1)\n                .batch(self.arguments.total_batch_size, drop_remainder=False)\n                .prefetch(tf.data.experimental.AUTOTUNE)\n                .as_numpy_iterator()\n            )\n\n        def calculate_steps(dataset: Union[Dataset, IterableDataset], is_train: bool):\n            \"\"\"Return total number of steps to train or evaluate on.\"\"\"\n            if hasattr(dataset, \"__len__\"):\n                num_steps = len(dataset) * (self.arguments.num_train_epochs if is_train else 1)\n                max_steps = self.arguments.max_training_steps if is_train else self.arguments.max_evaluation_steps\n                return min(num_steps, max_steps) if max_steps else num_steps\n            else:\n                num_steps = self.arguments.max_training_steps if is_train else self.arguments.max_evaluation_steps\n                if not num_steps:\n                    raise ValueError(\n                        f\"Specify the number of {'training' if is_train else 'evaluation'} steps for a generator/streaming dataset.\")\n                return num_steps\n\n        def to_tf_dataloader(dataset: Union[Dataset, IterableDataset], is_train: bool):\n            if hasattr(dataset, \"__len__\"):\n                return create_tf_dataset(dataset, is_train)\n            else:\n                return create_tf_dataset_from_iterable(dataset, is_train)\n\n        max_training_steps = calculate_steps(self.dataset_train, is_train=True)\n        dataloader_train = to_tf_dataloader(self.dataset_train, is_train=True)\n\n        if self.dataset_eval is not None and self.arguments.do_eval:\n            max_evaluation_steps = calculate_steps(self.dataset_eval, is_train=False)\n            dataloader_eval = to_tf_dataloader(self.dataset_eval, is_train=False)\n        else:\n            dataloader_eval, max_evaluation_steps = None, 0\n\n        return TrainerConfigureDataloaderFuncOutput(\n            dataloader_train=dataloader_train,\n            max_training_steps=max_training_steps,\n            dataloader_eval=dataloader_eval,\n            max_evaluation_steps=max_evaluation_steps\n        )\n\n    def configure_model(self) -&gt; TrainerConfigureModelFuncOutput:\n        \"\"\"The configure_model function is responsible for creating the model, optimizer and scheduler.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A model, optimizer, scheduler and config  in\n            TrainerConfigureModelFuncOutput Object\n        \"\"\"\n        extra_configs = {} if self.arguments.extra_configs is None else self.arguments.extra_configs\n        if self.arguments.model_class is not None:\n\n            if not hasattr(self.arguments.configs_to_initialize_model_class[\"config\"], \"get_partition_rules\"):\n                assert self.arguments.custom_rule is not None, (\n                    \"if you are using custom model to init you must\"\n                    \" pass custom_rule for partition rules \"\n                )\n\n            self.arguments.configs_to_initialize_model_class[\"config\"].axis_dims = self.arguments.sharding_array\n\n            model = self.arguments.model_class(\n                **self.arguments.configs_to_initialize_model_class,\n                _do_init=False\n            )\n\n            config = self.arguments.configs_to_initialize_model_class[\"config\"]\n\n        else:\n            extra_configs[\"gradient_checkpointing\"] = self.arguments.gradient_checkpointing\n\n            model = AutoEasyDeLModelForCausalLM.from_pretrained(\n                self.arguments.model_huggingface_repo_id,\n                dtype=self.arguments.dtype,\n                param_dtype=self.arguments.param_dtype,\n                _do_init=False\n            )\n            if hasattr(model, \"config\"):\n                for k, v in extra_configs.items():\n                    setattr(model.config, k, v)\n                config = model.config\n            else:\n                config = None\n                warnings.warn(\n                    \"Config is being set to None due to not detecting Model Configuration from taken Model \"\n                    \"this will cause errors later.\"\n                )\n        tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_training_steps)\n        return TrainerConfigureModelFuncOutput(\n            model=model,\n            tx=tx,\n            scheduler=scheduler,\n            config=config\n        )\n\n    def _save_state(\n            self,\n            state: \"EasyDeLState\",  # type: ignore\n            gather_fns: Optional[Any | Mapping[str, Callable] | dict[Callable]],\n            milestone: bool = False,\n            save_dir: Optional[str] = None,\n    ) -&gt; str:\n        step = int(\n            jax.device_get(\n                state.step\n            )\n        ) + self.arguments.step_start_point if self.arguments.step_start_point is not None else int(\n            jax.device_get(\n                state.step\n            )\n        )\n        checkpoint_name = f\"{self.arguments.model_name}-S{step}\"\n        filename = f\"{checkpoint_name}_{step}\" if milestone else f\"{checkpoint_name}\"\n        filename += \".easy\"\n        termcolor.cprint(f\"Saving Model {filename}.\", color=\"cyan\", force_color=True)\n\n        checkpoint_dir = os.path.join(self.arguments.save_dir,\n                                      self.arguments.model_name) if save_dir is None else save_dir\n        state.save_state(\n            filename=filename,\n            checkpoint_dir=checkpoint_dir,\n            gather_fns=gather_fns,\n            float_dtype=self.dtype,\n            verbose=self.arguments.verbose,\n            save_optimizer=self.arguments.save_optimizer_state,\n        )\n        open(os.path.join(checkpoint_dir, \"README.md\"), \"w\").write(self._get_information())\n        return filename\n\n    @abc.abstractmethod\n    def train(self):\n        \"\"\"abstract of Train Function to train model\"\"\"\n\n    @abc.abstractmethod\n    def eval(self, state):\n        \"\"\"abstract of Eval Function to evaluate model\"\"\"\n\n    def _get_information(self):\n        partition_rules = pprint.pformat(\n\n            self.arguments.custom_rule if self.arguments.custom_rule is not None else\n            self.arguments.model_class.config_class.get_partition_rules(self.arguments.fully_sharded_data_parallel)\n\n        )\n        makrdown = f\"\"\"\n---\ntags:\n- EasyDeL\n- {self.arguments.model_class.config_class.model_type}\n---\n# {self.arguments.model_name}\n\n## Trained With [EasyDeL](https://github.com/erfanzar/EasyDeL)\n\nEasyDeL is an open-source framework designed to enhance and streamline the training process of machine learning\nmodels. With a primary focus on Jax, EasyDeL aims to provide convenient and effective solutions for \ntraining Flax/Jax models on TPU/GPU for both serving and training purposes.\n\n## Using Example\n\n### Using From EasyDeLState (_*.easy_ files)\n\n```python\nimport os\n\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".99\"\n\nfrom src.python.easydel import EasyDeLState, AutoShardAndGatherFunctions\nfrom jax import numpy as jnp, lax\n\nshard_fns, gather_fns = AutoShardAndGatherFunctions.from_pretrained(\n    \"REPO_ID\", # Pytorch State should be saved to in order to find shard gather fns with no effort, otherwise read docs.\n    backend=\"gpu\",\n    depth_target=[\"params\", \"params\"],\n    flatten=False\n)\n\nstate = EasyDeLState.load_state(\n    \"REPO_ID/{self.arguments.model_name}.easy\",\n    dtype=jnp.float16,\n    param_dtype=jnp.float16,\n    precision=lax.Precision(\"fastest\"),\n    verbose=True,\n    state_shard_fns=shard_fns\n)\n# State file Ready to use ...\n```\n\n### Using From AutoEasyDeLModelForCausalLM (_from pytorch_)\n\n```python\nimport os\n\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".99\"\n\nfrom src.python.easydel import AutoEasyDeLModelForCausalLM\nfrom jax import numpy as jnp, lax\n\n\nmodel, params = AutoEasyDeLModelForCausalLM.from_pretrained(\n    \"REPO_ID/{self.arguments.model_name}\",\n    dtype=jnp.float16,\n    param_dtype=jnp.float16,\n    precision=lax.Precision(\"fastest\"),\n    auto_shard_params=True\n)\n# Model and Parameters Ready to use ...\n```\n\n\n## Training Detail\n\n- Model Architecture : {self.arguments.model_class.config_class.model_type}\n- Platform : {jax.devices()[0].platform.upper()}\n- Number of Devices : {len(jax.devices())}\n- Learning Rate Start : {self.arguments.learning_rate}\n- Learning Rate End : {self.arguments.learning_rate_end}\n- Optimizer : {self.arguments.optimizer}\n- Scheduler : {self.arguments.scheduler}\n- Warmup Steps : {self.arguments.warmup_steps}\n- Weight Decay : {self.arguments.weight_decay}\n- Z Loss : {self.arguments.z_loss}\n- Epoch : {self.arguments.num_train_epochs}\n- Batch size : {self.arguments.total_batch_size}\n- Sequence Length : {self.arguments.max_sequence_length}\n- EasyDeL init InputShape : {self.arguments.init_input_shape}\n- Dtype : {self.arguments.dtype}\n- Params Dtype : {self.arguments.param_dtype}\n- Gradient checkpointing : {self.arguments.gradient_checkpointing}\n- Fully Sharded Data Parallel : {self.arguments.fully_sharded_data_parallel}\n- Force batch GradientAccumulation : {self.arguments.force_batch_and_gradient_accumulation_steps_calculation}\n- Gradient Accumulation Steps : {self.arguments.gradient_accumulation_steps}\n- Max Training Steps : {self.arguments.max_training_steps}\n- Max Evaluation Steps : {self.arguments.max_evaluation_steps}\n- Training Time : {self.arguments.training_time}\n\n#### Sharding Partition Rules\n```python\npartition_rules = {partition_rules}\n```\n        \"\"\"\n        return makrdown\n\n    def save_pretrained(\n            self,\n            state: \"EasyDeLState\",  # type: ignore\n            save_dir: Optional[str] = None,\n            gather_fns: Optional[Any | Mapping[str, Callable] | dict[Callable]] = None,\n            to_torch: bool = False,\n            base_hf_auto_class=AutoModelForCausalLM,\n            easystate_to_huggingface_model_kwargs: Optional[dict] = None,\n            add_params_field_to_torch_convertation: bool = False,\n            torch_save_pretrained_kwargs: Optional[dict] = None\n    ):\n        if torch_save_pretrained_kwargs is None:\n            torch_save_pretrained_kwargs = {}\n        if easystate_to_huggingface_model_kwargs is None:\n            easystate_to_huggingface_model_kwargs = {}\n        if save_dir is None:\n            save_dir = os.path.join(self.arguments.save_dir, self.arguments.model_name)\n        if to_torch:\n            from ..transform.easydel_transform import easystate_to_huggingface_model\n\n            if easystate_to_huggingface_model_kwargs is None:\n                easystate_to_huggingface_model_kwargs = {}\n\n            model_config = state.module_config\n            if model_config is None:\n                model_config = state.module.config_class\n            model_type = model_config.model_type\n\n            model_class = base_hf_auto_class._model_mapping[type(model_config)]  # noqa\n\n            unsafe_dict = state.unsafe_dict(model_config.__dict__)\n            hf_model_config = AutoConfig.for_model(model_type=model_type)\n            blocked_statics = [\"torch_dtype\"]\n            kss = list(hf_model_config.__dict__.keys())\n            for k, v in unsafe_dict.items():\n                if not k.startswith(\"_\") and k in kss and k not in blocked_statics:\n                    if isinstance(v, str):\n                        if v.isnumeric():\n                            v = float(v)\n                            if v.is_integer():\n                                v = int(v)\n\n                    setattr(hf_model_config, k, v)\n\n            hf_model = easystate_to_huggingface_model(\n                state=state,\n                base_huggingface_module=model_class,\n                config=hf_model_config,\n                **easystate_to_huggingface_model_kwargs\n            )\n\n            open(os.path.join(save_dir, \"README.md\"), \"w\").write(self._get_information())\n            hf_model.save_pretrained(save_dir, **torch_save_pretrained_kwargs)\n            return hf_model\n        else:\n            self._save_state(\n                state=state,\n                gather_fns=gather_fns,\n                save_dir=save_dir\n            )\n            return state\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.__init__","title":"<code>__init__(arguments, dataset_train, dataset_eval=None, finetune=True, checkpoint_path=None, _do_init_fns=True)</code>","text":"<p>The init function is called when the class is instantiated. It sets up all the variables that are needed for training, including: - The timer to keep track of how long each epoch takes. - The dataloaders for both training and evaluation (if provided). - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,  it will be created from scratch using the arguments passed in by the user.  Note that this function also handles creating a mesh if one was not already specified in arguments  or loaded from a checkpoint file (see below).   This means that you can pass in either</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>arguments</code> <code>TrainArguments</code> <p>TrainArguments: Pass the arguments to the trainer</p> required <code>dataset_train</code> <code>Dataset</code> <p>Dataset: Pass the training dataset to the trainer</p> required <code>dataset_eval</code> <code>Dataset</code> <p>Dataset: Pass the validation dataset</p> <code>None</code> <code>finetune</code> <code>bool</code> <p>bool: Load the model from a checkpoint</p> <code>True</code> <code>checkpoint_path</code> <code>Union[str, PathLike]</code> <p>Union[str,os.PathLike] : Load the checkpoint path</p> <code>None</code> <code>_do_init_fns</code> <code>bool</code> <p>bool: Initialize the functions</p> <code>True</code> <p>Returns:</p> Type Description <p>Nothing, it just initializes the class</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>def __init__(\n        self,\n        arguments: TrainArguments,\n        dataset_train: Dataset,\n        dataset_eval: Dataset = None,\n        finetune: bool = True,\n        checkpoint_path: Union[str, os.PathLike] = None,\n        _do_init_fns: bool = True\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up all the variables that are needed for training, including:\n    - The timer to keep track of how long each epoch takes.\n    - The dataloaders for both training and evaluation (if provided).\n    - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,\n     it will be created from scratch using the arguments passed in by the user.\n     Note that this function also handles creating a mesh if one was not already specified in arguments\n     or loaded from a checkpoint file (see below).\n      This means that you can pass in either\n\n    Args:\n        self: Represent the instance of the class\n        arguments: TrainArguments: Pass the arguments to the trainer\n        dataset_train: Dataset: Pass the training dataset to the\n            trainer\n        dataset_eval: Dataset: Pass the validation dataset\n        finetune: bool: Load the model from a checkpoint\n        checkpoint_path: Union[str,os.PathLike] : Load the\n            checkpoint path\n        _do_init_fns: bool: Initialize the functions\n\n    Returns:\n        Nothing, it just initializes the class\n    \"\"\"\n    # Loggers\n    self.timer = getattr(self, \"timer\", None)\n    self.wandb_runtime: Optional[Union[\"Run\", \"RunDisabled\"]] = getattr(self, \"wandb_runtime\", None)\n\n    # Data\n    self.dataloader_train = getattr(self, \"dataloader_train\", None)\n    self.dataloader_eval = getattr(self, \"dataloader_eval\", None)\n    self.max_training_steps = getattr(self, \"max_training_steps\", None)\n    self.max_evaluation_steps = getattr(self, \"max_evaluation_steps\", None)\n    self.dataset_train = dataset_train\n    self.dataset_eval = dataset_eval\n\n    # Model Related\n    self.model = getattr(self, \"model\", None)\n    self.config = getattr(self, \"config\", None)\n    self.scheduler = getattr(self, \"scheduler\", None)\n    self.tx = getattr(self, \"tx\", None)\n    self.model_state = getattr(self, \"model_state\", None)\n\n    # LoRA Related\n    self.rapture = arguments.rapture\n    self.lora_parameters = getattr(self, \"lora_parameters\", None)\n    self.lora_model = getattr(self, \"lora_model\", None)\n    self.lora_tx = getattr(self, \"lora_tx\", None)\n    self.lora_opt_state = getattr(self, \"lora_opt_state\", None)\n    self.lora_apply_fn = getattr(self, \"lora_apply_fn\", None)\n\n    # PJit functions\n    self.create_sharded_state_from_params_function = getattr(\n        self,\n        \"create_sharded_state_from_params_function\",\n        None\n    )\n    self.sharded_train_step_function = getattr(self, \"sharded_train_step_function\", None)\n    self.sharded_eval_step_function = getattr(self, \"sharded_eval_step_function\", None)\n    self.initialize_state_function = getattr(self, \"initialize_state_function\", None)\n    self.mesh = getattr(self, \"mesh\", None)\n\n    # Checkpoint Managers\n    self.checkpoint_manager: fjformer.CheckpointManager | None = getattr(self, \"checkpoint_manager\", None)\n\n    # EasyState\n    self.state_shape = getattr(self, \"state_shape\", None)\n    self.state_partition_spec = getattr(self, \"state_partition_spec\", None)\n    self.sharded_state = getattr(self, \"sharded_state\", None)\n\n    # Rest\n\n    self.arguments = arguments\n    self.finetune = finetune\n    self.checkpoint_path = checkpoint_path\n    self.dtype = arguments.dtype\n    self.param_dtype = arguments.param_dtype\n    if self.arguments.track_memory:\n        if not self.arguments.performance_mode:\n            initialise_tracking()\n            self.arguments._stop_capturing_memory = False\n            self._start_capturing_memory().start()\n    if finetune:\n        if checkpoint_path is None:\n            prefix_print(\n                \"Warning\",\n                \"In case of using `finetune = True` and Passing `checkpoint_path = None`\"\n                \" you should pass parameters in train function\"\n            )\n    if _do_init_fns:\n        self.initialize_trainer_utils()\n    else:\n        prefix_print(\n            \"Warning\",\n            \"you have set `_do_init_fns = False` so function will not me initialized you have \"\n            f\"to do in manually (simply with `trainer.initialize_trainer_utils()` )\"\n        )\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.configure_dataloader","title":"<code>configure_dataloader()</code>","text":"<p>The configure_dataloader function is used to configure the dataloader for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <p>Returns:</p> Type Description <code>TrainerConfigureDataloaderFuncOutput</code> <p>A TrainerConfigureDataloaderFuncOutput object</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n    \"\"\"The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n    Args:\n        self: Refer to the class instance itself\n\n    Returns:\n        A TrainerConfigureDataloaderFuncOutput object\n    \"\"\"\n\n    def create_tf_dataset(dataset: Dataset, is_train: bool) -&gt; Iterator[ndarray[Any, Any]]:\n        return (\n            dataset.to_tf_dataset(\n                collate_fn=self.create_collate_function(\n                    max_sequence_length=self.arguments.max_sequence_length,\n                    truncation_mode=self.arguments.truncation_mode\n                ),\n                batch_size=self.arguments.total_batch_size,\n                drop_remainder=True,\n                shuffle=not is_train,\n                num_workers=self.arguments.dataloader_num_workers\n            )\n            .repeat(self.arguments.num_train_epochs if is_train else 1)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n            .as_numpy_iterator()\n        )\n\n    def create_tf_dataset_from_iterable(dataset: IterableDataset, is_train: bool) -&gt; Iterator[ndarray[Any, Any]]:\n        return (\n            tf.data.Dataset.from_generator(\n                lambda: dataset,\n                output_signature={\n                    col: tf.TensorSpec(shape=(self.arguments.max_sequence_length,), dtype=tf.int32)\n                    for col in next(iter(dataset)).keys()\n                }\n            )\n            .repeat(self.arguments.num_train_epochs if is_train else 1)\n            .batch(self.arguments.total_batch_size, drop_remainder=False)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n            .as_numpy_iterator()\n        )\n\n    def calculate_steps(dataset: Union[Dataset, IterableDataset], is_train: bool):\n        \"\"\"Return total number of steps to train or evaluate on.\"\"\"\n        if hasattr(dataset, \"__len__\"):\n            num_steps = len(dataset) * (self.arguments.num_train_epochs if is_train else 1)\n            max_steps = self.arguments.max_training_steps if is_train else self.arguments.max_evaluation_steps\n            return min(num_steps, max_steps) if max_steps else num_steps\n        else:\n            num_steps = self.arguments.max_training_steps if is_train else self.arguments.max_evaluation_steps\n            if not num_steps:\n                raise ValueError(\n                    f\"Specify the number of {'training' if is_train else 'evaluation'} steps for a generator/streaming dataset.\")\n            return num_steps\n\n    def to_tf_dataloader(dataset: Union[Dataset, IterableDataset], is_train: bool):\n        if hasattr(dataset, \"__len__\"):\n            return create_tf_dataset(dataset, is_train)\n        else:\n            return create_tf_dataset_from_iterable(dataset, is_train)\n\n    max_training_steps = calculate_steps(self.dataset_train, is_train=True)\n    dataloader_train = to_tf_dataloader(self.dataset_train, is_train=True)\n\n    if self.dataset_eval is not None and self.arguments.do_eval:\n        max_evaluation_steps = calculate_steps(self.dataset_eval, is_train=False)\n        dataloader_eval = to_tf_dataloader(self.dataset_eval, is_train=False)\n    else:\n        dataloader_eval, max_evaluation_steps = None, 0\n\n    return TrainerConfigureDataloaderFuncOutput(\n        dataloader_train=dataloader_train,\n        max_training_steps=max_training_steps,\n        dataloader_eval=dataloader_eval,\n        max_evaluation_steps=max_evaluation_steps\n    )\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.configure_functions","title":"<code>configure_functions()</code>  <code>abstractmethod</code>","text":"<p>The configure_functions function is responsible for configuring the functions that will be used in training. It does this by first defining a function called function_configurations, which initializes the model parameters and returns them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate on a batch of data, including:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <p>Returns:</p> Type Description <code>TrainerConfigureFunctionFuncOutput</code> <p>A TrainerConfigureFunctionFuncOutput object</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>@abc.abstractmethod\ndef configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n    \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n    It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n    them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n    on a batch of data, including:\n\n    Args:\n        self: Access the class attributes\n\n    Returns:\n        A TrainerConfigureFunctionFuncOutput object\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.configure_model","title":"<code>configure_model()</code>","text":"<p>The configure_model function is responsible for creating the model, optimizer and scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>TrainerConfigureModelFuncOutput</code> <p>A model, optimizer, scheduler and config  in</p> <code>TrainerConfigureModelFuncOutput</code> <p>TrainerConfigureModelFuncOutput Object</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>def configure_model(self) -&gt; TrainerConfigureModelFuncOutput:\n    \"\"\"The configure_model function is responsible for creating the model, optimizer and scheduler.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A model, optimizer, scheduler and config  in\n        TrainerConfigureModelFuncOutput Object\n    \"\"\"\n    extra_configs = {} if self.arguments.extra_configs is None else self.arguments.extra_configs\n    if self.arguments.model_class is not None:\n\n        if not hasattr(self.arguments.configs_to_initialize_model_class[\"config\"], \"get_partition_rules\"):\n            assert self.arguments.custom_rule is not None, (\n                \"if you are using custom model to init you must\"\n                \" pass custom_rule for partition rules \"\n            )\n\n        self.arguments.configs_to_initialize_model_class[\"config\"].axis_dims = self.arguments.sharding_array\n\n        model = self.arguments.model_class(\n            **self.arguments.configs_to_initialize_model_class,\n            _do_init=False\n        )\n\n        config = self.arguments.configs_to_initialize_model_class[\"config\"]\n\n    else:\n        extra_configs[\"gradient_checkpointing\"] = self.arguments.gradient_checkpointing\n\n        model = AutoEasyDeLModelForCausalLM.from_pretrained(\n            self.arguments.model_huggingface_repo_id,\n            dtype=self.arguments.dtype,\n            param_dtype=self.arguments.param_dtype,\n            _do_init=False\n        )\n        if hasattr(model, \"config\"):\n            for k, v in extra_configs.items():\n                setattr(model.config, k, v)\n            config = model.config\n        else:\n            config = None\n            warnings.warn(\n                \"Config is being set to None due to not detecting Model Configuration from taken Model \"\n                \"this will cause errors later.\"\n            )\n    tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_training_steps)\n    return TrainerConfigureModelFuncOutput(\n        model=model,\n        tx=tx,\n        scheduler=scheduler,\n        config=config\n    )\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.eval","title":"<code>eval(state)</code>  <code>abstractmethod</code>","text":"<p>abstract of Eval Function to evaluate model</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>@abc.abstractmethod\ndef eval(self, state):\n    \"\"\"abstract of Eval Function to evaluate model\"\"\"\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.finish","title":"<code>finish()</code>  <code>staticmethod</code>","text":"<p>The finish function is called when the experiment ends. It can be used to save data, upload files, or do any other cleanup tasks.</p> <p>Returns:</p> Type Description <p>A dictionary of the run's metadata</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>@staticmethod\ndef finish():\n    \"\"\"The finish function is called when the experiment ends.\n    It can be used to save data, upload files, or do any other cleanup tasks.\n\n    Returns:\n        A dictionary of the run's metadata\n    \"\"\"\n    if wandb is not None:\n        wandb.finish()\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.initialize_trainer_utils","title":"<code>initialize_trainer_utils()</code>","text":"The initialize_trainer_utils function is responsible for initializing the following <ul> <li>wandb_runtime (if you use_wandb is True)</li> <li>timer object (for logging time taken by various functions)</li> <li>dataloader objects for training and evaluation data, along with max steps per epoch.   The configure_dataloader function accomplishes this task.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A tuple of functions</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>def initialize_trainer_utils(self):\n    \"\"\"The initialize_trainer_utils function is responsible for initializing the following:\n        - wandb_runtime (if you use_wandb is True)\n        - timer object (for logging time taken by various functions)\n        - dataloader objects for training and evaluation data, along with max steps per epoch.\n          The configure_dataloader function accomplishes this task.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A tuple of functions\n    \"\"\"\n    self.wandb_runtime = None\n    if self.arguments.use_wandb:\n        self.wandb_runtime = self.arguments.get_wandb_init()\n    self.timer = Timers(\n        use_wandb=False,\n        tensorboard_writer=self.arguments.get_board()\n    )\n\n    self.timer(\"configure dataloaders\").start()\n    dataset_configurations = self.configure_dataloader()\n    self.dataloader_train = dataset_configurations.dataloader_train\n    self.max_training_steps = dataset_configurations.max_training_steps\n    self.dataloader_eval = dataset_configurations.dataloader_eval\n    self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n    self.timer(\"configure dataloaders\").stop()\n\n    self.timer.log([\"configure dataloaders\"])\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n    model_configurations = self.configure_model()\n    model = model_configurations.model\n    tx = model_configurations.tx\n    scheduler = model_configurations.scheduler\n    config = model_configurations.config\n    self.model = model\n    self.tx = tx\n    self.scheduler = scheduler\n    self.config = config\n    if self.rapture is not None:\n        lora_modules = self.rapture.apply_lora(\n            module=model,\n            parameters=self.arguments.rapture_config.parameters,\n            tx=tx,\n        )\n        self.lora_parameters = lora_modules.lora_parameters\n        self.lora_apply_fn = lora_modules.lora_module.__call__\n        self.lora_opt_state = lora_modules.lora_opt_state\n        self.lora_model = lora_modules.lora_module\n        self.lora_tx = lora_modules.lora_tx\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n    self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n    self.timer(\"configure functions and sharding them\").start()\n    function_configurations = self.configure_functions()\n    self.create_sharded_state_from_params_function = \\\n        function_configurations.create_sharded_state_from_params_function\n    self.sharded_train_step_function = function_configurations.sharded_train_step_function\n    self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n    self.mesh = function_configurations.mesh\n    self.checkpoint_manager = function_configurations.checkpoint_manager\n    self.initialize_state_function = function_configurations.initialize_state_function\n    self.timer(\"configure functions and sharding them\").stop()\n    self.timer.log([\"configure functions and sharding them\"])\n</code></pre>"},{"location":"generated-trainer-base_trainer/#src.python.easydel.trainer.base_trainer.BaseTrainer.train","title":"<code>train()</code>  <code>abstractmethod</code>","text":"<p>abstract of Train Function to train model</p> Source code in <code>src/python/easydel/trainer/base_trainer.py</code> <pre><code>@abc.abstractmethod\ndef train(self):\n    \"\"\"abstract of Train Function to train model\"\"\"\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-causal_language_model_trainer/","title":"trainer.causal_language_model_trainer.causal_language_model_trainer","text":""},{"location":"generated-trainer-causal_language_model_trainer-causal_language_model_trainer/#src.python.easydel.trainer.causal_language_model_trainer.causal_language_model_trainer.CausalLanguageModelTrainer","title":"<code>CausalLanguageModelTrainer</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/causal_language_model_trainer.py</code> <pre><code>class CausalLanguageModelTrainer(BaseTrainer):\n\n    def create_collate_function(\n            self,\n            max_sequence_length: int,\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n    ) -&gt; Callable:\n        def collate_fn(batch):\n            results = {}\n            for key in batch[0].keys():\n                if truncation_mode == \"keep_end\":\n                    corrected_sequence = [\n                        jnp.array(f[key])[..., -max_sequence_length:] for f in batch\n                    ]\n                else:\n                    corrected_sequence = [\n                        jnp.array(f[key])[..., :max_sequence_length] for f in batch\n                    ]\n                results[key] = jnp.stack(corrected_sequence).reshape(\n                    -1,\n                    corrected_sequence[0].shape[-1]\n                )\n            return results\n\n        return collate_fn\n\n    def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n        \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n        them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n        on a batch of data, including:\n\n        Args:\n            self: Access the class attributes\n\n        Returns:\n            A TrainerConfigureFunctionFuncOutput object\n        \"\"\"\n\n        def initialize_state_function():\n            initialized_parameters = self.model.init_weights(\n                jax.random.PRNGKey(0),\n                self.arguments.init_input_shape\n            )\n\n            if self.arguments.dtype == jnp.bfloat16:\n                initialized_parameters = self.model.to_bf16(initialized_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n            tx = self.tx\n            parameters = flax.core.freeze({\"params\": initialized_parameters})\n            tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n            if self.rapture is not None:\n                lora_parameters = self.lora_parameters\n                if self.arguments.dtype == jnp.bfloat16:\n                    lora_parameters = self.model.to_bf16(lora_parameters)\n                elif self.arguments.dtype == jnp.float16:\n                    lora_parameters = self.model.to_fp16(lora_parameters)\n\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=lora_parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(tx_init),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n            else:\n                return EasyDeLState.create(\n                    tx=tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=tx_init,\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n\n        def create_state_from_params_function(parameters):\n            if self.rapture is None:\n                return EasyDeLState.create(\n                    tx=self.tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n            else:\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n\n        state_shape = jax.eval_shape(initialize_state_function)\n        state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            state_shape\n        )\n        create_sharded_state_from_params_function = pjit(\n            create_state_from_params_function,\n            in_shardings=(state_partition_spec.params,),\n            out_shardings=state_partition_spec,\n            donate_argnums=(0,)\n        )\n        sharded_train_step_function = pjit(\n            create_casual_language_model_train_step(\n                partition_spec=self.arguments.step_partition_spec,\n                label_smoothing_factor=self.arguments.label_smoothing_factor,\n                z_loss=self.arguments.z_loss,\n            ),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n\n        sharded_eval_step_function = pjit(\n            create_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(PartitionSpec(), PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n\n        mesh = self.arguments.get_mesh()\n        self.arguments.ckpt_path_exists()\n        checkpoint_manager = self.arguments.get_streaming_checkpointer()\n        self.state_partition_spec = state_partition_spec\n        self.state_shape = state_shape\n\n        return TrainerConfigureFunctionFuncOutput(\n            create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n            sharded_train_step_function=sharded_train_step_function,\n            sharded_eval_step_function=sharded_eval_step_function,\n            mesh=mesh,\n            checkpoint_manager=checkpoint_manager,\n            initialize_state_function=initialize_state_function\n        )\n\n    def initialize_state(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None,\n    ) -&gt; Tuple[EasyDeLState, Mapping[str, Callable], Mapping[str, Callable]]:\n        if model_parameters is None and state is None and self.rapture is None and self.checkpoint_path is None:\n            raise RuntimeError(\n                \"You are passing `model_parameters=None`, `state=None`, and `checkpoint_path=None` and also you are not\"\n                \" using LoRA, if you are \"\n                \"Using LoRA make sure to pass parameters and Rapture Config correctly otherwise pass the \"\n                \"model_parameters or state.\"\n            )\n        if model_parameters is None and state is None:\n            model_parameters = self.lora_parameters\n        with self.mesh:\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                self.state_partition_spec,\n                dtype_specs=self.dtype\n            )\n            if state is not None:\n                sharded_state = state\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n                if sharded_state.opt_state is None:\n                    prefix_print(\n                        \"Action\", \"Optimizer State is not Found!, initializing one.\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = sharded_state.init_opt_state()\n                        opt_state = sharded_state.opt_state if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                            lambda f, x: f(x),\n                            shard_fns.opt_state,\n                            sharded_state.opt_state\n                        )\n                        sharded_state = sharded_state.replace(\n                            opt_state=opt_state\n                        )\n            elif self.finetune:\n\n                if model_parameters is None and self.checkpoint_path is not None:\n                    prefix_print(\n                        \"Action\", f\"Loading Model From {self.checkpoint_path}\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = EasyDeLState.load_state(\n                            verbose=self.arguments.verbose,\n                            state_shard_fns=shard_fns,\n                            init_optimizer_state=True,\n                            checkpoint_path=self.checkpoint_path,\n                            input_shape=self.arguments.init_input_shape,\n                            config_kwargs=self.arguments.loaded_model_config_kwargs\n                        )\n                        state_shape = jax.eval_shape(lambda: sharded_state)\n                        state_partition_spec = match_partition_rules(\n                            self.config.get_partition_rules(\n                                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                            state_shape\n                        )\n                        sharded_train_step_function = pjit(\n                            create_casual_language_model_train_step(\n                                partition_spec=self.arguments.step_partition_spec,\n                                label_smoothing_factor=self.arguments.label_smoothing_factor,\n                                z_loss=self.arguments.z_loss,\n                            ),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n                            donate_argnums=(0, 0),\n                        )\n\n                        sharded_eval_step_function = pjit(\n                            create_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(PartitionSpec(), PartitionSpec(), PartitionSpec()),\n                            donate_argnums=(0, 0),\n                        )\n\n                        self.state_partition_spec = state_partition_spec\n                        self.state_shape = state_shape\n                        self.sharded_train_step_function = sharded_train_step_function\n                        self.sharded_eval_step_function = sharded_eval_step_function\n\n                    if self.arguments.remove_ckpt_after_load:\n                        os.remove(self.checkpoint_path)\n                elif model_parameters is not None and self.checkpoint_path is None:\n                    prefix_print(\n                        \"Action\", f\"Sharding Passed Parameters\"\n                    )\n                    from flax.core import unfreeze\n                    if not isinstance(model_parameters, flax.core.FrozenDict):\n                        prefix_print(\n                            \"Warning\",\n                            \"Model Parameters should be like FrozenDict({'params': params}) make sure to \"\n                            \"pass as type FrozenDict in case of not getting UnExcepted Errors \"\n                        )\n\n                    model_parameters = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                        lambda f, x: f(x),\n                        shard_fns.params,\n                        model_parameters,\n                    )\n                    sharded_state = self.create_sharded_state_from_params_function(model_parameters)\n                elif model_parameters is not None and self.checkpoint_path is not None:\n                    raise EasyDeLTimerError(\n                        \"You can't pass `model_parameters` and `checkpoint_path` at same time\"\n                    )\n                else:\n                    raise EasyDeLTimerError(\n                        \"You should pass `model_parameters` or `checkpoint_path` to trainer in order to load model\"\n                    )\n            else:\n                sharded_state = self.initialize_state_function()\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n\n            self.sharded_state = sharded_state\n            return sharded_state, shard_fns, gather_fns\n\n    def _save_state(\n            self,\n            state: EasyDeLState,\n            gather_fns: Optional[Any | Mapping[str, Callable] | dict[Callable]],\n            milestone: bool = False\n    ) -&gt; str:\n        step = int(\n            jax.device_get(\n                state.step\n            )\n        ) + self.arguments.step_start_point if self.arguments.step_start_point is not None else int(\n            jax.device_get(\n                state.step\n            )\n        )\n\n        checkpoint_dir = os.path.join(self.arguments.save_dir, self.arguments.model_name)\n        filename_extension = \".easy\"\n        if self.arguments.save_total_limit:\n            checkpoint_files = glob(os.path.join(checkpoint_dir, f\"*{filename_extension}\"))\n            checkpoint_files.sort(key=os.path.getmtime)\n            for old_checkpoint in checkpoint_files[:-self.arguments.save_total_limit]:\n                os.remove(old_checkpoint)\n                termcolor.cprint(f\"Removed old checkpoint: {old_checkpoint}\", color=\"red\", force_color=True)\n\n        checkpoint_name = f\"{self.arguments.model_name}-S{step}\"\n        filename = f\"{checkpoint_name}_{step}\" if milestone else f\"{checkpoint_name}\"\n        filename += \".easy\"\n        termcolor.cprint(f\"Saving Model {filename}.\", color=\"cyan\", force_color=True)\n        state.save_state(\n            filename=filename,\n            checkpoint_dir=checkpoint_dir,\n            gather_fns=gather_fns,\n            float_dtype=self.dtype,\n            verbose=self.arguments.verbose,\n            save_optimizer=self.arguments.save_optimizer_state,\n        )\n        return filename\n\n    def train(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None\n    ) -&gt; CausalLMTrainerOutput:\n        \"\"\"The train function is the main function of this module.\n        It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n        The train function returns an CausalLMTrainerOutput object that contains the last saved file name, predict func,\n        train state, mesh and checkpoint streamer.\n\n        Args:\n            self: Make the class methods aware of other methods and\n                attributes within the class\n            model_parameters: flax.core.FrozenDict: Load a pre-trained\n                model\n            state: Optional[EasyDeLState]: Ready to Use State\n\n        Returns:\n            An object of type \"CausalLMTrainerOutput\"\n        \"\"\"\n\n        def get_layer_names(frozen_dict, prefix=\"\"):\n            layer_names = {}\n            for key, value in frozen_dict.items():\n                if isinstance(value, FrozenDict):\n                    layer_names.update(get_layer_names(value, prefix=f\"{prefix}_{key}\"))\n                else:\n                    layer_name = f\"{prefix}_{key}\".lstrip(\"/\")\n                    layer_names[layer_name] = value\n            return layer_names\n\n        def count_model_parameters(_p):\n            termcolor.cprint(\n                f\"Model Contain {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9} \"\n                f\"Billion Parameters\",\n                color=\"red\", force_color=True\n            )\n\n        checkpoint_path = \"SAVING_SKIPPED\"\n        if self.arguments.performance_mode:\n            termcolor.cprint(\n                \"Performance Mode is ON, we will ignore the Memory Tracking, WANDB Logging, and extra information \"\n                \"Process.\",\n                color=\"red\",\n                force_color=True\n            )\n        start_time = time.time()\n        sharded_state, shard_fns, gather_fns = self.initialize_state(\n            model_parameters=model_parameters,\n            state=state\n        )\n\n        count_model_parameters(sharded_state.params)\n        with self.mesh:\n            pbar = tqdm(total=self.max_training_steps)\n            current_step = int(jax.device_get(sharded_state.step))\n            loss_sum = None\n            accuracy_sum = None\n            pbar.update(sharded_state.step.tolist())  # type: ignore\n            if self.wandb_runtime is not None:\n                model_parameters_number = sum(\n                    n.size for n in\n                    jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_state.params))[0]\n                ) / 1e9\n                self.wandb_runtime.log(\n                    {\n                        \"Number of Model Parameters (Billion)\": model_parameters_number\n                    }\n                )\n                wandb.summary[\"Number of Model Parameters (Billion)\"] = model_parameters_number\n            try:\n                train_iter = iter(self.dataloader_train)\n                for epoch in range(self.arguments.num_train_epochs):\n                    time_s = time.time()\n                    for _ in range(self.max_training_steps // self.arguments.num_train_epochs):\n                        try:\n                            batch = next(train_iter)\n                        except StopIteration:\n                            train_iter = iter(self.dataloader_train)\n                            batch = next(train_iter)\n                        current_step += 1\n                        if (\n                                self.arguments.step_start_point is not None\n                                and\n                                self.arguments.step_start_point &gt; current_step\n                        ):\n                            pbar.update(1)\n                        elif current_step &lt; self.max_training_steps:\n\n                            time_prev = time_s\n                            time_s = time.time()\n                            step_time = time_s - time_prev\n\n                            for ssb in self.arguments.ids_to_pop_from_dataset:\n                                _ = batch.pop(ssb, None)\n\n                            (\n                                sharded_state,\n                                loss,\n                                metrics,\n                            ) = self.sharded_train_step_function(sharded_state, batch)\n\n                            trained_tokens = jnp.multiply(\n                                self.arguments.max_sequence_length, jnp.multiply(\n                                    current_step,\n                                    self.arguments.total_batch_size\n                                )\n                            )  # It's faster\n\n                            with jax.spmd_mode(\"allow_all\"):\n                                calculating_metrics_start = time.time()\n                                loss_sum = loss if loss_sum is None else loss_sum + loss\n                                accuracy = metrics[\"accuracy\"]\n                                accuracy_sum = accuracy if accuracy_sum is None else accuracy_sum + accuracy\n                                mean_loss = loss_sum / (current_step - self.arguments.step_start_point)\n                                mean_accuracy = accuracy_sum / (current_step - self.arguments.step_start_point)\n                                perplexity = jnp.exp(loss)\n                                calculating_metrics_end = time.time()\n                                train_metrics = {\n                                    \"train/loss\": loss.tolist(),\n                                    \"train/mean_loss\": mean_loss.tolist(),\n                                    \"train/accuracy\": accuracy,\n                                    \"train/mean_accuracy\": mean_accuracy.tolist(),\n                                    \"train/learning_rate\": self.scheduler(current_step).tolist(),\n                                    \"train/step\": current_step,\n                                    \"train/step_time\": step_time,\n                                    \"train/perplexity\": perplexity.tolist(),\n                                    \"train/trained_tokens\": trained_tokens,\n                                    \"train/regularization_z_loss\": metrics[\"regularization_z_loss\"].tolist(),\n                                    \"train/epoch\": epoch,\n                                }\n                            if self.arguments.log_grad_norms:\n                                train_metrics.update(\n                                    {\n                                        \"train/max_grad_norm\": metrics[\"max_grad_norm\"].tolist(),\n                                        \"train/mean_grad_norm\": metrics[\"mean_grad_norm\"].tolist(),\n                                    }\n                                )\n                            aux_loss = metrics.get(\"aux_loss\", None)\n                            if aux_loss is not None:\n                                train_metrics.update(\n                                    {\n                                        \"train/aux_loss\": aux_loss.tolist()\n                                    }\n                                )\n                            pbar.update(1)\n                            pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in train_metrics.items()})\n                            if not self.arguments.performance_mode:\n                                if self.arguments.log_grad_norms:\n                                    train_metrics.update({\n                                        f\"grad_norm/{layer_name}\": grad_norm.tolist()\n                                        for layer_name, grad_norm in get_layer_names(metrics[\"grad_norms\"]).items()\n                                    })\n                                train_metrics.update(\n                                    {\n                                        \"time_cal/calculating_metrics_step_time\": (\n                                                calculating_metrics_end - calculating_metrics_start\n                                        )\n                                    }\n                                )\n                                train_metrics.update(self.arguments.captured_memory)\n                            if self.wandb_runtime is not None and not self.arguments.performance_mode:\n                                with jax.spmd_mode(\"allow_all\"):\n                                    self.wandb_runtime.log(train_metrics)\n                            if self.arguments.training_time is not None:\n                                if time.time() - start_time &gt; self.arguments.training_time:\n                                    raise EasyDeLTimerError(\"Time Out\")\n                        else:\n                            break\n                        if self.arguments.save_steps is not None and current_step % self.arguments.save_steps == 0:\n                            if self.rapture is None:\n                                filename = self._save_state(\n                                    state=sharded_state,\n                                    gather_fns=gather_fns,\n                                    milestone=True\n                                )\n                                checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n                            else:\n                                print(\n                                    termcolor.colored(\n                                        \"Info : \", color=\"red\", force_color=True\n                                    ),\n                                    termcolor.colored(\n                                        \"You can not use `save_steps` while using LoRA \"\n                                        \"right now. this action will be skipped\", color=\"white\", force_color=True\n                                    )\n                                )\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n\n            except EasyDeLTimerError:\n                termcolor.cprint(\n                    \"Training reached out maximum training Time Killing training Process \"\n                    \"and Will return Current State of the Model with Parameters.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n            if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n                print(\n                    termcolor.colored(\n                        \"Info : \", color=\"red\", force_color=True\n                    ),\n                    termcolor.colored(\n                        \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                    )\n                )\n                sharded_state = sharded_state.replace(\n                    params=self.rapture.merge_parameters(sharded_state.params)\n                )\n            output = CausalLMTrainerOutput(\n                state=sharded_state,\n                mesh=self.mesh,\n                shard_fns=shard_fns,\n                gather_fns=gather_fns,\n                checkpoint_manager=self.checkpoint_manager,\n            )\n            if self.arguments.save_steps is None or self.arguments.do_last_save:\n                shard_fns, gather_fns = make_shard_and_gather_fns(\n                    match_partition_rules(\n                        self.config.get_partition_rules(\n                            fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                        jax.eval_shape(lambda: sharded_state)\n                    ),\n                    dtype_specs=self.dtype\n                )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n                # crashing errors and saving errors\n                filename = self._save_state(\n                    state=sharded_state,\n                    gather_fns=gather_fns\n                )\n                checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n            if self.arguments.do_eval:\n                for _ in self.eval(\n                        sharded_state\n                ):\n                    ...\n\n            output.checkpoint_path = checkpoint_path\n            output.last_save_file_name = filename\n            self.arguments._stop_capturing_memory = True\n            self.finish()\n            return output\n\n    def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n        \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n        assert self.dataloader_eval is not None, \"`dataloader_eval` is required by evaluator function.\"\n        with self.mesh:\n            pbar = tqdm(total=self.max_evaluation_steps)\n            pbar.set_description(\"Evaluating\")\n            current_step = 0\n            loss_sum = None\n            accuracy_sum = None\n\n            try:\n                eval_iter = iter(self.dataloader_eval)\n                for _ in range(self.max_evaluation_steps):\n                    try:\n                        batch = next(eval_iter)\n                    except StopIteration:\n                        eval_iter = iter(self.dataloader_eval)\n                        batch = next(eval_iter)\n                    current_step += 1\n                    time_start = time.time()\n                    for key in self.arguments.ids_to_pop_from_dataset:\n                        _ = batch.pop(key, None)\n                    metrics = self.sharded_eval_step_function(\n                        model_state,\n                        batch\n                    )\n                    total_time = time.time() - time_start\n                    (\n                        loss, accuracy, aux_loss\n                    ) = metrics\n\n                    loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                    accuracy_sum = (\n                        accuracy.tolist() if (\n                                accuracy_sum is None\n                        ) else accuracy_sum + accuracy\n                    )\n\n                    eval_metrics = {\n                        \"eval/loss\": loss.tolist(),\n                        \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                        \"eval/mean_accuracy_sum\": accuracy_sum / (\n                                current_step - self.arguments.step_start_point\n                        ),\n                        \"eval/step\": current_step,\n                        \"eval/step_time\": total_time,\n                        \"eval/perplexity\": jnp.exp(loss).tolist(),\n                    }\n                    if aux_loss is not None:\n                        eval_metrics.update(\n                            {\"eval/aux_loss\": aux_loss}\n                        )\n                    log_metrics = copy.deepcopy(eval_metrics)\n                    eval_metrics.update(self.arguments.captured_memory)\n                    if self.arguments.use_wandb:\n                        with jax.spmd_mode(\"allow_all\"):\n                            self.wandb_runtime.log(\n                                eval_metrics\n                            )\n\n                    pbar.update(1)\n                    pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                    yield log_metrics\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-causal_language_model_trainer/#src.python.easydel.trainer.causal_language_model_trainer.causal_language_model_trainer.CausalLanguageModelTrainer.configure_functions","title":"<code>configure_functions()</code>","text":"<p>The configure_functions function is responsible for configuring the functions that will be used in training. It does this by first defining a function called function_configurations, which initializes the model parameters and returns them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate on a batch of data, including:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <p>Returns:</p> Type Description <code>TrainerConfigureFunctionFuncOutput</code> <p>A TrainerConfigureFunctionFuncOutput object</p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/causal_language_model_trainer.py</code> <pre><code>def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n    \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n    It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n    them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n    on a batch of data, including:\n\n    Args:\n        self: Access the class attributes\n\n    Returns:\n        A TrainerConfigureFunctionFuncOutput object\n    \"\"\"\n\n    def initialize_state_function():\n        initialized_parameters = self.model.init_weights(\n            jax.random.PRNGKey(0),\n            self.arguments.init_input_shape\n        )\n\n        if self.arguments.dtype == jnp.bfloat16:\n            initialized_parameters = self.model.to_bf16(initialized_parameters)\n        elif self.arguments.dtype == jnp.float16:\n            initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n        tx = self.tx\n        parameters = flax.core.freeze({\"params\": initialized_parameters})\n        tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n        if self.rapture is not None:\n            lora_parameters = self.lora_parameters\n            if self.arguments.dtype == jnp.bfloat16:\n                lora_parameters = self.model.to_bf16(lora_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                lora_parameters = self.model.to_fp16(lora_parameters)\n\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=lora_parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(tx_init),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n        else:\n            return EasyDeLState.create(\n                tx=tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=tx_init,\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n\n    def create_state_from_params_function(parameters):\n        if self.rapture is None:\n            return EasyDeLState.create(\n                tx=self.tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n        else:\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n\n    state_shape = jax.eval_shape(initialize_state_function)\n    state_partition_spec = match_partition_rules(\n        self.config.get_partition_rules(\n            fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n        state_shape\n    )\n    create_sharded_state_from_params_function = pjit(\n        create_state_from_params_function,\n        in_shardings=(state_partition_spec.params,),\n        out_shardings=state_partition_spec,\n        donate_argnums=(0,)\n    )\n    sharded_train_step_function = pjit(\n        create_casual_language_model_train_step(\n            partition_spec=self.arguments.step_partition_spec,\n            label_smoothing_factor=self.arguments.label_smoothing_factor,\n            z_loss=self.arguments.z_loss,\n        ),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n        donate_argnums=(0, 0),\n    )\n\n    sharded_eval_step_function = pjit(\n        create_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(PartitionSpec(), PartitionSpec(), PartitionSpec()),\n        donate_argnums=(0, 0),\n    )\n\n    mesh = self.arguments.get_mesh()\n    self.arguments.ckpt_path_exists()\n    checkpoint_manager = self.arguments.get_streaming_checkpointer()\n    self.state_partition_spec = state_partition_spec\n    self.state_shape = state_shape\n\n    return TrainerConfigureFunctionFuncOutput(\n        create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n        sharded_train_step_function=sharded_train_step_function,\n        sharded_eval_step_function=sharded_eval_step_function,\n        mesh=mesh,\n        checkpoint_manager=checkpoint_manager,\n        initialize_state_function=initialize_state_function\n    )\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-causal_language_model_trainer/#src.python.easydel.trainer.causal_language_model_trainer.causal_language_model_trainer.CausalLanguageModelTrainer.eval","title":"<code>eval(model_state)</code>","text":"<p>Evaluate the Given Model State and yield the eval metrics</p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/causal_language_model_trainer.py</code> <pre><code>def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n    \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n    assert self.dataloader_eval is not None, \"`dataloader_eval` is required by evaluator function.\"\n    with self.mesh:\n        pbar = tqdm(total=self.max_evaluation_steps)\n        pbar.set_description(\"Evaluating\")\n        current_step = 0\n        loss_sum = None\n        accuracy_sum = None\n\n        try:\n            eval_iter = iter(self.dataloader_eval)\n            for _ in range(self.max_evaluation_steps):\n                try:\n                    batch = next(eval_iter)\n                except StopIteration:\n                    eval_iter = iter(self.dataloader_eval)\n                    batch = next(eval_iter)\n                current_step += 1\n                time_start = time.time()\n                for key in self.arguments.ids_to_pop_from_dataset:\n                    _ = batch.pop(key, None)\n                metrics = self.sharded_eval_step_function(\n                    model_state,\n                    batch\n                )\n                total_time = time.time() - time_start\n                (\n                    loss, accuracy, aux_loss\n                ) = metrics\n\n                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                accuracy_sum = (\n                    accuracy.tolist() if (\n                            accuracy_sum is None\n                    ) else accuracy_sum + accuracy\n                )\n\n                eval_metrics = {\n                    \"eval/loss\": loss.tolist(),\n                    \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                    \"eval/mean_accuracy_sum\": accuracy_sum / (\n                            current_step - self.arguments.step_start_point\n                    ),\n                    \"eval/step\": current_step,\n                    \"eval/step_time\": total_time,\n                    \"eval/perplexity\": jnp.exp(loss).tolist(),\n                }\n                if aux_loss is not None:\n                    eval_metrics.update(\n                        {\"eval/aux_loss\": aux_loss}\n                    )\n                log_metrics = copy.deepcopy(eval_metrics)\n                eval_metrics.update(self.arguments.captured_memory)\n                if self.arguments.use_wandb:\n                    with jax.spmd_mode(\"allow_all\"):\n                        self.wandb_runtime.log(\n                            eval_metrics\n                        )\n\n                pbar.update(1)\n                pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                yield log_metrics\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                color=\"cyan\",\n                force_color=True\n            )\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-causal_language_model_trainer/#src.python.easydel.trainer.causal_language_model_trainer.causal_language_model_trainer.CausalLanguageModelTrainer.train","title":"<code>train(model_parameters=None, state=None)</code>","text":"<p>The train function is the main function of this module. It takes a model_parameters argument which can be used to load a pretrained model and finetune it. The train function returns an CausalLMTrainerOutput object that contains the last saved file name, predict func, train state, mesh and checkpoint streamer.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the class methods aware of other methods and attributes within the class</p> required <code>model_parameters</code> <code>Optional[FrozenDict]</code> <p>flax.core.FrozenDict: Load a pre-trained model</p> <code>None</code> <code>state</code> <code>Optional[EasyDeLState]</code> <p>Optional[EasyDeLState]: Ready to Use State</p> <code>None</code> <p>Returns:</p> Type Description <code>CausalLMTrainerOutput</code> <p>An object of type \"CausalLMTrainerOutput\"</p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/causal_language_model_trainer.py</code> <pre><code>def train(\n        self,\n        model_parameters: Optional[flax.core.FrozenDict] = None,\n        state: Optional[EasyDeLState] = None\n) -&gt; CausalLMTrainerOutput:\n    \"\"\"The train function is the main function of this module.\n    It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n    The train function returns an CausalLMTrainerOutput object that contains the last saved file name, predict func,\n    train state, mesh and checkpoint streamer.\n\n    Args:\n        self: Make the class methods aware of other methods and\n            attributes within the class\n        model_parameters: flax.core.FrozenDict: Load a pre-trained\n            model\n        state: Optional[EasyDeLState]: Ready to Use State\n\n    Returns:\n        An object of type \"CausalLMTrainerOutput\"\n    \"\"\"\n\n    def get_layer_names(frozen_dict, prefix=\"\"):\n        layer_names = {}\n        for key, value in frozen_dict.items():\n            if isinstance(value, FrozenDict):\n                layer_names.update(get_layer_names(value, prefix=f\"{prefix}_{key}\"))\n            else:\n                layer_name = f\"{prefix}_{key}\".lstrip(\"/\")\n                layer_names[layer_name] = value\n        return layer_names\n\n    def count_model_parameters(_p):\n        termcolor.cprint(\n            f\"Model Contain {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9} \"\n            f\"Billion Parameters\",\n            color=\"red\", force_color=True\n        )\n\n    checkpoint_path = \"SAVING_SKIPPED\"\n    if self.arguments.performance_mode:\n        termcolor.cprint(\n            \"Performance Mode is ON, we will ignore the Memory Tracking, WANDB Logging, and extra information \"\n            \"Process.\",\n            color=\"red\",\n            force_color=True\n        )\n    start_time = time.time()\n    sharded_state, shard_fns, gather_fns = self.initialize_state(\n        model_parameters=model_parameters,\n        state=state\n    )\n\n    count_model_parameters(sharded_state.params)\n    with self.mesh:\n        pbar = tqdm(total=self.max_training_steps)\n        current_step = int(jax.device_get(sharded_state.step))\n        loss_sum = None\n        accuracy_sum = None\n        pbar.update(sharded_state.step.tolist())  # type: ignore\n        if self.wandb_runtime is not None:\n            model_parameters_number = sum(\n                n.size for n in\n                jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_state.params))[0]\n            ) / 1e9\n            self.wandb_runtime.log(\n                {\n                    \"Number of Model Parameters (Billion)\": model_parameters_number\n                }\n            )\n            wandb.summary[\"Number of Model Parameters (Billion)\"] = model_parameters_number\n        try:\n            train_iter = iter(self.dataloader_train)\n            for epoch in range(self.arguments.num_train_epochs):\n                time_s = time.time()\n                for _ in range(self.max_training_steps // self.arguments.num_train_epochs):\n                    try:\n                        batch = next(train_iter)\n                    except StopIteration:\n                        train_iter = iter(self.dataloader_train)\n                        batch = next(train_iter)\n                    current_step += 1\n                    if (\n                            self.arguments.step_start_point is not None\n                            and\n                            self.arguments.step_start_point &gt; current_step\n                    ):\n                        pbar.update(1)\n                    elif current_step &lt; self.max_training_steps:\n\n                        time_prev = time_s\n                        time_s = time.time()\n                        step_time = time_s - time_prev\n\n                        for ssb in self.arguments.ids_to_pop_from_dataset:\n                            _ = batch.pop(ssb, None)\n\n                        (\n                            sharded_state,\n                            loss,\n                            metrics,\n                        ) = self.sharded_train_step_function(sharded_state, batch)\n\n                        trained_tokens = jnp.multiply(\n                            self.arguments.max_sequence_length, jnp.multiply(\n                                current_step,\n                                self.arguments.total_batch_size\n                            )\n                        )  # It's faster\n\n                        with jax.spmd_mode(\"allow_all\"):\n                            calculating_metrics_start = time.time()\n                            loss_sum = loss if loss_sum is None else loss_sum + loss\n                            accuracy = metrics[\"accuracy\"]\n                            accuracy_sum = accuracy if accuracy_sum is None else accuracy_sum + accuracy\n                            mean_loss = loss_sum / (current_step - self.arguments.step_start_point)\n                            mean_accuracy = accuracy_sum / (current_step - self.arguments.step_start_point)\n                            perplexity = jnp.exp(loss)\n                            calculating_metrics_end = time.time()\n                            train_metrics = {\n                                \"train/loss\": loss.tolist(),\n                                \"train/mean_loss\": mean_loss.tolist(),\n                                \"train/accuracy\": accuracy,\n                                \"train/mean_accuracy\": mean_accuracy.tolist(),\n                                \"train/learning_rate\": self.scheduler(current_step).tolist(),\n                                \"train/step\": current_step,\n                                \"train/step_time\": step_time,\n                                \"train/perplexity\": perplexity.tolist(),\n                                \"train/trained_tokens\": trained_tokens,\n                                \"train/regularization_z_loss\": metrics[\"regularization_z_loss\"].tolist(),\n                                \"train/epoch\": epoch,\n                            }\n                        if self.arguments.log_grad_norms:\n                            train_metrics.update(\n                                {\n                                    \"train/max_grad_norm\": metrics[\"max_grad_norm\"].tolist(),\n                                    \"train/mean_grad_norm\": metrics[\"mean_grad_norm\"].tolist(),\n                                }\n                            )\n                        aux_loss = metrics.get(\"aux_loss\", None)\n                        if aux_loss is not None:\n                            train_metrics.update(\n                                {\n                                    \"train/aux_loss\": aux_loss.tolist()\n                                }\n                            )\n                        pbar.update(1)\n                        pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in train_metrics.items()})\n                        if not self.arguments.performance_mode:\n                            if self.arguments.log_grad_norms:\n                                train_metrics.update({\n                                    f\"grad_norm/{layer_name}\": grad_norm.tolist()\n                                    for layer_name, grad_norm in get_layer_names(metrics[\"grad_norms\"]).items()\n                                })\n                            train_metrics.update(\n                                {\n                                    \"time_cal/calculating_metrics_step_time\": (\n                                            calculating_metrics_end - calculating_metrics_start\n                                    )\n                                }\n                            )\n                            train_metrics.update(self.arguments.captured_memory)\n                        if self.wandb_runtime is not None and not self.arguments.performance_mode:\n                            with jax.spmd_mode(\"allow_all\"):\n                                self.wandb_runtime.log(train_metrics)\n                        if self.arguments.training_time is not None:\n                            if time.time() - start_time &gt; self.arguments.training_time:\n                                raise EasyDeLTimerError(\"Time Out\")\n                    else:\n                        break\n                    if self.arguments.save_steps is not None and current_step % self.arguments.save_steps == 0:\n                        if self.rapture is None:\n                            filename = self._save_state(\n                                state=sharded_state,\n                                gather_fns=gather_fns,\n                                milestone=True\n                            )\n                            checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n                        else:\n                            print(\n                                termcolor.colored(\n                                    \"Info : \", color=\"red\", force_color=True\n                                ),\n                                termcolor.colored(\n                                    \"You can not use `save_steps` while using LoRA \"\n                                    \"right now. this action will be skipped\", color=\"white\", force_color=True\n                                )\n                            )\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                color=\"cyan\",\n                force_color=True\n            )\n\n        except EasyDeLTimerError:\n            termcolor.cprint(\n                \"Training reached out maximum training Time Killing training Process \"\n                \"and Will return Current State of the Model with Parameters.\",\n                color=\"cyan\",\n                force_color=True\n            )\n        if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n            print(\n                termcolor.colored(\n                    \"Info : \", color=\"red\", force_color=True\n                ),\n                termcolor.colored(\n                    \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                )\n            )\n            sharded_state = sharded_state.replace(\n                params=self.rapture.merge_parameters(sharded_state.params)\n            )\n        output = CausalLMTrainerOutput(\n            state=sharded_state,\n            mesh=self.mesh,\n            shard_fns=shard_fns,\n            gather_fns=gather_fns,\n            checkpoint_manager=self.checkpoint_manager,\n        )\n        if self.arguments.save_steps is None or self.arguments.do_last_save:\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                match_partition_rules(\n                    self.config.get_partition_rules(\n                        fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                    ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                    jax.eval_shape(lambda: sharded_state)\n                ),\n                dtype_specs=self.dtype\n            )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n            # crashing errors and saving errors\n            filename = self._save_state(\n                state=sharded_state,\n                gather_fns=gather_fns\n            )\n            checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n        if self.arguments.do_eval:\n            for _ in self.eval(\n                    sharded_state\n            ):\n                ...\n\n        output.checkpoint_path = checkpoint_path\n        output.last_save_file_name = filename\n        self.arguments._stop_capturing_memory = True\n        self.finish()\n        return output\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-fwd_bwd_functions/","title":"trainer.causal_language_model_trainer.fwd_bwd_functions","text":""},{"location":"generated-trainer-causal_language_model_trainer-fwd_bwd_functions/#src.python.easydel.trainer.causal_language_model_trainer.fwd_bwd_functions.create_casual_language_model_evaluation_step","title":"<code>create_casual_language_model_evaluation_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp'))</code>","text":"<p>The create_casual_language_model_evaluation_step function is used to create a function that calculates the loss  and accuracy of a model. It takes in a set of parameters, which are then passed into the state.apply_fn function to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify the partitioning of the model parameters</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp')</code> <p>Returns:</p> Type Description <p>A function that can be used to calculate the loss and accuracy</p> <p>of a model</p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/fwd_bwd_functions.py</code> <pre><code>def create_casual_language_model_evaluation_step(\n        partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\")\n):\n    \"\"\"The create_casual_language_model_evaluation_step function is used to create a function that calculates the loss\n     and accuracy of a model. It takes in a set of parameters, which are then passed into the state.apply_fn function\n    to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these\n    logits.\n\n    Args:\n        partition_spec: Specify the partitioning of the model parameters\n\n    Returns:\n        A function that can be used to calculate the loss and accuracy\n        of a model\n    \"\"\"\n\n    def casual_language_model_evaluation_step(state, batch_eval):\n        \"\"\"The casual_language_model_evaluation_step function is used to calculate the loss and accuracy of a model.\n        It takes in a set of parameters, which are then passed into the state.apply_fn function\n        to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from\n        these logits.\n\n        Args:\n            state: Store the model parameters and other information\n                about the training process\n            batch_eval: Pass the batch of data to the function\n\n        Returns:\n            The loss and accuracy of the model\n        \"\"\"\n        batch_eval = with_sharding_constraint(batch_eval, partition_spec)\n\n        def calculate_loss(params):\n            \"\"\"\n            The calculate_loss function is used to calculate the loss and accuracy of a model.\n            It takes in a set of parameters, which are then passed into the state.apply_fn function\n            to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated\n            from these logits.\n\n            :param params: Pass the model parameters to the function\n            :return: The loss and the accuracy\n\n            \"\"\"\n            labels = batch_eval.get(\"labels\", None)\n            if labels is None:\n                labels = batch_eval[\"input_ids\"][..., 1:]\n            else:\n                labels = labels[..., 1:]\n            model_outputs = state.apply_fn(params=params, **batch_eval, return_dict=True)\n            logits = model_outputs.logits\n            aux_loss = getattr(model_outputs, \"aux_loss\", None)\n            valid = jnp.where(\n                (batch_eval[\"attention_mask\"][:, 1:].astype(jnp.float32) != 0)\n                &amp; (labels &gt; 0),\n                1.0,\n                0.0,\n            )\n            loss, accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :],\n                labels,\n                valid,\n            )\n            if aux_loss is not None:\n                loss += aux_loss\n            return loss, (accuracy, aux_loss)\n\n        loss__, (accuracy__, aux_loss__) = calculate_loss(state.params)\n        return loss__, accuracy__, aux_loss__\n\n    return casual_language_model_evaluation_step\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-fwd_bwd_functions/#src.python.easydel.trainer.causal_language_model_trainer.fwd_bwd_functions.create_casual_language_model_train_step","title":"<code>create_casual_language_model_train_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp'), label_smoothing_factor=0.0, z_loss=0.0, gradient_accumulation_steps=1)</code>","text":"<p>The create_casual_language_model_train_step function is a training step function that takes in the current state of the model,and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state with new parameters based on these gradients.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify which devices the model will be split across</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp')</code> <code>label_smoothing_factor</code> <p>A float in [0, 1] specifying the amount of label smoothing to apply, where 0 means no smoothing.</p> <code>0.0</code> <code>z_loss</code> <p>A regularization term that adds a penalty for large weights, where 0 means no regularization.</p> <code>0.0</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int : gradient accumulation step size from arguments</p> <code>1</code> <p>Returns:</p> Type Description <p>A casual_language_model_train_step function that takes in the</p> <p>current state of the model,</p> Source code in <code>src/python/easydel/trainer/causal_language_model_trainer/fwd_bwd_functions.py</code> <pre><code>def create_casual_language_model_train_step(\n        partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\"),\n        label_smoothing_factor=0.0,\n        z_loss=0.0,\n        gradient_accumulation_steps: int = 1,\n):\n    \"\"\"The create_casual_language_model_train_step function is a training step function that takes in the current state\n    of the model,and a batch of data. It then calculates the loss and accuracy for this batch, and returns\n    an updated state with new parameters based on these gradients.\n\n    Args:\n        partition_spec: Specify which devices the model will be split\n            across\n        label_smoothing_factor: A float in [0, 1] specifying the amount\n            of label smoothing to apply, where 0 means no smoothing.\n        z_loss: A regularization term that adds a penalty for large\n            weights, where 0 means no regularization.\n        gradient_accumulation_steps: int : gradient accumulation step\n            size from arguments\n\n    Returns:\n        A casual_language_model_train_step function that takes in the\n        current state of the model,\n    \"\"\"\n    assert gradient_accumulation_steps &gt; 0, \"gradient_accumulation_steps must be greater than 0\"  # Ignore\n\n    def casual_language_model_train_step(state, batch):\n        \"\"\"The casual_language_model_train_step function is a training step function that takes in the current state\n        of the model and a batch of data. It then calculates the loss and accuracy for this batch,\n        and returns an updated state with new parameters based on these gradients.\n\n        Args:\n            state: Store the model parameters\n            batch: Pass the data to the model, dict with input_ids(bs,\n                seq_len), labels(bs, seq_len-1), attention_mask(bs,\n                seq_len)\n\n        Returns:\n            A tuple of (state, loss, accuracy)\n        \"\"\"\n        batch = with_sharding_constraint(batch, partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.get(\"labels\", None)\n            if labels is None:\n                labels = batch[\"input_ids\"][..., 1:]\n            else:\n                labels = labels[..., 1:]\n            model_outputs = state.apply_fn(params=params, **batch, return_dict=True)\n            logits = model_outputs.logits\n            aux_loss = getattr(model_outputs, \"aux_loss\", None)\n            loss_normalizing_factor = (\n                SpecialLossNormalizingFactor.NUM_REAL_TARGET_TOKENS\n            )\n            # loss_weights is 1 unless the label is &lt;= 0 or the attention mask is 0\n            loss_weights = jnp.where(\n                (batch[\"attention_mask\"][:, 1:] != 0) &amp; (labels &gt; 0), 1, 0\n            )\n            lnf, weights = get_loss_normalizing_factor_and_weights(\n                loss_normalizing_factor,\n                {\n                    \"decoder_target_tokens\": labels,\n                    \"decoder_loss_weights\": loss_weights,\n                },\n            )\n            (\n                loss,\n                z_loss_computed,\n                weight_sum,\n                accuracy,\n            ) = compute_weighted_cross_entropy_and_accuracy(\n                logits=logits[:, :-1, :],\n                targets=labels,\n                weights=weights,\n                label_smoothing=label_smoothing_factor,\n                z_loss=z_loss,\n                loss_normalizing_factor=lnf,\n            )\n            if aux_loss is not None:\n                loss += aux_loss\n            return loss, (accuracy, z_loss_computed, aux_loss)\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (loss__, (accuracy__, z_loss_computed__, aux_loss__)), grad = grad_fn(state.params)\n        state = state.apply_gradients(grads=grad)\n\n        grad_norms = jax.tree_map(jnp.linalg.norm, grad)\n        max_grad_norm = jax.tree_util.tree_reduce(jnp.maximum, grad_norms)\n        mean_grad_norm = jax.tree_util.tree_reduce(\n            jnp.add, jax.tree_map(jnp.sum, grad_norms)\n        ) / jax.tree_util.tree_reduce(jnp.add, jax.tree_map(jnp.size, grad_norms))\n        metrics = {\n            \"accuracy\": accuracy__,\n            \"regularization_z_loss\": z_loss_computed__,\n            \"max_grad_norm\": max_grad_norm,\n            \"mean_grad_norm\": mean_grad_norm,\n            \"grad_norms\": grad_norms,\n        }\n        if aux_loss__ is not None:\n            metrics.update({\"aux_loss\": aux_loss__})\n        return state, loss__, metrics\n\n    return casual_language_model_train_step\n</code></pre>"},{"location":"generated-trainer-causal_language_model_trainer-modeling_output/","title":"trainer.causal_language_model_trainer.modeling_output","text":""},{"location":"generated-trainer-dpo-dpo_trainer/","title":"trainer.dpo.dpo_trainer","text":""},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer","title":"<code>DPOTrainer</code>","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>easydel DPO Trainer Class</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>class DPOTrainer(BaseTrainer, ABC):\n    \"\"\"\n    easydel DPO Trainer Class\n    \"\"\"\n\n    def __init__(\n            self,\n            arguments: TrainArguments,\n            model_state: EasyDeLState | str,\n            ref_model_state: Optional[EasyDeLState | str] = None,\n            beta: float = 0.1,\n            label_smoothing: float = .0,\n            loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] = \"sigmoid\",\n            label_pad_token_id: int = -100,\n            padding_value: int = None,\n            train_dataset: Optional[Dataset] = None,\n            eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n            tokenizer: Optional[PreTrainedTokenizerBase] = None,\n            data_collator: Optional[Callable] = None,\n            max_length: Optional[int] = None,\n            max_prompt_length: Optional[int] = None,\n            max_target_length: Optional[int] = None,\n            precompute_ref_log_probs: bool = False,\n            model_init_kwargs: Optional[Dict] = None,\n            ref_model_init_kwargs: Optional[Dict] = None,\n            reference_free: bool = False,\n            auto_shard_model_state: bool = True,\n            auto_shard_ref_model_state: bool = True,\n            is_encoder_decoder: Optional[bool] = False,\n            dataset_map_arguments: Optional[dict] = None,\n            low_mem_usage: bool = True,\n            auto_fix_data: bool = True,\n            _do_init_fns: bool = True,\n    ):\n\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object.\n\n\n        :param self: Refer to the object itself\n        :param model_state: EasyDeLState | str: Pass the model state to the trainer\n        :param ref_model_state: Optional[EasyDeLState | str]: Pass the reference model state\n        :param beta: float: Control the strength of the regularization term\n        :param label_smoothing: float: Smooth the labels\n        :param loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] : Determine the loss function used\n        :param arguments: TrainArguments: Pass the arguments to the trainer\n        :param label_pad_token_id: int: Pad the labels\n        :param padding_value: int: Specify the value that is used for padding\n        :param train_dataset: Optional[Dataset]: Load the training dataset\n        :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer\n        :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer\n        :param max_length: Optional[int]: Set the maximum length of the input sequence\n        :param max_prompt_length: Optional[int]: Set the maximum length of the prompt\n        :param max_target_length: Optional[int]: Truncate the target sequence\n        :param data_collator: Optional[Callable]: Function to be used for creating datasets.\n        :param precompute_ref_log_probs: bool: Precompute the log probabilities of the reference model\n        :param model_init_kwargs: Optional[Dict]: Pass in the model_kwargs to model for init process\n        :param ref_model_init_kwargs: Optional[Dict]: Pass the ref_model_init_kwargs to ref_model for init process\n        :param auto_shard_model_state: bool: whenever to automatically shard `model_state`\n        :param auto_shard_ref_model_state: bool: whenever to automatically shard `ref_model_state`\n        :param dataset_map_arguments: Optional[dict]: arguments to be passed to train and eval datasets for\n        tokenizing process with `dataset.map`.\n        :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure\n        model with provided training Arguments\n        :param : Set the padding value for the model\n        \"\"\"\n        assert arguments is not None, (\n            \"You Have to pass arguments that will be used for training but you have passed\"\n            \"`arguments=None`\"\n        )\n        assert isinstance(arguments, TrainArguments), (\n            f\"arguments type must be `TrainArguments` but got {type(arguments)}\"\n        )\n        if model_init_kwargs is None:\n            model_init_kwargs = {}\n        elif not isinstance(model_state, str):\n            raise ValueError(\"You passed model_kwargs to the DPOTrainer. But your model is already instantiated.\")\n\n        if ref_model_init_kwargs is None:\n            ref_model_init_kwargs = {}\n        elif not isinstance(ref_model_state, str):\n            raise ValueError(\n                \"You passed ref_model_kwargs to the DPOTrainer. But your ref_model is already instantiated.\"\n            )\n\n        if isinstance(model_state, str):\n            warnings.warn(\n                \"You passed a model_id to the DPOTrainer. This will automatically create an \"\n                \"`AutoEasyDeLModelForCausalLM` for you.\"\n            )\n            model_state = EasyDeLState.from_pretrained(\n                model_state,\n                **model_init_kwargs\n            )\n        if isinstance(ref_model_state, str):\n            warnings.warn(\n                \"You passed a ref model_id to the DPOTrainer. This will automatically create an \"\n                \"`AutoEasyDeLModelForCausalLM`\"\n            )\n            ref_model_state = EasyDeLState.from_pretrained(\n                ref_model_state,\n                **ref_model_init_kwargs\n            )\n\n        if loss_type in [\"hinge\", \"ipo\", \"kto_pair\"] and label_smoothing &gt; 0:\n            warnings.warn(\n                \"You are using a loss type that does not support label smoothing. Ignoring label_smoothing parameter.\"\n            )\n        self.auto_fix_data = auto_fix_data\n\n        if tokenizer is None:\n            raise ValueError(\"tokenizer must be specified to tokenize a DPO dataset.\")\n        if max_length is None:\n            warnings.warn(\n                \"`max_length` is not set in the DPOTrainer's init\"\n                \" it will default to `512` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_length = 512\n        if max_prompt_length is None:\n            warnings.warn(\n                \"`max_prompt_length` is not set in the DPOTrainer's init\"\n                \" it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_prompt_length = 128\n\n        if max_target_length is None and is_encoder_decoder:\n            warnings.warn(\n                \"When using an encoder decoder architecture, you should set `max_target_length` in the \"\n                \"DPOTrainer's init it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_target_length = 128\n\n        padding_value = padding_value if padding_value is not None else tokenizer.pad_token_id\n        self.max_length = max_length\n        self.label_pad_token_id = label_pad_token_id\n        self.padding_value = padding_value\n        self.max_prompt_length = max_prompt_length\n        self.truncation_mode = arguments.truncation_mode\n\n        self.max_target_length = max_target_length\n        self.tokenizer = tokenizer\n        self.precompute_ref_log_probs = precompute_ref_log_probs\n        self.reference_free = reference_free\n        self.is_encoder_decoder = False\n        self._precomputed_train_ref_log_probs = False\n        self._precomputed_eval_ref_log_probs = False\n        self.beta = beta\n        self.label_smoothing = label_smoothing\n        self.loss_type = loss_type\n        self.low_mem_usage = low_mem_usage\n        data_collator = DPODataCollatorWithPadding(\n            max_prompt_length=self.max_prompt_length,\n            max_target_length=self.max_target_length,\n            pad_token_id=tokenizer.pad_token_id,\n            label_pad_token_id=label_pad_token_id,\n            is_encoder_decoder=False,\n        ) if data_collator is None else data_collator\n        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n        if dataset_map_arguments is None:\n            dataset_map_arguments = {}\n        train_dataset = train_dataset.map(\n            self.tokenize_row,\n            **dataset_map_arguments\n        )\n        if eval_dataset is not None:\n            eval_dataset = eval_dataset.map(\n                self.tokenize_row,\n                **dataset_map_arguments\n            )\n\n        self.arguments = arguments\n        self.hp_name = None\n        self.deepspeed = None\n        self.is_in_train = False\n\n        self.data_collator = data_collator\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.tokenizer = tokenizer\n        self.ref_model_state = ref_model_state\n        self.model_state = model_state\n        self._loggers_initialized = False\n        self.mesh = self.arguments.get_mesh()\n        assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n\n        self.concatenated_forward = create_concatenated_forward(\n            is_encoder_decoder=self.is_encoder_decoder,\n            padding_value=padding_value,\n            label_pad_token_id=label_pad_token_id,\n        )\n        self.auto_shard_ref_model_state = auto_shard_ref_model_state\n        self.auto_shard_model_state = auto_shard_model_state\n\n        self._cached_p_l_s = None\n        self._cached_c_l_s = None\n        self._cached_r_l_s = None\n        super().__init__(\n            arguments=arguments,\n            dataset_train=train_dataset,\n            dataset_eval=eval_dataset,\n            finetune=True,\n            checkpoint_path=None,\n            _do_init_fns=_do_init_fns\n        )\n\n    def initialize_trainer_utils(self):\n        \"\"\"\n        The initialize_trainer_utils function is responsible for initializing the following:\n            - wandb_runtime (if you use_wandb is True)\n            - timer object (for logging time taken by various functions)\n            - dataloader objects for training and evaluation data, along with max steps per epoch.\n              The configure_dataloader function accomplishes this task.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of functions\n\n        \"\"\"\n        self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n        self.timer = Timers(\n            use_wandb=False,\n            tensorboard_writer=self.arguments.get_board()\n        )\n\n        self.timer(\"configure dataloaders\").start()\n        dataset_configurations = self.configure_dataloader()\n        self.dataloader_train = dataset_configurations.dataloader_train\n        self.max_training_steps = dataset_configurations.max_training_steps\n        self.dataloader_eval = dataset_configurations.dataloader_eval\n        self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n        self.timer(\"configure dataloaders\").stop()\n\n        self.timer.log([\"configure dataloaders\"])\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n        model_configurations = self.configure_model()\n        model = model_configurations.model\n        tx = model_configurations.tx\n        scheduler = model_configurations.scheduler\n        config = model_configurations.config\n        self.model = model\n        self.tx = tx\n        self.scheduler = scheduler\n        self.config = config\n        if self.rapture is not None:\n            lora_modules = self.rapture.apply_lora(\n                module=model,\n                parameters=self.arguments.rapture_config.parameters,\n                tx=tx,\n            )\n            self.lora_parameters = lora_modules.lora_parameters\n            self.lora_apply_fn = lora_modules.lora_module.__call__\n            self.lora_opt_state = lora_modules.lora_opt_state\n            self.lora_model = lora_modules.lora_module\n            self.lora_tx = lora_modules.lora_tx\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n        self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n\n        self.timer(\"configure functions and sharding them\").start()\n\n        if self.auto_shard_model_state:\n            self.timer(\"Sharding Model State\").start()\n            self.model_state: EasyDeLState = self.shard_states(\n                self.model_state,\n                self.model_state.module.config.get_partition_rules(self.arguments.fully_sharded_data_parallel)\n            )\n\n            termcolor.cprint(\"initializing TX and Schedulers for `model_state`\", force_color=True, color=\"cyan\")\n\n            params_with_opt = (\n                self.model_state.params[\n                    'params'\n                ] if '_overwrite_with_gradient' in self.model_state.params else self.model_state.params\n            )\n            opt_state = self.tx.init(params_with_opt)\n\n            self.model_state = self.model_state.replace(\n                opt_state=opt_state,\n                tx=self.tx\n            )\n\n            self.timer(\"Sharding Model State\").stop()\n            self.timer.log([\"Sharding Model State\"])\n        if self.auto_shard_ref_model_state and self.ref_model_state is not None:\n            self.timer(\"Sharding Ref Model State\").start()\n            self.ref_model_state = self.shard_states(\n                self.ref_model_state,\n                self.ref_model_state.module.config.get_partition_rules(self.arguments.fully_sharded_data_parallel)\n            )\n            self.timer(\"Sharding Ref Model State\").stop()\n            self.timer.log([\"Sharding Ref Model State\"])\n\n        function_configurations = self.configure_functions()\n        self.create_sharded_state_from_params_function = (\n            function_configurations.create_sharded_state_from_params_function\n        )\n        self.sharded_train_step_function = function_configurations.sharded_train_step_function\n        self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n        self.mesh = function_configurations.mesh\n        self.checkpoint_manager = function_configurations.checkpoint_manager\n        self.initialize_state_function = function_configurations.initialize_state_function\n        self.timer(\"configure functions and sharding them\").stop()\n        self.timer.log([\"configure functions and sharding them\"])\n\n    def create_collate_function(\n            self,\n            max_sequence_length: int,\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n    ) -&gt; Callable:\n        return self.data_collator\n\n    def shard_states(self, state, rules):\n        with self.arguments.get_mesh():\n            partition_spec = match_partition_rules(rules=rules, params=jax.eval_shape(lambda: state))\n\n            def _shard(x):\n                return x\n\n            shard = pjit(\n                _shard,\n                in_shardings=(PartitionSpec(),),\n                out_shardings=partition_spec\n            )\n            return shard(state)\n\n    def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n        dataloader_train = self.get_train_dataloader()\n        max_evaluation_steps = None\n        dataloader_eval = None\n\n        max_training_steps = self.arguments.num_train_epochs * len(\n            dataloader_train\n        ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n        if self.eval_dataset is not None:\n            dataloader_eval = self.get_eval_dataloader(self.eval_dataset)\n            max_evaluation_steps = len(dataloader_eval)\n        return TrainerConfigureDataloaderFuncOutput(\n            dataloader_train=dataloader_train,  # type:ignore\n            max_training_steps=max_training_steps,\n            dataloader_eval=dataloader_eval,\n            max_evaluation_steps=max_evaluation_steps\n        )\n\n    def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n        def initialize_state_function():\n            initialized_parameters = self.model.init_weights(\n                jax.random.PRNGKey(0),\n                self.arguments.init_input_shape\n            )\n\n            if self.arguments.dtype == jnp.bfloat16:\n                initialized_parameters = self.model.to_bf16(initialized_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n            tx = self.tx\n            parameters = flax.core.freeze({\"params\": initialized_parameters})\n            tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n            if self.rapture is not None:\n                lora_parameters = self.lora_parameters\n                if self.arguments.dtype == jnp.bfloat16:\n                    lora_parameters = self.model.to_bf16(lora_parameters)\n                elif self.arguments.dtype == jnp.float16:\n                    lora_parameters = self.model.to_fp16(lora_parameters)\n\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=lora_parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(tx_init),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model_state.module.config,\n                    module_config_args=None,\n                )\n            else:\n                return EasyDeLState.create(\n                    tx=tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model_state.module.config),\n                    tx_init=tx_init,\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n\n        def create_state_from_params_function(parameters):\n            if self.rapture is None:\n                return EasyDeLState.create(\n                    tx=self.tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model_state.module.config),\n                    tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n            else:\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model_state.module.config,\n                    module_config_args=None,\n                )\n\n        state_shape = jax.eval_shape(lambda: self.model_state)\n\n        state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            state_shape\n        )\n        create_sharded_state_from_params_function = pjit(\n            create_state_from_params_function,\n            in_shardings=(state_partition_spec.params,),\n            out_shardings=state_partition_spec,\n            donate_argnums=(0,)\n        )\n        train_function = create_dpo_train_function(\n            concatenated_forward=self.concatenated_forward,\n            ref_state=self.ref_model_state,\n            loss_type=self.loss_type,\n            reference_free=self.reference_free,\n            label_smoothing=self.label_smoothing,\n            beta=self.beta\n        )\n        sharded_train_step_function = pjit(\n            train_function,\n            in_shardings=(state_partition_spec, self.arguments.step_partition_spec),\n            out_shardings=(state_partition_spec, PartitionSpec()),\n        )\n\n        eval_function = create_dpo_eval_function(\n            concatenated_forward=self.concatenated_forward,\n            ref_state=self.ref_model_state,\n            loss_type=self.loss_type,\n            reference_free=self.reference_free,\n            label_smoothing=self.label_smoothing,\n            beta=self.beta\n        )\n\n        sharded_eval_step_function = pjit(\n            eval_function,\n            in_shardings=(state_partition_spec, self.arguments.step_partition_spec),\n            out_shardings=(state_partition_spec, PartitionSpec()),\n        )\n\n        self.arguments.ckpt_path_exists()\n        self.state_partition_spec = state_partition_spec\n        self.state_shape = state_shape\n        checkpoint_manager = self.arguments.get_streaming_checkpointer()\n        mesh = self.arguments.get_mesh()\n        return TrainerConfigureFunctionFuncOutput(\n            initialize_state_function=initialize_state_function,\n            sharded_train_step_function=sharded_train_step_function,\n            create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n            checkpoint_manager=checkpoint_manager,\n            mesh=mesh,\n            sharded_eval_step_function=sharded_eval_step_function\n        )\n\n    def configure_model(self) -&gt; TrainerConfigureModelFuncOutput:\n        config = self.model_state.module.config\n        tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_training_steps)\n        return TrainerConfigureModelFuncOutput(\n            model=self.model_state.module,\n            config=config,  # type: ignore\n            scheduler=scheduler,\n            tx=tx\n        )\n\n    def _get_train_dataloader(self) -&gt; tensorflow.data.Dataset:\n\n        \"\"\"\n        The _get_train_dataloader function is used to create a tensorflow.data.Dataset object for the training dataset.\n\n        :param self: Represent the instance of the class\n        :return: A dataloader object\n        \"\"\"\n        if self.train_dataset is None:\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\n\n        train_dataset = self.train_dataset\n        data_collator = self.data_collator\n\n        return tensorflow_datasets.as_numpy(\n            train_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=True,\n                drop_remainder=True\n            )\n        )\n\n    def _get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the evaluation [`~tensorflow.data.Dataset`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n\n        return tensorflow_datasets.as_numpy(\n            eval_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=self.data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=False,\n                drop_remainder=True\n            )\n        )\n\n    def get_train_dataloader(\n            self,\n    ) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the training [`~tensorflow.data.Dataset`].\n        \"\"\"\n\n        if self.precompute_ref_log_probs and not self._precomputed_train_ref_log_probs:\n\n            data_loader = tensorflow_datasets.as_numpy(\n                self.train_dataset.to_tf_dataset(\n                    batch_size=self.arguments.total_batch_size,\n                    collate_fn=self.data_collator,\n                    num_workers=self.arguments.dataloader_num_workers,\n                    shuffle=False,\n                    drop_remainder=True\n                )\n            )\n            reference_chosen_log_probs = []\n            reference_rejected_log_probs = []\n            for padded_batch in tqdm(iterable=data_loader, desc=\"Train dataset reference log probs\"):\n                reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(\n                    self.model_state,\n                    padded_batch,\n                )\n                reference_chosen_log_probs.append(reference_chosen_logp)\n                reference_rejected_log_probs.append(reference_rejected_logp)\n\n            all_reference_chosen_log_probs = jnp.concatenate(reference_chosen_log_probs)\n            all_reference_rejected_log_probs = jnp.concatenate(reference_rejected_log_probs)\n            self.train_dataset = self.train_dataset.add_column(\n                name=\"reference_chosen_log_probs\", column=all_reference_chosen_log_probs\n            )\n            self.train_dataset = self.train_dataset.add_column(\n                name=\"reference_rejected_log_probs\", column=all_reference_rejected_log_probs\n            )\n\n            self._precomputed_train_ref_log_probs = True\n        return self._get_train_dataloader()\n\n    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the evaluation [`~tensorflow.data.Dataset`].\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n\n        if self.precompute_ref_log_probs and not self._precomputed_eval_ref_log_probs:\n\n            # prepare dataloader\n            data_loader = tensorflow_datasets.as_numpy(\n                eval_dataset.to_tf_dataset(\n                    batch_size=self.arguments.total_batch_size,\n                    collate_fn=self.data_collator,\n                    num_workers=self.arguments.dataloader_num_workers,\n                    shuffle=False,\n                    drop_remainder=True\n                )\n            )\n\n            reference_chosen_log_probs = []\n            reference_rejected_log_probs = []\n            for padded_batch in tqdm(iterable=data_loader, desc=\"Eval dataset reference log probs\"):\n                reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(\n                    self.model_state,\n                    padded_batch\n                )\n                reference_chosen_log_probs.append(reference_chosen_logp.cpu())\n                reference_rejected_log_probs.append(reference_rejected_logp.cpu())\n\n            all_reference_chosen_log_probs = jnp.concatenate(reference_chosen_log_probs)\n            all_reference_rejected_log_probs = jnp.concatenate(reference_rejected_log_probs)\n\n            eval_dataset = eval_dataset.add_column(name=\"reference_chosen_log_probs\",\n                                                   column=all_reference_chosen_log_probs)\n            eval_dataset = eval_dataset.add_column(\n                name=\"reference_rejected_log_probs\", column=all_reference_rejected_log_probs\n            )\n\n            if self.eval_dataset is not None:\n                self.eval_dataset = eval_dataset\n            self._precomputed_eval_ref_log_probs = True\n\n        return self._get_eval_dataloader(eval_dataset=eval_dataset)\n\n    def build_tokenized_answer(self, prompt, answer):\n        \"\"\"\n        Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n        It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n        \"\"\"\n\n        full_tokenized = self.tokenizer(prompt + answer, add_special_tokens=False)\n        prompt_input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n        answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids):]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids):]\n        prompt_input_ids = jnp.asarray(prompt_input_ids, dtype=\"i4\")\n        answer_input_ids = jnp.asarray(answer_input_ids, dtype=\"i4\")\n        full_concat_input_ids = jnp.concatenate(\n            (\n                prompt_input_ids,\n                answer_input_ids\n            )\n        )\n\n        # Prepare input tokens for token by token comparison\n        full_input_ids = jnp.array(full_tokenized[\"input_ids\"])\n\n        if len(full_input_ids) != len(full_concat_input_ids):\n            raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n\n        response_token_ids_start_idx = len(prompt_input_ids)\n        if prompt_input_ids.tolist() != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n            response_token_ids_start_idx -= 1\n\n        prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n        prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n\n        if len(prompt_input_ids) != len(prompt_attention_mask):\n            raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n\n        answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n\n        return dict(\n            prompt_input_ids=jnp.array(prompt_input_ids, dtype=\"i4\"),\n            prompt_attention_mask=jnp.array(prompt_attention_mask, dtype=\"i4\"),\n            input_ids=jnp.array(answer_input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(answer_attention_mask, dtype=\"i4\"),\n        )\n\n    def tokenize_row(self, feature, state: EasyDeLState = None) -&gt; Dict:\n\n        \"\"\"\n        The tokenize_row function is responsible for taking a single row of data and converting it into the format that\n        the model expects. This includes:\n        - Tokenizing the text (using HuggingFace's tokenizer)\n        - Padding/truncating sequences to a fixed length (if necessary)\n        - Creating attention masks, which tell the model which tokens are padding and which aren't.\n\n        :param self: Represent the instance of the class\n        :param feature: Pass in the data from the dataset\n        :param state: EasyDeLState: Keep track of the state of the tokenizer\n        :return: A dictionary of the following keys\n        \"\"\"\n        batch = {}\n        prompt = feature[\"prompt\"]\n        chosen = feature[\"chosen\"]\n        rejected = feature[\"rejected\"]\n\n        if not isinstance(prompt, str):\n            raise ValueError(f\"prompt should be an str but got {type(prompt)} , {prompt}\")\n        prompt_tokens = self.tokenizer(\n            prompt,\n            add_special_tokens=False,\n            return_tensors=\"np\",\n        )\n        prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n\n        if not isinstance(chosen, str):\n            raise ValueError(f\"chosen should be an str but got {type(chosen)} , {chosen}\")\n        chosen_tokens = self.build_tokenized_answer(prompt, chosen)\n\n        if not isinstance(rejected, str):\n            raise ValueError(f\"rejected should be an str but got {type(rejected)}\")\n        rejected_tokens = self.build_tokenized_answer(prompt, rejected)\n        v2d = lambda ar: ar.reshape(1, -1) if ar.ndim == 1 else ar\n\n        def add_tkn(n, ar):\n            return jnp.concatenate(\n                (\n                    jnp.array(n).reshape(1, 1),\n                    v2d(ar)\n                ), axis=-1\n            )\n\n        def add_post_tkn(n, ar):\n            return jnp.concatenate(\n                (\n                    v2d(ar),\n                    jnp.array(n).reshape(1, 1)\n                ), axis=-1\n            )\n\n        prompt_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            prompt_tokens[\"prompt_input_ids\"]\n        )\n        chosen_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            chosen_tokens[\"prompt_input_ids\"]\n        )\n        rejected_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            rejected_tokens[\"prompt_input_ids\"]\n        )\n\n        prompt_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, prompt_tokens[\"prompt_attention_mask\"]\n        )\n        chosen_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, chosen_tokens[\"prompt_attention_mask\"]\n        )\n        rejected_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, rejected_tokens[\"prompt_attention_mask\"]\n        )\n\n        # add EOS token to end of answer\n        chosen_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, chosen_tokens[\"input_ids\"])\n        chosen_tokens[\"attention_mask\"] = add_post_tkn(1, chosen_tokens[\"attention_mask\"])\n\n        rejected_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, rejected_tokens[\"input_ids\"])\n        rejected_tokens[\"attention_mask\"] = add_post_tkn(1, rejected_tokens[\"attention_mask\"])\n\n        longer_response_length = max(chosen_tokens[\"input_ids\"].shape[-1], rejected_tokens[\"input_ids\"].shape[-1])\n\n        # if combined sequence is too long, truncate the prompt\n        for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n            length_rn = answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length\n            if length_rn &gt; self.max_length:\n\n                if self.truncation_mode == \"keep_start\":\n                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                        answer_tokens[k] = answer_tokens[k][:, : self.max_prompt_length]\n                elif self.truncation_mode == \"keep_end\":\n                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                        answer_tokens[k] = answer_tokens[k][:, -self.max_prompt_length:]\n                else:\n                    raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n        # if that's still too long, truncate the response\n        for answer_tokens in [chosen_tokens, rejected_tokens]:\n            if answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length &gt; self.max_length:\n                for k in [\"input_ids\", \"attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, : self.max_length - self.max_prompt_length]\n\n        chosen_sequence_tokens = {\n            k: jnp.concatenate(\n                (v2d(chosen_tokens[f\"prompt_{k}\"]), v2d(chosen_tokens[k])),\n                axis=-1\n            ) for k in [\"input_ids\", \"attention_mask\"]\n        }\n        rejected_sequence_tokens = {\n            k: jnp.concatenate(\n                (v2d(rejected_tokens[f\"prompt_{k}\"]), v2d(rejected_tokens[k])),\n                axis=-1\n            ) for k in [\"input_ids\", \"attention_mask\"]\n        }\n        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"labels\"].at[\n                                           : len(chosen_tokens[\"prompt_input_ids\"])\n                                           ].set([self.label_pad_token_id] * len(chosen_tokens[\"prompt_input_ids\"]))\n        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"labels\"].at[\n                                             : len(rejected_tokens[\"prompt_input_ids\"])\n                                             ].set(\n            ([self.label_pad_token_id] * len(rejected_tokens[\"prompt_input_ids\"]))\n        )\n\n        for k, tokens_ in {\n            \"chosen_\": chosen_sequence_tokens,\n            \"rejected_\": rejected_sequence_tokens,\n            \"\": prompt_tokens,\n        }.items():\n            for type_key, tokens in tokens_.items():\n                if type_key == \"token_type_ids\":\n                    continue\n\n                b, s = tokens.shape\n\n                if self.max_prompt_length &gt; s:\n                    if k == \"chosen_\":\n                        if type_key == \"input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n\n                        tokens = tokens[..., :self.max_target_length]\n\n                        if tokens.shape[-1] != self.max_target_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_target_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                        tokens = tokens[..., :self.max_target_length]\n                    elif k == \"rejected_\":\n                        if type_key == \"input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_target_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        tokens = tokens[..., :self.max_target_length]\n                        if tokens.shape[-1] != self.max_target_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_target_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                    elif k == \"\":\n                        if type_key == \"prompt_input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"prompt_attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"prompt_labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        tokens = tokens[..., :self.max_prompt_length]\n                        if tokens.shape[-1] != self.max_prompt_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_prompt_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                batch[f\"{k}{type_key}\"] = tokens\n        return batch\n\n    def compute_reference_log_probs(\n            self,\n            state: EasyDeLState,\n            padded_batch: Dict,\n    ) -&gt; tuple[Any, Any]:\n        \"\"\"\n        Computes log probabilities of the reference model for a single padded batch of a DPO specific dataset.\n        \"\"\"\n\n        if self.ref_model_state is None:\n            (\n                reference_chosen_log_probs,\n                reference_rejected_log_probs,\n                _,\n                _,\n            ) = self.concatenated_forward(\n                apply_fn=state.apply_fn,\n                params=state.params,\n                batch=padded_batch,\n            )\n        else:\n            (\n                reference_chosen_log_probs,\n                reference_rejected_log_probs,\n                _,\n                _,\n            ) = self.concatenated_forward(\n                apply_fn=self.ref_model_state.apply_fn,\n                params=self.ref_model_state.params,\n                batch=padded_batch,\n            )\n\n        return reference_chosen_log_probs, reference_rejected_log_probs\n\n    def _save_state(\n            self,\n            state: EasyDeLState,\n            gather_fns: Optional[Any | Mapping[str, Callable] | dict[Callable]],\n            milestone: bool = False\n    ) -&gt; str:\n        step = int(\n            jax.device_get(\n                state.step\n            )\n        ) + self.arguments.step_start_point if self.arguments.step_start_point is not None else int(\n            jax.device_get(\n                state.step\n            )\n        )\n        checkpoint_name = f\"{self.arguments.model_name}-S{step}\"\n        filename = f\"{checkpoint_name}_{step}\" if milestone else f\"{checkpoint_name}\"\n        filename += \".easy\"\n        termcolor.cprint(f\"Saving Model {filename}.\", color=\"cyan\", force_color=True)\n        state.save_state(\n            filename=filename,\n            checkpoint_dir=os.path.join(self.arguments.save_dir, self.arguments.model_name),\n            gather_fns=gather_fns,\n            float_dtype=self.dtype,\n            verbose=self.arguments.verbose,\n            save_optimizer=self.arguments.save_optimizer_state,\n        )\n        return filename\n\n    def train(self) -&gt; DPOTrainerOutput:\n        assert self.model_state is not None, \"model_state can not be None for training purpose\"\n        with self.mesh:\n            with jax.default_device(jax.devices(\"cpu\")[0]) if self.low_mem_usage else leave_alone_context_manager:\n                dir_prefix: str = \"/dev/shm\" if sys.platform != \"win32\" else \".\"\n                checkpoint_path = \"SAVING_SKIPPED\"\n\n                pbar = tqdm(total=self.max_training_steps)\n                pbar.set_description(\"Training\")\n                current_step = self.model_state.step.tolist() if isinstance(\n                    self.model_state.step,\n                    jax.Array\n                ) else self.model_state.step\n\n                loss_sum = None\n                chosen_rewards_sum = None\n                rejected_rewards_sum = None\n\n                try:\n                    for epoch_index in range(self.arguments.num_train_epochs):\n                        for batch in self.dataloader_train:\n                            current_step += 1\n                            if self.arguments.step_start_point &gt; current_step:\n                                ...\n                            elif current_step &lt; self.max_training_steps:\n                                time_start = time.time()\n\n                                self.model_state, metrics = self.sharded_train_step_function(\n                                    self.model_state,\n                                    batch\n                                )\n                                total_time = time.time() - time_start\n                                (\n                                    loss, chosen_rewards, rejected_rewards\n                                ) = metrics.loss, metrics.chosen_rewards[0], metrics.rejected_rewards[0]\n\n                                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n\n                                rejected_rewards_sum = (\n                                    rejected_rewards.tolist() if (\n                                            rejected_rewards_sum is None\n                                    ) else rejected_rewards_sum + rejected_rewards\n                                )\n                                chosen_rewards_sum = (\n                                    chosen_rewards.tolist() if (\n                                            chosen_rewards_sum is None\n                                    ) else chosen_rewards_sum + chosen_rewards\n                                )\n                                train_metrics = {\n                                    \"train/loss\": loss.tolist(),\n                                    \"train/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                                    \"train/mean_rejected_rewards\": rejected_rewards_sum / (\n                                            current_step - self.arguments.step_start_point\n                                    ),\n                                    \"train/mean_chosen_rewards\": chosen_rewards_sum / (\n                                            current_step - self.arguments.step_start_point\n                                    ),\n                                    \"train/learning_rate\": self.scheduler(\n                                        jax.device_get(self.model_state.step)\n                                    ).tolist(),\n                                    \"train/step\": current_step,\n                                    \"train/step_time\": total_time,\n                                    \"train/perplexity\": jnp.exp(loss).tolist(),\n                                    \"train/epoch\": epoch_index\n                                }\n                                log_metrics = copy.deepcopy(train_metrics)\n                                train_metrics.update(self.arguments.captured_memory)\n                                if self.arguments.use_wandb:\n                                    with jax.spmd_mode(\"allow_all\"):\n                                        self.wandb_runtime.log(\n                                            train_metrics\n                                        )\n                                pbar.update(1)\n                                pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in log_metrics.items()})\n                            else:\n                                break\n                except KeyboardInterrupt:\n                    termcolor.cprint(\n                        \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                        color=\"cyan\",\n                        force_color=True\n                    )\n\n                except EasyDeLTimerError:\n                    termcolor.cprint(\n                        \"Training reached out maximum training Time Killing training Process \"\n                        \"and Will return Current State of the Model with Parameters.\",\n                        color=\"cyan\",\n                        force_color=True\n                    )\n\n                if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n                    print(\n                        termcolor.colored(\n                            \"Info : \", color=\"red\", force_color=True\n                        ),\n                        termcolor.colored(\n                            \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                        )\n                    )\n                    self.model_state = self.model_state.replace(\n                        params=self.rapture.merge_parameters(self.model_state.params)\n                    )\n\n                shard_fns, gather_fns = make_shard_and_gather_fns(\n                    partition_specs=match_partition_rules(\n                        rules=self.model_state.module.config.get_partition_rules(\n                            self.arguments.fully_sharded_data_parallel\n                        ),\n                        params=jax.eval_shape(lambda: self.model_state)\n                    ),\n                    dtype_specs=self.arguments.dtype\n                )\n                output = DPOTrainerOutput(\n                    state=self.model_state,\n                    mesh=self.mesh,\n                    shard_fns=shard_fns,\n                    gather_fns=gather_fns,\n                    checkpoint_manager=self.checkpoint_manager,\n                )\n                if self.arguments.save_steps is None and self.arguments.do_last_save:\n                    shard_fns, gather_fns = make_shard_and_gather_fns(\n                        match_partition_rules(\n                            self.config.get_partition_rules(\n                                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                            jax.eval_shape(lambda: self.model_state)\n                        ),\n                        dtype_specs=self.dtype\n                    )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n                    # crashing errors and saving errors\n                    filename = self._save_state(\n                        state=self.model_state,\n                        gather_fns=gather_fns\n                    )\n                    checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n                if self.arguments.do_eval:\n                    for _ in self.eval(\n                            self.model_state\n                    ):\n                        ...\n\n                output.checkpoint_path = checkpoint_path\n                output.last_save_file_name = filename\n                self.finish()\n\n        return output\n\n    def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n        \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n        assert self.eval_dataset is not None, \"`dataloader_eval` is required by evaluator function.\"\n        with self.mesh:\n            pbar = tqdm(total=self.max_evaluation_steps)\n            pbar.set_description(\"Evaluating\")\n            current_step = 0\n            loss_sum = None\n            chosen_rewards_sum = None\n            rejected_rewards_sum = None\n\n            try:\n                for batch in self.dataloader_eval:\n                    current_step += 1\n                    time_start = time.time()\n                    for key in self.arguments.ids_to_pop_from_dataset:\n                        _ = batch.pop(key, None)\n                    for key in list(batch.keys()):\n                        if not (\n                                key.endswith(\"_input_ids\")\n                                or key.endswith(\"_attention_mask\")\n                                or key.endswith(\"_labels\")\n                        ):\n                            _ = batch.pop(key, None)\n\n                    metrics = self.sharded_eval_step_function(\n                        model_state,\n                        batch\n                    )\n                    total_time = time.time() - time_start\n                    (\n                        loss, chosen_rewards, rejected_rewards\n                    ) = metrics.loss, metrics.chosen_rewards[0], metrics.rejected_rewards[0]\n\n                    loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                    rejected_rewards_sum = (\n                        rejected_rewards.tolist() if (\n                                rejected_rewards_sum is None\n                        ) else rejected_rewards_sum + rejected_rewards\n                    )\n                    chosen_rewards_sum = (\n                        chosen_rewards.tolist() if (\n                                chosen_rewards_sum is None\n                        ) else chosen_rewards_sum + chosen_rewards\n                    )\n\n                    eval_metrics = {\n                        \"eval/loss\": loss.tolist(),\n                        \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                        \"eval/mean_rejected_rewards\": rejected_rewards_sum / (\n                                current_step - self.arguments.step_start_point\n                        ),\n                        \"eval/mean_chosen_rewards\": chosen_rewards_sum / (\n                                current_step - self.arguments.step_start_point\n                        ),\n                        \"eval/step\": current_step,\n                        \"eval/step_time\": total_time,\n                        \"eval/perplexity\": jnp.exp(loss).tolist(),\n                    }\n                    log_metrics = copy.deepcopy(eval_metrics)\n                    eval_metrics.update(self.arguments.captured_memory)\n                    if self.arguments.use_wandb:\n                        with jax.spmd_mode(\"allow_all\"):\n                            self.wandb_runtime.log(\n                                eval_metrics\n                            )\n\n                    pbar.update(1)\n                    pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                    yield eval_metrics\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n\n    def __repr__(self):\n\n        \"\"\"\n        The __repr__ function is used to generate a string representation of an object.\n        This function should return a string that can be parsed by the Python interpreter\n        to recreate the object. The __repr__ function is called when you use print() on an\n        object, or when you type its name in the REPL.\n\n        :param self: Refer to the instance of the class\n        :return: A string representation of the object\n        \"\"\"\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                try:\n                    repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n                except TypeError:\n                    repr_src = f\"\\t{k} : \" + \"EasyDeLReadingError\" + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n\n        return string + \")\"\n\n    def __str__(self):\n\n        \"\"\"\n        The __str__ function is called when you use the print function or when str() is used.\n        It should return a string representation of the object.\n\n        :param self: Refer to the instance of the class\n        :return: The object's string representation\n        \"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.__init__","title":"<code>__init__(arguments, model_state, ref_model_state=None, beta=0.1, label_smoothing=0.0, loss_type='sigmoid', label_pad_token_id=-100, padding_value=None, train_dataset=None, eval_dataset=None, tokenizer=None, data_collator=None, max_length=None, max_prompt_length=None, max_target_length=None, precompute_ref_log_probs=False, model_init_kwargs=None, ref_model_init_kwargs=None, reference_free=False, auto_shard_model_state=True, auto_shard_ref_model_state=True, is_encoder_decoder=False, dataset_map_arguments=None, low_mem_usage=True, auto_fix_data=True, _do_init_fns=True)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object.</p> <p>:param self: Refer to the object itself :param model_state: EasyDeLState | str: Pass the model state to the trainer :param ref_model_state: Optional[EasyDeLState | str]: Pass the reference model state :param beta: float: Control the strength of the regularization term :param label_smoothing: float: Smooth the labels :param loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] : Determine the loss function used :param arguments: TrainArguments: Pass the arguments to the trainer :param label_pad_token_id: int: Pad the labels :param padding_value: int: Specify the value that is used for padding :param train_dataset: Optional[Dataset]: Load the training dataset :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer :param max_length: Optional[int]: Set the maximum length of the input sequence :param max_prompt_length: Optional[int]: Set the maximum length of the prompt :param max_target_length: Optional[int]: Truncate the target sequence :param data_collator: Optional[Callable]: Function to be used for creating datasets. :param precompute_ref_log_probs: bool: Precompute the log probabilities of the reference model :param model_init_kwargs: Optional[Dict]: Pass in the model_kwargs to model for init process :param ref_model_init_kwargs: Optional[Dict]: Pass the ref_model_init_kwargs to ref_model for init process :param auto_shard_model_state: bool: whenever to automatically shard <code>model_state</code> :param auto_shard_ref_model_state: bool: whenever to automatically shard <code>ref_model_state</code> :param dataset_map_arguments: Optional[dict]: arguments to be passed to train and eval datasets for tokenizing process with <code>dataset.map</code>. :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure model with provided training Arguments :param : Set the padding value for the model</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def __init__(\n        self,\n        arguments: TrainArguments,\n        model_state: EasyDeLState | str,\n        ref_model_state: Optional[EasyDeLState | str] = None,\n        beta: float = 0.1,\n        label_smoothing: float = .0,\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] = \"sigmoid\",\n        label_pad_token_id: int = -100,\n        padding_value: int = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        data_collator: Optional[Callable] = None,\n        max_length: Optional[int] = None,\n        max_prompt_length: Optional[int] = None,\n        max_target_length: Optional[int] = None,\n        precompute_ref_log_probs: bool = False,\n        model_init_kwargs: Optional[Dict] = None,\n        ref_model_init_kwargs: Optional[Dict] = None,\n        reference_free: bool = False,\n        auto_shard_model_state: bool = True,\n        auto_shard_ref_model_state: bool = True,\n        is_encoder_decoder: Optional[bool] = False,\n        dataset_map_arguments: Optional[dict] = None,\n        low_mem_usage: bool = True,\n        auto_fix_data: bool = True,\n        _do_init_fns: bool = True,\n):\n\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object.\n\n\n    :param self: Refer to the object itself\n    :param model_state: EasyDeLState | str: Pass the model state to the trainer\n    :param ref_model_state: Optional[EasyDeLState | str]: Pass the reference model state\n    :param beta: float: Control the strength of the regularization term\n    :param label_smoothing: float: Smooth the labels\n    :param loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] : Determine the loss function used\n    :param arguments: TrainArguments: Pass the arguments to the trainer\n    :param label_pad_token_id: int: Pad the labels\n    :param padding_value: int: Specify the value that is used for padding\n    :param train_dataset: Optional[Dataset]: Load the training dataset\n    :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer\n    :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer\n    :param max_length: Optional[int]: Set the maximum length of the input sequence\n    :param max_prompt_length: Optional[int]: Set the maximum length of the prompt\n    :param max_target_length: Optional[int]: Truncate the target sequence\n    :param data_collator: Optional[Callable]: Function to be used for creating datasets.\n    :param precompute_ref_log_probs: bool: Precompute the log probabilities of the reference model\n    :param model_init_kwargs: Optional[Dict]: Pass in the model_kwargs to model for init process\n    :param ref_model_init_kwargs: Optional[Dict]: Pass the ref_model_init_kwargs to ref_model for init process\n    :param auto_shard_model_state: bool: whenever to automatically shard `model_state`\n    :param auto_shard_ref_model_state: bool: whenever to automatically shard `ref_model_state`\n    :param dataset_map_arguments: Optional[dict]: arguments to be passed to train and eval datasets for\n    tokenizing process with `dataset.map`.\n    :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure\n    model with provided training Arguments\n    :param : Set the padding value for the model\n    \"\"\"\n    assert arguments is not None, (\n        \"You Have to pass arguments that will be used for training but you have passed\"\n        \"`arguments=None`\"\n    )\n    assert isinstance(arguments, TrainArguments), (\n        f\"arguments type must be `TrainArguments` but got {type(arguments)}\"\n    )\n    if model_init_kwargs is None:\n        model_init_kwargs = {}\n    elif not isinstance(model_state, str):\n        raise ValueError(\"You passed model_kwargs to the DPOTrainer. But your model is already instantiated.\")\n\n    if ref_model_init_kwargs is None:\n        ref_model_init_kwargs = {}\n    elif not isinstance(ref_model_state, str):\n        raise ValueError(\n            \"You passed ref_model_kwargs to the DPOTrainer. But your ref_model is already instantiated.\"\n        )\n\n    if isinstance(model_state, str):\n        warnings.warn(\n            \"You passed a model_id to the DPOTrainer. This will automatically create an \"\n            \"`AutoEasyDeLModelForCausalLM` for you.\"\n        )\n        model_state = EasyDeLState.from_pretrained(\n            model_state,\n            **model_init_kwargs\n        )\n    if isinstance(ref_model_state, str):\n        warnings.warn(\n            \"You passed a ref model_id to the DPOTrainer. This will automatically create an \"\n            \"`AutoEasyDeLModelForCausalLM`\"\n        )\n        ref_model_state = EasyDeLState.from_pretrained(\n            ref_model_state,\n            **ref_model_init_kwargs\n        )\n\n    if loss_type in [\"hinge\", \"ipo\", \"kto_pair\"] and label_smoothing &gt; 0:\n        warnings.warn(\n            \"You are using a loss type that does not support label smoothing. Ignoring label_smoothing parameter.\"\n        )\n    self.auto_fix_data = auto_fix_data\n\n    if tokenizer is None:\n        raise ValueError(\"tokenizer must be specified to tokenize a DPO dataset.\")\n    if max_length is None:\n        warnings.warn(\n            \"`max_length` is not set in the DPOTrainer's init\"\n            \" it will default to `512` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_length = 512\n    if max_prompt_length is None:\n        warnings.warn(\n            \"`max_prompt_length` is not set in the DPOTrainer's init\"\n            \" it will default to `128` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_prompt_length = 128\n\n    if max_target_length is None and is_encoder_decoder:\n        warnings.warn(\n            \"When using an encoder decoder architecture, you should set `max_target_length` in the \"\n            \"DPOTrainer's init it will default to `128` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_target_length = 128\n\n    padding_value = padding_value if padding_value is not None else tokenizer.pad_token_id\n    self.max_length = max_length\n    self.label_pad_token_id = label_pad_token_id\n    self.padding_value = padding_value\n    self.max_prompt_length = max_prompt_length\n    self.truncation_mode = arguments.truncation_mode\n\n    self.max_target_length = max_target_length\n    self.tokenizer = tokenizer\n    self.precompute_ref_log_probs = precompute_ref_log_probs\n    self.reference_free = reference_free\n    self.is_encoder_decoder = False\n    self._precomputed_train_ref_log_probs = False\n    self._precomputed_eval_ref_log_probs = False\n    self.beta = beta\n    self.label_smoothing = label_smoothing\n    self.loss_type = loss_type\n    self.low_mem_usage = low_mem_usage\n    data_collator = DPODataCollatorWithPadding(\n        max_prompt_length=self.max_prompt_length,\n        max_target_length=self.max_target_length,\n        pad_token_id=tokenizer.pad_token_id,\n        label_pad_token_id=label_pad_token_id,\n        is_encoder_decoder=False,\n    ) if data_collator is None else data_collator\n    self._stored_metrics = defaultdict(lambda: defaultdict(list))\n    if dataset_map_arguments is None:\n        dataset_map_arguments = {}\n    train_dataset = train_dataset.map(\n        self.tokenize_row,\n        **dataset_map_arguments\n    )\n    if eval_dataset is not None:\n        eval_dataset = eval_dataset.map(\n            self.tokenize_row,\n            **dataset_map_arguments\n        )\n\n    self.arguments = arguments\n    self.hp_name = None\n    self.deepspeed = None\n    self.is_in_train = False\n\n    self.data_collator = data_collator\n    self.train_dataset = train_dataset\n    self.eval_dataset = eval_dataset\n    self.tokenizer = tokenizer\n    self.ref_model_state = ref_model_state\n    self.model_state = model_state\n    self._loggers_initialized = False\n    self.mesh = self.arguments.get_mesh()\n    assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n\n    self.concatenated_forward = create_concatenated_forward(\n        is_encoder_decoder=self.is_encoder_decoder,\n        padding_value=padding_value,\n        label_pad_token_id=label_pad_token_id,\n    )\n    self.auto_shard_ref_model_state = auto_shard_ref_model_state\n    self.auto_shard_model_state = auto_shard_model_state\n\n    self._cached_p_l_s = None\n    self._cached_c_l_s = None\n    self._cached_r_l_s = None\n    super().__init__(\n        arguments=arguments,\n        dataset_train=train_dataset,\n        dataset_eval=eval_dataset,\n        finetune=True,\n        checkpoint_path=None,\n        _do_init_fns=_do_init_fns\n    )\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is used to generate a string representation of an object. This function should return a string that can be parsed by the Python interpreter to recreate the object. The repr function is called when you use print() on an object, or when you type its name in the REPL.</p> <p>:param self: Refer to the instance of the class :return: A string representation of the object</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"\n    The __repr__ function is used to generate a string representation of an object.\n    This function should return a string that can be parsed by the Python interpreter\n    to recreate the object. The __repr__ function is called when you use print() on an\n    object, or when you type its name in the REPL.\n\n    :param self: Refer to the instance of the class\n    :return: A string representation of the object\n    \"\"\"\n    string = f\"{self.__class__.__name__}(\\n\"\n    for k, v in self.__dict__.items():\n        if not k.startswith(\"_\"):\n            try:\n                repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n            except TypeError:\n                repr_src = f\"\\t{k} : \" + \"EasyDeLReadingError\" + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n\n    return string + \")\"\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you use the print function or when str() is used. It should return a string representation of the object.</p> <p>:param self: Refer to the instance of the class :return: The object's string representation</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def __str__(self):\n\n    \"\"\"\n    The __str__ function is called when you use the print function or when str() is used.\n    It should return a string representation of the object.\n\n    :param self: Refer to the instance of the class\n    :return: The object's string representation\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.build_tokenized_answer","title":"<code>build_tokenized_answer(prompt, answer)</code>","text":"<p>Llama tokenizer does satisfy <code>enc(a + b) = enc(a) + enc(b)</code>. It does ensure <code>enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]</code>.</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def build_tokenized_answer(self, prompt, answer):\n    \"\"\"\n    Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n    It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n    \"\"\"\n\n    full_tokenized = self.tokenizer(prompt + answer, add_special_tokens=False)\n    prompt_input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n    answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids):]\n    answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids):]\n    prompt_input_ids = jnp.asarray(prompt_input_ids, dtype=\"i4\")\n    answer_input_ids = jnp.asarray(answer_input_ids, dtype=\"i4\")\n    full_concat_input_ids = jnp.concatenate(\n        (\n            prompt_input_ids,\n            answer_input_ids\n        )\n    )\n\n    # Prepare input tokens for token by token comparison\n    full_input_ids = jnp.array(full_tokenized[\"input_ids\"])\n\n    if len(full_input_ids) != len(full_concat_input_ids):\n        raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n\n    response_token_ids_start_idx = len(prompt_input_ids)\n    if prompt_input_ids.tolist() != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n        response_token_ids_start_idx -= 1\n\n    prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n    prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n\n    if len(prompt_input_ids) != len(prompt_attention_mask):\n        raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n\n    answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n    answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n\n    return dict(\n        prompt_input_ids=jnp.array(prompt_input_ids, dtype=\"i4\"),\n        prompt_attention_mask=jnp.array(prompt_attention_mask, dtype=\"i4\"),\n        input_ids=jnp.array(answer_input_ids, dtype=\"i4\"),\n        attention_mask=jnp.array(answer_attention_mask, dtype=\"i4\"),\n    )\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.compute_reference_log_probs","title":"<code>compute_reference_log_probs(state, padded_batch)</code>","text":"<p>Computes log probabilities of the reference model for a single padded batch of a DPO specific dataset.</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def compute_reference_log_probs(\n        self,\n        state: EasyDeLState,\n        padded_batch: Dict,\n) -&gt; tuple[Any, Any]:\n    \"\"\"\n    Computes log probabilities of the reference model for a single padded batch of a DPO specific dataset.\n    \"\"\"\n\n    if self.ref_model_state is None:\n        (\n            reference_chosen_log_probs,\n            reference_rejected_log_probs,\n            _,\n            _,\n        ) = self.concatenated_forward(\n            apply_fn=state.apply_fn,\n            params=state.params,\n            batch=padded_batch,\n        )\n    else:\n        (\n            reference_chosen_log_probs,\n            reference_rejected_log_probs,\n            _,\n            _,\n        ) = self.concatenated_forward(\n            apply_fn=self.ref_model_state.apply_fn,\n            params=self.ref_model_state.params,\n            batch=padded_batch,\n        )\n\n    return reference_chosen_log_probs, reference_rejected_log_probs\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.eval","title":"<code>eval(model_state)</code>","text":"<p>Evaluate the Given Model State and yield the eval metrics</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n    \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n    assert self.eval_dataset is not None, \"`dataloader_eval` is required by evaluator function.\"\n    with self.mesh:\n        pbar = tqdm(total=self.max_evaluation_steps)\n        pbar.set_description(\"Evaluating\")\n        current_step = 0\n        loss_sum = None\n        chosen_rewards_sum = None\n        rejected_rewards_sum = None\n\n        try:\n            for batch in self.dataloader_eval:\n                current_step += 1\n                time_start = time.time()\n                for key in self.arguments.ids_to_pop_from_dataset:\n                    _ = batch.pop(key, None)\n                for key in list(batch.keys()):\n                    if not (\n                            key.endswith(\"_input_ids\")\n                            or key.endswith(\"_attention_mask\")\n                            or key.endswith(\"_labels\")\n                    ):\n                        _ = batch.pop(key, None)\n\n                metrics = self.sharded_eval_step_function(\n                    model_state,\n                    batch\n                )\n                total_time = time.time() - time_start\n                (\n                    loss, chosen_rewards, rejected_rewards\n                ) = metrics.loss, metrics.chosen_rewards[0], metrics.rejected_rewards[0]\n\n                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                rejected_rewards_sum = (\n                    rejected_rewards.tolist() if (\n                            rejected_rewards_sum is None\n                    ) else rejected_rewards_sum + rejected_rewards\n                )\n                chosen_rewards_sum = (\n                    chosen_rewards.tolist() if (\n                            chosen_rewards_sum is None\n                    ) else chosen_rewards_sum + chosen_rewards\n                )\n\n                eval_metrics = {\n                    \"eval/loss\": loss.tolist(),\n                    \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                    \"eval/mean_rejected_rewards\": rejected_rewards_sum / (\n                            current_step - self.arguments.step_start_point\n                    ),\n                    \"eval/mean_chosen_rewards\": chosen_rewards_sum / (\n                            current_step - self.arguments.step_start_point\n                    ),\n                    \"eval/step\": current_step,\n                    \"eval/step_time\": total_time,\n                    \"eval/perplexity\": jnp.exp(loss).tolist(),\n                }\n                log_metrics = copy.deepcopy(eval_metrics)\n                eval_metrics.update(self.arguments.captured_memory)\n                if self.arguments.use_wandb:\n                    with jax.spmd_mode(\"allow_all\"):\n                        self.wandb_runtime.log(\n                            eval_metrics\n                        )\n\n                pbar.update(1)\n                pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                yield eval_metrics\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                color=\"cyan\",\n                force_color=True\n            )\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.get_eval_dataloader","title":"<code>get_eval_dataloader(eval_dataset=None)</code>","text":"<p>Returns the evaluation [<code>~tensorflow.data.Dataset</code>].</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n    \"\"\"\n    Returns the evaluation [`~tensorflow.data.Dataset`].\n    \"\"\"\n    if eval_dataset is None and self.eval_dataset is None:\n        raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n    eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n\n    if self.precompute_ref_log_probs and not self._precomputed_eval_ref_log_probs:\n\n        # prepare dataloader\n        data_loader = tensorflow_datasets.as_numpy(\n            eval_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=self.data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=False,\n                drop_remainder=True\n            )\n        )\n\n        reference_chosen_log_probs = []\n        reference_rejected_log_probs = []\n        for padded_batch in tqdm(iterable=data_loader, desc=\"Eval dataset reference log probs\"):\n            reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(\n                self.model_state,\n                padded_batch\n            )\n            reference_chosen_log_probs.append(reference_chosen_logp.cpu())\n            reference_rejected_log_probs.append(reference_rejected_logp.cpu())\n\n        all_reference_chosen_log_probs = jnp.concatenate(reference_chosen_log_probs)\n        all_reference_rejected_log_probs = jnp.concatenate(reference_rejected_log_probs)\n\n        eval_dataset = eval_dataset.add_column(name=\"reference_chosen_log_probs\",\n                                               column=all_reference_chosen_log_probs)\n        eval_dataset = eval_dataset.add_column(\n            name=\"reference_rejected_log_probs\", column=all_reference_rejected_log_probs\n        )\n\n        if self.eval_dataset is not None:\n            self.eval_dataset = eval_dataset\n        self._precomputed_eval_ref_log_probs = True\n\n    return self._get_eval_dataloader(eval_dataset=eval_dataset)\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.get_train_dataloader","title":"<code>get_train_dataloader()</code>","text":"<p>Returns the training [<code>~tensorflow.data.Dataset</code>].</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def get_train_dataloader(\n        self,\n) -&gt; tensorflow.data.Dataset:\n    \"\"\"\n    Returns the training [`~tensorflow.data.Dataset`].\n    \"\"\"\n\n    if self.precompute_ref_log_probs and not self._precomputed_train_ref_log_probs:\n\n        data_loader = tensorflow_datasets.as_numpy(\n            self.train_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=self.data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=False,\n                drop_remainder=True\n            )\n        )\n        reference_chosen_log_probs = []\n        reference_rejected_log_probs = []\n        for padded_batch in tqdm(iterable=data_loader, desc=\"Train dataset reference log probs\"):\n            reference_chosen_logp, reference_rejected_logp = self.compute_reference_log_probs(\n                self.model_state,\n                padded_batch,\n            )\n            reference_chosen_log_probs.append(reference_chosen_logp)\n            reference_rejected_log_probs.append(reference_rejected_logp)\n\n        all_reference_chosen_log_probs = jnp.concatenate(reference_chosen_log_probs)\n        all_reference_rejected_log_probs = jnp.concatenate(reference_rejected_log_probs)\n        self.train_dataset = self.train_dataset.add_column(\n            name=\"reference_chosen_log_probs\", column=all_reference_chosen_log_probs\n        )\n        self.train_dataset = self.train_dataset.add_column(\n            name=\"reference_rejected_log_probs\", column=all_reference_rejected_log_probs\n        )\n\n        self._precomputed_train_ref_log_probs = True\n    return self._get_train_dataloader()\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.initialize_trainer_utils","title":"<code>initialize_trainer_utils()</code>","text":"The initialize_trainer_utils function is responsible for initializing the following <ul> <li>wandb_runtime (if you use_wandb is True)</li> <li>timer object (for logging time taken by various functions)</li> <li>dataloader objects for training and evaluation data, along with max steps per epoch.   The configure_dataloader function accomplishes this task.</li> </ul> <p>:param self: Represent the instance of the class :return: A tuple of functions</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def initialize_trainer_utils(self):\n    \"\"\"\n    The initialize_trainer_utils function is responsible for initializing the following:\n        - wandb_runtime (if you use_wandb is True)\n        - timer object (for logging time taken by various functions)\n        - dataloader objects for training and evaluation data, along with max steps per epoch.\n          The configure_dataloader function accomplishes this task.\n\n    :param self: Represent the instance of the class\n    :return: A tuple of functions\n\n    \"\"\"\n    self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n    self.timer = Timers(\n        use_wandb=False,\n        tensorboard_writer=self.arguments.get_board()\n    )\n\n    self.timer(\"configure dataloaders\").start()\n    dataset_configurations = self.configure_dataloader()\n    self.dataloader_train = dataset_configurations.dataloader_train\n    self.max_training_steps = dataset_configurations.max_training_steps\n    self.dataloader_eval = dataset_configurations.dataloader_eval\n    self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n    self.timer(\"configure dataloaders\").stop()\n\n    self.timer.log([\"configure dataloaders\"])\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n    model_configurations = self.configure_model()\n    model = model_configurations.model\n    tx = model_configurations.tx\n    scheduler = model_configurations.scheduler\n    config = model_configurations.config\n    self.model = model\n    self.tx = tx\n    self.scheduler = scheduler\n    self.config = config\n    if self.rapture is not None:\n        lora_modules = self.rapture.apply_lora(\n            module=model,\n            parameters=self.arguments.rapture_config.parameters,\n            tx=tx,\n        )\n        self.lora_parameters = lora_modules.lora_parameters\n        self.lora_apply_fn = lora_modules.lora_module.__call__\n        self.lora_opt_state = lora_modules.lora_opt_state\n        self.lora_model = lora_modules.lora_module\n        self.lora_tx = lora_modules.lora_tx\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n    self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n\n    self.timer(\"configure functions and sharding them\").start()\n\n    if self.auto_shard_model_state:\n        self.timer(\"Sharding Model State\").start()\n        self.model_state: EasyDeLState = self.shard_states(\n            self.model_state,\n            self.model_state.module.config.get_partition_rules(self.arguments.fully_sharded_data_parallel)\n        )\n\n        termcolor.cprint(\"initializing TX and Schedulers for `model_state`\", force_color=True, color=\"cyan\")\n\n        params_with_opt = (\n            self.model_state.params[\n                'params'\n            ] if '_overwrite_with_gradient' in self.model_state.params else self.model_state.params\n        )\n        opt_state = self.tx.init(params_with_opt)\n\n        self.model_state = self.model_state.replace(\n            opt_state=opt_state,\n            tx=self.tx\n        )\n\n        self.timer(\"Sharding Model State\").stop()\n        self.timer.log([\"Sharding Model State\"])\n    if self.auto_shard_ref_model_state and self.ref_model_state is not None:\n        self.timer(\"Sharding Ref Model State\").start()\n        self.ref_model_state = self.shard_states(\n            self.ref_model_state,\n            self.ref_model_state.module.config.get_partition_rules(self.arguments.fully_sharded_data_parallel)\n        )\n        self.timer(\"Sharding Ref Model State\").stop()\n        self.timer.log([\"Sharding Ref Model State\"])\n\n    function_configurations = self.configure_functions()\n    self.create_sharded_state_from_params_function = (\n        function_configurations.create_sharded_state_from_params_function\n    )\n    self.sharded_train_step_function = function_configurations.sharded_train_step_function\n    self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n    self.mesh = function_configurations.mesh\n    self.checkpoint_manager = function_configurations.checkpoint_manager\n    self.initialize_state_function = function_configurations.initialize_state_function\n    self.timer(\"configure functions and sharding them\").stop()\n    self.timer.log([\"configure functions and sharding them\"])\n</code></pre>"},{"location":"generated-trainer-dpo-dpo_trainer/#src.python.easydel.trainer.dpo.dpo_trainer.DPOTrainer.tokenize_row","title":"<code>tokenize_row(feature, state=None)</code>","text":"<p>The tokenize_row function is responsible for taking a single row of data and converting it into the format that the model expects. This includes: - Tokenizing the text (using HuggingFace's tokenizer) - Padding/truncating sequences to a fixed length (if necessary) - Creating attention masks, which tell the model which tokens are padding and which aren't.</p> <p>:param self: Represent the instance of the class :param feature: Pass in the data from the dataset :param state: EasyDeLState: Keep track of the state of the tokenizer :return: A dictionary of the following keys</p> Source code in <code>src/python/easydel/trainer/dpo/dpo_trainer.py</code> <pre><code>def tokenize_row(self, feature, state: EasyDeLState = None) -&gt; Dict:\n\n    \"\"\"\n    The tokenize_row function is responsible for taking a single row of data and converting it into the format that\n    the model expects. This includes:\n    - Tokenizing the text (using HuggingFace's tokenizer)\n    - Padding/truncating sequences to a fixed length (if necessary)\n    - Creating attention masks, which tell the model which tokens are padding and which aren't.\n\n    :param self: Represent the instance of the class\n    :param feature: Pass in the data from the dataset\n    :param state: EasyDeLState: Keep track of the state of the tokenizer\n    :return: A dictionary of the following keys\n    \"\"\"\n    batch = {}\n    prompt = feature[\"prompt\"]\n    chosen = feature[\"chosen\"]\n    rejected = feature[\"rejected\"]\n\n    if not isinstance(prompt, str):\n        raise ValueError(f\"prompt should be an str but got {type(prompt)} , {prompt}\")\n    prompt_tokens = self.tokenizer(\n        prompt,\n        add_special_tokens=False,\n        return_tensors=\"np\",\n    )\n    prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n\n    if not isinstance(chosen, str):\n        raise ValueError(f\"chosen should be an str but got {type(chosen)} , {chosen}\")\n    chosen_tokens = self.build_tokenized_answer(prompt, chosen)\n\n    if not isinstance(rejected, str):\n        raise ValueError(f\"rejected should be an str but got {type(rejected)}\")\n    rejected_tokens = self.build_tokenized_answer(prompt, rejected)\n    v2d = lambda ar: ar.reshape(1, -1) if ar.ndim == 1 else ar\n\n    def add_tkn(n, ar):\n        return jnp.concatenate(\n            (\n                jnp.array(n).reshape(1, 1),\n                v2d(ar)\n            ), axis=-1\n        )\n\n    def add_post_tkn(n, ar):\n        return jnp.concatenate(\n            (\n                v2d(ar),\n                jnp.array(n).reshape(1, 1)\n            ), axis=-1\n        )\n\n    prompt_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        prompt_tokens[\"prompt_input_ids\"]\n    )\n    chosen_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        chosen_tokens[\"prompt_input_ids\"]\n    )\n    rejected_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        rejected_tokens[\"prompt_input_ids\"]\n    )\n\n    prompt_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, prompt_tokens[\"prompt_attention_mask\"]\n    )\n    chosen_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, chosen_tokens[\"prompt_attention_mask\"]\n    )\n    rejected_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, rejected_tokens[\"prompt_attention_mask\"]\n    )\n\n    # add EOS token to end of answer\n    chosen_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, chosen_tokens[\"input_ids\"])\n    chosen_tokens[\"attention_mask\"] = add_post_tkn(1, chosen_tokens[\"attention_mask\"])\n\n    rejected_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, rejected_tokens[\"input_ids\"])\n    rejected_tokens[\"attention_mask\"] = add_post_tkn(1, rejected_tokens[\"attention_mask\"])\n\n    longer_response_length = max(chosen_tokens[\"input_ids\"].shape[-1], rejected_tokens[\"input_ids\"].shape[-1])\n\n    # if combined sequence is too long, truncate the prompt\n    for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n        length_rn = answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length\n        if length_rn &gt; self.max_length:\n\n            if self.truncation_mode == \"keep_start\":\n                for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, : self.max_prompt_length]\n            elif self.truncation_mode == \"keep_end\":\n                for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, -self.max_prompt_length:]\n            else:\n                raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n    # if that's still too long, truncate the response\n    for answer_tokens in [chosen_tokens, rejected_tokens]:\n        if answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length &gt; self.max_length:\n            for k in [\"input_ids\", \"attention_mask\"]:\n                answer_tokens[k] = answer_tokens[k][:, : self.max_length - self.max_prompt_length]\n\n    chosen_sequence_tokens = {\n        k: jnp.concatenate(\n            (v2d(chosen_tokens[f\"prompt_{k}\"]), v2d(chosen_tokens[k])),\n            axis=-1\n        ) for k in [\"input_ids\", \"attention_mask\"]\n    }\n    rejected_sequence_tokens = {\n        k: jnp.concatenate(\n            (v2d(rejected_tokens[f\"prompt_{k}\"]), v2d(rejected_tokens[k])),\n            axis=-1\n        ) for k in [\"input_ids\", \"attention_mask\"]\n    }\n    chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n    chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"labels\"].at[\n                                       : len(chosen_tokens[\"prompt_input_ids\"])\n                                       ].set([self.label_pad_token_id] * len(chosen_tokens[\"prompt_input_ids\"]))\n    rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n    rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"labels\"].at[\n                                         : len(rejected_tokens[\"prompt_input_ids\"])\n                                         ].set(\n        ([self.label_pad_token_id] * len(rejected_tokens[\"prompt_input_ids\"]))\n    )\n\n    for k, tokens_ in {\n        \"chosen_\": chosen_sequence_tokens,\n        \"rejected_\": rejected_sequence_tokens,\n        \"\": prompt_tokens,\n    }.items():\n        for type_key, tokens in tokens_.items():\n            if type_key == \"token_type_ids\":\n                continue\n\n            b, s = tokens.shape\n\n            if self.max_prompt_length &gt; s:\n                if k == \"chosen_\":\n                    if type_key == \"input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n\n                    tokens = tokens[..., :self.max_target_length]\n\n                    if tokens.shape[-1] != self.max_target_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_target_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n                    tokens = tokens[..., :self.max_target_length]\n                elif k == \"rejected_\":\n                    if type_key == \"input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_target_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    tokens = tokens[..., :self.max_target_length]\n                    if tokens.shape[-1] != self.max_target_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_target_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n                elif k == \"\":\n                    if type_key == \"prompt_input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"prompt_attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"prompt_labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    tokens = tokens[..., :self.max_prompt_length]\n                    if tokens.shape[-1] != self.max_prompt_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_prompt_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n            batch[f\"{k}{type_key}\"] = tokens\n    return batch\n</code></pre>"},{"location":"generated-trainer-dpo-fwd_bwd_functions/","title":"trainer.dpo.fwd_bwd_functions","text":""},{"location":"generated-trainer-dpo-fwd_bwd_functions/#src.python.easydel.trainer.dpo.fwd_bwd_functions.concatenated_inputs","title":"<code>concatenated_inputs(batch, is_encoder_decoder=False, label_pad_token_id=-100, padding_value=0, truncation_mode='keep_end', fixed_max_length=None)</code>","text":"<p>The concatenated_inputs function takes a batch of chosen and rejected examples, and concatenates them together. This is useful for training the model to predict whether an example was chosen by the human annotator. The function also pads all inputs to the same length as the longest input in that batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Union[List, Array]]</code> <p>Dict[str,Union[List,chex.Array]]: Pass the batch of data into the function,</p> required <code>is_encoder_decoder</code> <code>bool</code> <p>bool: Determine whether the model is an encoder-decoder model</p> <code>False</code> <code>label_pad_token_id</code> <code>int</code> <p>int: Pad the labels with a value of -100</p> <code>-100</code> <code>padding_value</code> <code>int</code> <p>int: Pad the input_ids and attention_mask arrays to the same length</p> <code>0</code> <code>truncation_mode</code> <code>Literal['keep_end', 'keep_start']</code> <p>typing.Literal[\"keep_end\", \"keep_start\"]: is left padded or not should it keep start of the</p> <code>'keep_end'</code> <code>fixed_max_length</code> <code>int | None</code> <p>int|None: by providing fixed_max_length the func will always return a fixed sequence length and won't use dynamic methods.</p> <code>None</code> <p>Allow for the batch to be a list of arrays or just an array, Specify the type of data that is being passed in</p> <p>array or the end of the array?.</p> <p>Returns:</p> Type Description <code>Dict[str, Array]</code> <p>A dictionary of the concatenated inputs</p> Source code in <code>src/python/easydel/trainer/dpo/fwd_bwd_functions.py</code> <pre><code>def concatenated_inputs(\n        batch: Dict[str, Union[List, chex.Array]],\n        is_encoder_decoder: bool = False,\n        label_pad_token_id: int = -100,\n        padding_value: int = 0,\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n        fixed_max_length: int | None = None\n) -&gt; Dict[str, chex.Array]:\n    \"\"\"The concatenated_inputs function takes a batch of chosen and rejected examples,\n    and concatenates them together. This is useful for training the model to predict whether an example was chosen\n    by the human annotator. The function also pads all inputs to\n    the same length as the longest input in that batch.\n\n    Args:\n        batch: Dict[str,Union[List,chex.Array]]: Pass the batch of data\n            into the function,\n        is_encoder_decoder: bool: Determine whether the model is an\n            encoder-decoder model\n        label_pad_token_id: int: Pad the labels with a value of -100\n        padding_value: int: Pad the input_ids and attention_mask arrays\n            to the same length\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"]: is\n            left padded or not should it keep start of the\n        fixed_max_length: int|None: by providing fixed_max_length the\n            func will always return a fixed sequence length and won't\n            use dynamic methods.\n    Allow for the batch to be a list of arrays or just an array,\n    Specify the type of data that is being passed in\n\n    array or the end of the array?.\n\n    Returns:\n        A dictionary of the concatenated inputs\n    \"\"\"\n    concatenated_batch = {}\n    if fixed_max_length is None:\n        if is_encoder_decoder:\n            max_length = max(batch[\"chosen_labels\"].shape[-1], batch[\"rejected_labels\"].shape[-1])\n        else:\n            max_length = max(batch[\"chosen_input_ids\"].shape[-1], batch[\"rejected_input_ids\"].shape[-1])\n    else:\n        max_length = fixed_max_length\n    for k in batch:\n        if k.startswith(\"chosen\") and isinstance(batch[k], jax.Array):\n            if \"labels\" in k or is_encoder_decoder:\n                pad_value = label_pad_token_id\n            elif k.endswith(\"_input_ids\"):\n                pad_value = padding_value\n            elif k.endswith(\"_attention_mask\"):\n                pad_value = 0\n            else:\n                raise KeyError(\"couldn't find pad_value [Dataset Issue]\")\n            concatenated_key = k.replace(\"chosen\", \"concatenated\")\n            concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n    for k in batch:\n        if k.startswith(\"rejected\") and isinstance(batch[k], jax.Array):\n            if \"labels\" in k or is_encoder_decoder:\n                pad_value = label_pad_token_id\n            elif k.endswith(\"_input_ids\"):\n                assert padding_value is not None, \"`padding_value` can not be set as `None`\"\n                pad_value = padding_value\n            elif k.endswith(\"_attention_mask\"):\n                pad_value = 0\n            else:\n                raise KeyError(\"couldn't find pad_value [Dataset Issue]\")\n            concatenated_key = k.replace(\"rejected\", \"concatenated\")\n            v2d = lambda ar: ar.reshape(ar.shape[0], -1)\n            concatenated_batch[concatenated_key] = jnp.concatenate(\n                (\n                    v2d(concatenated_batch[concatenated_key]),\n                    pad_to_length(v2d(batch[k]), max_length, pad_value=pad_value),\n                ),\n                axis=0,\n            )\n    for k in list(concatenated_batch.keys()):\n        val = concatenated_batch[k]\n        if val.ndim == 3:\n            # making 3d array 2d\n            concatenated_batch[k] = val.reshape(val.shape[0], -1)\n    if is_encoder_decoder:\n        concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1)\n        concatenated_batch[\"concatenated_attention_mask\"] = (\n            batch[\"prompt_attention_mask\"].repeat(2, 1)\n        )\n\n    return concatenated_batch\n</code></pre>"},{"location":"generated-trainer-dpo-fwd_bwd_functions/#src.python.easydel.trainer.dpo.fwd_bwd_functions.create_concatenated_forward","title":"<code>create_concatenated_forward(is_encoder_decoder, label_pad_token_id, padding_value, truncation_mode='keep_end', fixed_max_length=None)</code>","text":"<p>The create_concatenated_forward function is a helper function that creates a forward pass function for the model. The forward pass function takes in an apply_fn, which is the model's apply_fn, and runs it on concatenated inputs. It returns chosen log probs, rejected log probs, chosen logits and rejected logits.</p> <p>Parameters:</p> Name Type Description Default <code>is_encoder_decoder</code> <p>Determine whether the model is an encoder- decoder model or not</p> required <code>label_pad_token_id</code> <p>Pad the labels to the same length</p> required <code>padding_value</code> <p>Pad the inputs to the same length</p> required <code>truncation_mode</code> <code>Literal['keep_end', 'keep_start']</code> <p>typing.Literal[\"keep_end\",\"keep_start\"]: where to pad and where to keep.</p> <code>'keep_end'</code> <code>fixed_max_length</code> <code>int | None</code> <p>int|None: by providing fixed_max_length the func will always return a fixed sequence length</p> <code>None</code> <p>and won't use dynamic methods.</p> <p>Returns:</p> Type Description <p>A function that takes in a apply_fn, params and a batch of</p> <p>inputs,</p> Source code in <code>src/python/easydel/trainer/dpo/fwd_bwd_functions.py</code> <pre><code>def create_concatenated_forward(\n        is_encoder_decoder,\n        label_pad_token_id,\n        padding_value,\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n        fixed_max_length: int | None = None\n):\n    \"\"\"The create_concatenated_forward function is a helper function that creates a forward pass function for the\n    model. The forward pass function takes in an apply_fn, which is the model's apply_fn, and runs it on concatenated\n    inputs. It returns chosen log probs, rejected log probs, chosen logits and rejected logits.\n\n    Args:\n        is_encoder_decoder: Determine whether the model is an encoder-\n            decoder model or not\n        label_pad_token_id: Pad the labels to the same length\n        padding_value: Pad the inputs to the same length\n        truncation_mode: typing.Literal[\"keep_end\",\"keep_start\"]: where\n            to pad and where to keep.\n        fixed_max_length: int|None: by providing fixed_max_length the\n            func will always return a fixed sequence length\n    and won't use dynamic methods.\n\n    Returns:\n        A function that takes in a apply_fn, params and a batch of\n        inputs,\n    \"\"\"\n\n    def concatenated_forward(\n            apply_fn: Callable,\n            params: dict | flax.core.FrozenDict,\n            batch: Dict[str, Union[List, chex.Array]]\n\n    ) -&gt; Tuple[chex.Array, chex.Array, chex.Array, chex.Array]:\n        \"\"\"The concatenated_forward function is used to compute the log-probabilities of both chosen and rejected labels.\n\n        Args:\n            apply_fn: Callable: Pass in the model function\n            params: dict | flax.core.FrozenDict: Pass the model\n                parameters to the function\n            batch: Dict[str, Union[List, chex.Array]] : Pass the batch\n                of data to the concatenated_forward function\n\n        Returns:\n            The log_probs of the chosen and rejected labels, as well as\n            their corresponding logits\n        \"\"\"\n        assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n        concatenated_batch = concatenated_inputs(\n            batch,\n            is_encoder_decoder=is_encoder_decoder,\n            label_pad_token_id=label_pad_token_id,\n            padding_value=padding_value,\n            truncation_mode=truncation_mode,\n            fixed_max_length=fixed_max_length\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n        concatenated_batch[\"concatenated_input_ids\"] = concatenated_batch[\"concatenated_input_ids\"].reshape(\n            concatenated_batch[\"concatenated_input_ids\"].shape[0], -1\n        )\n        concatenated_batch[\"concatenated_labels\"] = concatenated_batch[\"concatenated_labels\"].reshape(\n            concatenated_batch[\"concatenated_labels\"].shape[0], -1\n        )\n        concatenated_batch[\"concatenated_attention_mask\"] = concatenated_batch[\"concatenated_attention_mask\"].reshape(\n            concatenated_batch[\"concatenated_attention_mask\"].shape[0], -1\n        )\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if is_encoder_decoder\n            else {}\n        )\n        all_logits = apply_fn(\n            concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n            params=params,\n            **model_kwargs,\n        ).logits\n\n        all_log_probs = get_batch_log_probs(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=False,\n            is_encoder_decoder=is_encoder_decoder,\n            label_pad_token_id=label_pad_token_id,\n        )\n\n        chosen_log_probs = all_log_probs[:len_chosen]\n        rejected_log_probs = all_log_probs[len_chosen:]\n\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n\n        return chosen_log_probs, rejected_log_probs, chosen_logits, rejected_logits\n\n    return concatenated_forward\n</code></pre>"},{"location":"generated-trainer-dpo-fwd_bwd_functions/#src.python.easydel.trainer.dpo.fwd_bwd_functions.create_dpo_eval_function","title":"<code>create_dpo_eval_function(concatenated_forward, ref_state=None, beta=0.1, label_smoothing=0, loss_type='sigmoid', reference_free=False)</code>","text":"<p>The create_dpo_eval_function function is a helper function that creates the DPO evaluating step.</p> <p>Parameters:</p> Name Type Description Default <code>concatenated_forward</code> <code>Callable</code> <p>Callable: Define the forward pass of the model</p> required <code>ref_state</code> <code>EasyDeLState</code> <p>EasyDeLState: Specify the reference policy</p> <code>None</code> <code>beta</code> <code>float</code> <p>float: Scale the logits</p> <code>0.1</code> <code>label_smoothing</code> <code>float</code> <p>float: Smooth the labels</p> <code>0</code> <code>loss_type</code> <code>Literal['sigmoid', 'hinge', 'ipo', 'kto']</code> <p>Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"]: Determine the loss function</p> <code>'sigmoid'</code> <code>reference_free</code> <code>bool</code> <p>bool: Indicate whether the reference policy is used or not</p> <code>False</code> <p>Returns:</p> Type Description <p>A function that takes in a state and a batch</p> Source code in <code>src/python/easydel/trainer/dpo/fwd_bwd_functions.py</code> <pre><code>def create_dpo_eval_function(\n        concatenated_forward: Callable,\n        ref_state: EasyDeLState = None,\n        beta: float = 0.1,\n        label_smoothing: float = 0,\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] = \"sigmoid\",\n        reference_free: bool = False,\n):\n    \"\"\"The create_dpo_eval_function function is a helper function that creates the DPO evaluating step.\n\n    Args:\n        concatenated_forward: Callable: Define the forward pass of the\n            model\n        ref_state: EasyDeLState: Specify the reference policy\n        beta: float: Scale the logits\n        label_smoothing: float: Smooth the labels\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"]: Determine\n            the loss function\n        reference_free: bool: Indicate whether the reference policy is\n            used or not\n\n    Returns:\n        A function that takes in a state and a batch\n    \"\"\"\n\n    def _sigmoid_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array = None,  # IGNORED\n            reference_chosen_log_probs: chex.Array = None,  # IGNORED\n            policy_rejected_log_probs: chex.Array = None,  # IGNORED\n            reference_rejected_log_probs: chex.Array = None  # IGNORED\n    ):\n\n        \"\"\"The _sigmoid_dpo_loss function is a helper function for the sigmoid_dpo_loss\n            function. It computes the loss of each example in a batch, given its logits\n            and (optionally) its chosen/rejected log probabilities under both policies.\n\n        Args:\n            logits: chex.Array: Compute the loss\n            policy_chosen_log_probs: chex.Array: Calculate the policy\n                loss\n            reference_chosen_log_probs: chex.Array: Compute the loss for\n                the reference policy # IGNORED\n            policy_rejected_log_probs: chex.Array: Calculate the loss\n                for the rejected samples # IGNORED\n            reference_rejected_log_probs: chex.Array: Calculate the loss\n                of rejected samples # IGNORED\n\n        Returns:\n            an array represent loss\n        \"\"\"\n        losses = (\n                -jax.nn.log_sigmoid(beta * logits) * (1 - label_smoothing)\n                - jax.nn.log_sigmoid(-beta * logits) * label_smoothing\n        )\n        return losses\n\n    def _hinge_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array,  # IGNORED\n            reference_chosen_log_probs: chex.Array,  # IGNORED\n            policy_rejected_log_probs: chex.Array,  # IGNORED\n            reference_rejected_log_probs: chex.Array  # IGNORED\n    ):\n\n        \"\"\"The _hinge_dpo_loss function is a helper function that computes the loss for DPO.\n\n        Args:\n            logits: chex.Array: Calculate the hinge loss\n            policy_chosen_log_probs: chex.Array: Compute the policy loss\n            reference_chosen_log_probs: chex.Array: Compute the loss for\n                the reference policy # IGNORED\n            policy_rejected_log_probs: chex.Array: Calculate the loss\n                for the rejected samples # IGNORED\n            reference_rejected_log_probs: chex.Array: Calculate the loss\n                of rejected samples # IGNORED\n\n        Returns:\n            an array represent The hinge loss\n        \"\"\"\n        return jax.relu(1 - beta * logits)\n\n    def _ipo_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array,  # IGNORED\n            reference_chosen_log_probs: chex.Array,  # IGNORED\n            policy_rejected_log_probs: chex.Array,  # IGNORED\n            reference_rejected_log_probs: chex.Array  # IGNORED\n    ):\n        \"\"\"The _ipo_dpo_loss function is a helper function that calculates the loss for\n        the IPO-DPO algorithm. It takes in the logits, policy_chosen_log_probs,\n        reference_chosen_log_probs, policy rejected log probs and reference rejected\n        log probs as inputs. The output of this function is used to calculate the loss\n        for each batch of data.\n\n                :param logits: chex.Array: Calculate the loss\n                :param policy_chosen_log_probs: chex.Array: Compute the\n                :param reference_chosen_log_probs: chex.Array: Compute the loss for the reference policy # IGNORED\n                :param policy_rejected_log_probs: chex.Array: Calculate the loss for the rejected samples # IGNORED\n                :param reference_rejected_log_probs: chex.Array: Calculate the loss of rejected samples # IGNORED\n                :return: an array represent loss\n        \"\"\"\n        return (logits - 1 / (2 * beta)) ** 2\n\n    def _kto_pair_dpo_loss(\n            logits: chex.Array,  # IGNORED\n            policy_chosen_log_probs: chex.Array,\n            reference_chosen_log_probs: chex.Array,\n            policy_rejected_log_probs: chex.Array,\n            reference_rejected_log_probs: chex.Array\n    ):\n\n        \"\"\"The _kto_pair_dpo_loss function is a helper function that computes the loss for\n        a single pair of trajectories. It takes in two sets of log probabilities, one from\n        the policy and one from the reference distribution. The first set are the log\n        probabilities for actions taken by each agent in a trajectory, while the second set\n        are those for actions not taken by each agent (i.e., rejected). The function then\n        computes KL divergences between these two sets of distributions and uses them to compute losses.\n\n        Args:\n            logits: chex.Array: Calculate the log_probs\n            policy_chosen_log_probs: chex.Array: Calculate the chosen_kl\n                # IGNORED\n            reference_chosen_log_probs: chex.Array: Calculate the\n                chosen_kl\n            policy_rejected_log_probs: chex.Array: Calculate the\n                rejected_kl variable\n            reference_rejected_log_probs: chex.Array: Calculate the\n                rejected_kl variable\n\n        Returns:\n            an array represent loss\n        \"\"\"\n        chosen_kl = jax.lax.clamp(\n            min=0,\n            x=jnp.mean(policy_chosen_log_probs - reference_chosen_log_probs),\n            max=1e9\n        )\n        rejected_kl = jax.lax.clamp(\n            min=0,\n            x=jnp.mean(policy_rejected_log_probs - reference_rejected_log_probs),\n            max=1e9\n        )\n\n        chosen_log_ratios = policy_chosen_log_probs - reference_chosen_log_probs\n        rejected_log_ratios = policy_rejected_log_probs - reference_rejected_log_probs\n        losses = jnp.concatenate(\n            (\n                1 - jax.nn.sigmoid(beta * (chosen_log_ratios - rejected_kl)),\n                1 - jax.nn.sigmoid(beta * (chosen_kl - rejected_log_ratios)),\n            ),\n            0,\n        )\n\n        return losses\n\n    if loss_type == \"sigmoid\":\n        _loss_func = _sigmoid_dpo_loss\n    elif loss_type == \"hinge\":\n        _loss_func = _hinge_dpo_loss\n    elif loss_type == \"ipo\":\n        _loss_func = _ipo_dpo_loss\n    elif loss_type == \"kto_pair\":\n        _loss_func = _kto_pair_dpo_loss\n    else:\n        raise ValueError(f\"UnKnown loss_type {loss_type}\")\n\n    def dpo_step(\n            state: EasyDeLState,\n            batch: dict\n    ) -&gt; DPOStepOut:\n\n        \"\"\"The dpo_step function is the core of DPO. It takes a state and a batch,\n        and returns an updated state. The update is done by calculating the loss\n        for each example in the batch, then taking its gradient with respect to\n        the parameters of the policy network (which are stored in `state`). This\n        gradient is then used to update `state`.\n\n        Args:\n            state: EasyDeLState: Store the parameters of the model\n            batch: dict: Pass the data to the model\n\n        Returns:\n            A `DPOStepOut` class\n        \"\"\"\n\n        def calculate_loss(params: dict | flax.core.FrozenDict):\n            (\n                policy_chosen_log_probs,\n                policy_rejected_log_probs,\n                policy_chosen_logits,\n                policy_rejected_logits,\n            ) = concatenated_forward(\n                state.apply_fn,\n                params,\n                batch\n            )\n\n            if \"reference_chosen_log_probs\" in batch and \"reference_rejected_log_probs\" in batch:\n                reference_chosen_log_probs = batch[\"reference_chosen_log_probs\"]\n                reference_rejected_log_probs = batch[\"reference_rejected_log_probs\"]\n            else:\n                if ref_state is None:\n                    (\n                        reference_chosen_log_probs,\n                        reference_rejected_log_probs,\n                        _,\n                        _,\n                    ) = concatenated_forward(\n                        state.apply_fn,\n                        state.params,\n                        batch\n                    )\n                else:\n                    (\n                        reference_chosen_log_probs,\n                        reference_rejected_log_probs,\n                        _,\n                        _,\n                    ) = concatenated_forward(\n                        ref_state.apply_fn,\n                        ref_state.params,\n                        batch\n                    )\n\n            pi_log_ratios = policy_chosen_log_probs - policy_rejected_log_probs\n\n            if reference_free:\n                ref_log_ratios = 0\n            else:\n                ref_log_ratios = reference_chosen_log_probs - reference_rejected_log_probs\n\n            logits = pi_log_ratios - ref_log_ratios\n            losses = _loss_func(\n                logits,\n                policy_chosen_log_probs,\n                reference_chosen_log_probs,\n                policy_rejected_log_probs,\n                reference_rejected_log_probs\n            )\n            chosen_rewards = (\n                    beta\n                    * (\n                            policy_chosen_log_probs - reference_chosen_log_probs\n                    )\n            )\n            rejected_rewards = (\n                    beta\n                    * (\n                            policy_rejected_log_probs\n                            - reference_rejected_log_probs\n                    )\n            )\n            return losses[0], (chosen_rewards, rejected_rewards)\n\n        __loss, (__chosen_rewards, __rejected_rewards) = calculate_loss(state.params)\n\n        return DPOStepOut(\n            loss=__loss,\n            rejected_rewards=__rejected_rewards,\n            chosen_rewards=__chosen_rewards\n        )\n\n    return dpo_step\n</code></pre>"},{"location":"generated-trainer-dpo-fwd_bwd_functions/#src.python.easydel.trainer.dpo.fwd_bwd_functions.create_dpo_train_function","title":"<code>create_dpo_train_function(concatenated_forward, ref_state=None, beta=0.1, label_smoothing=0, loss_type='sigmoid', reference_free=False)</code>","text":"<p>The create_dpo_train_function function is a helper function that creates the DPO training step.</p> <p>Parameters:</p> Name Type Description Default <code>concatenated_forward</code> <code>Callable</code> <p>Callable: Define the forward pass of the model</p> required <code>ref_state</code> <code>EasyDeLState</code> <p>EasyDeLState: Specify the reference policy</p> <code>None</code> <code>beta</code> <code>float</code> <p>float: Scale the logits</p> <code>0.1</code> <code>label_smoothing</code> <code>float</code> <p>float: Smooth the labels</p> <code>0</code> <code>loss_type</code> <code>Literal['sigmoid', 'hinge', 'ipo', 'kto']</code> <p>Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"]: Determine the loss function</p> <code>'sigmoid'</code> <code>reference_free</code> <code>bool</code> <p>bool: Indicate whether the reference policy is used or not</p> <code>False</code> <p>Returns:</p> Type Description <p>A function that takes in a state and a batch</p> Source code in <code>src/python/easydel/trainer/dpo/fwd_bwd_functions.py</code> <pre><code>def create_dpo_train_function(\n        concatenated_forward: Callable,\n        ref_state: EasyDeLState = None,\n        beta: float = 0.1,\n        label_smoothing: float = 0,\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"] = \"sigmoid\",\n        reference_free: bool = False,\n):\n    \"\"\"The create_dpo_train_function function is a helper function that creates the DPO training step.\n\n    Args:\n        concatenated_forward: Callable: Define the forward pass of the\n            model\n        ref_state: EasyDeLState: Specify the reference policy\n        beta: float: Scale the logits\n        label_smoothing: float: Smooth the labels\n        loss_type: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto\"]: Determine\n            the loss function\n        reference_free: bool: Indicate whether the reference policy is\n            used or not\n\n    Returns:\n        A function that takes in a state and a batch\n    \"\"\"\n\n    def _sigmoid_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array = None,  # IGNORED\n            reference_chosen_log_probs: chex.Array = None,  # IGNORED\n            policy_rejected_log_probs: chex.Array = None,  # IGNORED\n            reference_rejected_log_probs: chex.Array = None  # IGNORED\n    ):\n\n        \"\"\"The _sigmoid_dpo_loss function is a helper function for the sigmoid_dpo_loss\n            function. It computes the loss of each example in a batch, given its logits\n            and (optionally) its chosen/rejected log probabilities under both policies.\n\n        Args:\n            logits: chex.Array: Compute the loss\n            policy_chosen_log_probs: chex.Array: Calculate the policy\n                loss\n            reference_chosen_log_probs: chex.Array: Compute the loss for\n                the reference policy # IGNORED\n            policy_rejected_log_probs: chex.Array: Calculate the loss\n                for the rejected samples # IGNORED\n            reference_rejected_log_probs: chex.Array: Calculate the loss\n                of rejected samples # IGNORED\n\n        Returns:\n            an array represent loss\n        \"\"\"\n        losses = (\n                -jax.nn.log_sigmoid(beta * logits) * (1 - label_smoothing)\n                - jax.nn.log_sigmoid(-beta * logits) * label_smoothing\n        )\n        return losses\n\n    def _hinge_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array,  # IGNORED\n            reference_chosen_log_probs: chex.Array,  # IGNORED\n            policy_rejected_log_probs: chex.Array,  # IGNORED\n            reference_rejected_log_probs: chex.Array  # IGNORED\n    ):\n\n        \"\"\"The _hinge_dpo_loss function is a helper function that computes the loss for DPO.\n\n        Args:\n            logits: chex.Array: Calculate the hinge loss\n            policy_chosen_log_probs: chex.Array: Compute the policy loss\n            reference_chosen_log_probs: chex.Array: Compute the loss for\n                the reference policy # IGNORED\n            policy_rejected_log_probs: chex.Array: Calculate the loss\n                for the rejected samples # IGNORED\n            reference_rejected_log_probs: chex.Array: Calculate the loss\n                of rejected samples # IGNORED\n\n        Returns:\n            an array represent The hinge loss\n        \"\"\"\n        return jax.relu(1 - beta * logits)\n\n    def _ipo_dpo_loss(\n            logits: chex.Array,\n            policy_chosen_log_probs: chex.Array,  # IGNORED\n            reference_chosen_log_probs: chex.Array,  # IGNORED\n            policy_rejected_log_probs: chex.Array,  # IGNORED\n            reference_rejected_log_probs: chex.Array  # IGNORED\n    ):\n        \"\"\"The _ipo_dpo_loss function is a helper function that calculates the loss for\n        the IPO-DPO algorithm. It takes in the logits, policy_chosen_log_probs,\n        reference_chosen_log_probs, policy rejected log probs and reference rejected\n        log probs as inputs. The output of this function is used to calculate the loss\n        for each batch of data.\n\n                :param logits: chex.Array: Calculate the loss\n                :param policy_chosen_log_probs: chex.Array: Compute the\n                :param reference_chosen_log_probs: chex.Array: Compute the loss for the reference policy # IGNORED\n                :param policy_rejected_log_probs: chex.Array: Calculate the loss for the rejected samples # IGNORED\n                :param reference_rejected_log_probs: chex.Array: Calculate the loss of rejected samples # IGNORED\n                :return: an array represent loss\n        \"\"\"\n        return (logits - 1 / (2 * beta)) ** 2\n\n    def _kto_pair_dpo_loss(\n            logits: chex.Array,  # IGNORED\n            policy_chosen_log_probs: chex.Array,\n            reference_chosen_log_probs: chex.Array,\n            policy_rejected_log_probs: chex.Array,\n            reference_rejected_log_probs: chex.Array\n    ):\n\n        \"\"\"The _kto_pair_dpo_loss function is a helper function that computes the loss for\n        a single pair of trajectories. It takes in two sets of log probabilities, one from\n        the policy and one from the reference distribution. The first set are the log\n        probabilities for actions taken by each agent in a trajectory, while the second set\n        are those for actions not taken by each agent (i.e., rejected). The function then\n        computes KL divergences between these two sets of distributions and uses them to compute losses.\n\n        Args:\n            logits: chex.Array: Calculate the log_probs\n            policy_chosen_log_probs: chex.Array: Calculate the chosen_kl\n                # IGNORED\n            reference_chosen_log_probs: chex.Array: Calculate the\n                chosen_kl\n            policy_rejected_log_probs: chex.Array: Calculate the\n                rejected_kl variable\n            reference_rejected_log_probs: chex.Array: Calculate the\n                rejected_kl variable\n\n        Returns:\n            an array represent loss\n        \"\"\"\n        chosen_kl = jax.lax.clamp(\n            min=0,\n            x=jnp.mean(policy_chosen_log_probs - reference_chosen_log_probs),\n            max=1e9\n        )\n        rejected_kl = jax.lax.clamp(\n            min=0,\n            x=jnp.mean(policy_rejected_log_probs - reference_rejected_log_probs),\n            max=1e9\n        )\n\n        chosen_log_ratios = policy_chosen_log_probs - reference_chosen_log_probs\n        rejected_log_ratios = policy_rejected_log_probs - reference_rejected_log_probs\n        losses = jnp.concatenate(\n            (\n                1 - jax.nn.sigmoid(beta * (chosen_log_ratios - rejected_kl)),\n                1 - jax.nn.sigmoid(beta * (chosen_kl - rejected_log_ratios)),\n            ),\n            0,\n        )\n\n        return losses\n\n    if loss_type == \"sigmoid\":\n        _loss_func = _sigmoid_dpo_loss\n    elif loss_type == \"hinge\":\n        _loss_func = _hinge_dpo_loss\n    elif loss_type == \"ipo\":\n        _loss_func = _ipo_dpo_loss\n    elif loss_type == \"kto_pair\":\n        _loss_func = _kto_pair_dpo_loss\n    else:\n        raise ValueError(f\"UnKnown loss_type {loss_type}\")\n\n    def dpo_step(\n            state: EasyDeLState,\n            batch: dict\n    ) -&gt; tuple[EasyDeLState, DPOStepOut]:\n\n        \"\"\"The dpo_step function is the core of DPO. It takes a state and a batch,\n        and returns an updated state. The update is done by calculating the loss\n        for each example in the batch, then taking its gradient with respect to\n        the parameters of the policy network (which are stored in `state`). This\n        gradient is then used to update `state`.\n\n        Args:\n            state: EasyDeLState: Store the parameters of the model\n            batch: dict: Pass the data to the model\n\n        Returns:\n            A new state, which is a collection of the parameters and\n            apply_fn\n        \"\"\"\n\n        def calculate_loss(params: dict | flax.core.FrozenDict):\n            (\n                policy_chosen_log_probs,\n                policy_rejected_log_probs,\n                policy_chosen_logits,\n                policy_rejected_logits,\n            ) = concatenated_forward(\n                state.apply_fn,\n                params,\n                batch\n            )\n\n            if \"reference_chosen_log_probs\" in batch and \"reference_rejected_log_probs\" in batch:\n                reference_chosen_log_probs = batch[\"reference_chosen_log_probs\"]\n                reference_rejected_log_probs = batch[\"reference_rejected_log_probs\"]\n            else:\n                if ref_state is None:\n                    (\n                        reference_chosen_log_probs,\n                        reference_rejected_log_probs,\n                        _,\n                        _,\n                    ) = concatenated_forward(\n                        state.apply_fn,\n                        state.params,\n                        batch\n                    )\n                else:\n                    (\n                        reference_chosen_log_probs,\n                        reference_rejected_log_probs,\n                        _,\n                        _,\n                    ) = concatenated_forward(\n                        ref_state.apply_fn,\n                        ref_state.params,\n                        batch\n                    )\n\n            pi_log_ratios = policy_chosen_log_probs - policy_rejected_log_probs\n\n            if reference_free:\n                ref_log_ratios = 0\n            else:\n                ref_log_ratios = reference_chosen_log_probs - reference_rejected_log_probs\n\n            logits = pi_log_ratios - ref_log_ratios\n            losses = _loss_func(\n                logits,\n                policy_chosen_log_probs,\n                reference_chosen_log_probs,\n                policy_rejected_log_probs,\n                reference_rejected_log_probs\n            )\n            chosen_rewards = (\n                    beta\n                    * (\n                            policy_chosen_log_probs - reference_chosen_log_probs\n                    )\n            )\n            rejected_rewards = (\n                    beta\n                    * (\n                            policy_rejected_log_probs\n                            - reference_rejected_log_probs\n                    )\n            )\n            return losses[0], (chosen_rewards, rejected_rewards)\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (__loss, (__chosen_rewards, __rejected_rewards)), grads = grad_fn(state.params)\n        new_state = state.apply_gradients(grads=grads)\n        return new_state, DPOStepOut(\n            loss=__loss,\n            rejected_rewards=__rejected_rewards,\n            chosen_rewards=__chosen_rewards\n        )\n\n    return dpo_step\n</code></pre>"},{"location":"generated-trainer-dpo-fwd_bwd_functions/#src.python.easydel.trainer.dpo.fwd_bwd_functions.get_batch_log_probs","title":"<code>get_batch_log_probs(logits, labels, average_log_prob=False, label_pad_token_id=-100, is_encoder_decoder=False)</code>","text":"<p>The get_batch_log_probs function computes the log probability of a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>chex.Array: Compute the log_softmax of the input</p> required <code>labels</code> <code>Array</code> <p>chex.Array: Mask the logits</p> required <code>average_log_prob</code> <code>bool</code> <p>bool: Determine whether to average the log prob over the sequence length</p> <code>False</code> <code>label_pad_token_id</code> <code>int</code> <p>int: Mask out the padding tokens in the labels</p> <code>-100</code> <code>is_encoder_decoder</code> <code>bool</code> <p>bool: Indicate whether the model is an encoder-decoder model</p> <code>False</code> <p>:param : Determine whether to average the log probability over all tokens or not</p> <p>Returns:</p> Type Description <code>Array</code> <p>The log probability of the labels given the logits</p> Source code in <code>src/python/easydel/trainer/dpo/fwd_bwd_functions.py</code> <pre><code>def get_batch_log_probs(\n        logits: chex.Array,\n        labels: chex.Array,\n        average_log_prob: bool = False,\n        label_pad_token_id: int = -100,\n        is_encoder_decoder: bool = False,\n) -&gt; chex.Array:\n    \"\"\"The get_batch_log_probs function computes the log probability of a batch of sequences.\n\n    Args:\n        logits: chex.Array: Compute the log_softmax of the input\n        labels: chex.Array: Mask the logits\n        average_log_prob: bool: Determine whether to average the log\n            prob over the sequence length\n        label_pad_token_id: int: Mask out the padding tokens in the\n            labels\n        is_encoder_decoder: bool: Indicate whether the model is an\n            encoder-decoder model\n    :param : Determine whether to average the log probability over all tokens or not\n\n    Returns:\n        The log probability of the labels given the logits\n    \"\"\"\n\n    # sudo code\n    # (per_token_log_probs * loss_mask).sum(-1)\n    # or\n    # (per_token_log_probs * loss_mask).sum(-1) / loss_mask.sum(-1)\n\n    if logits.shape[:-1] != labels.shape:\n        raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n\n    if not is_encoder_decoder:\n        labels = labels[:, 1:]\n        logits = logits[:, :-1, :]\n\n    batch, seq_len, dim = logits.shape\n    loss_mask = labels != label_pad_token_id\n    labels = jax.lax.select(\n        labels == label_pad_token_id,\n        jnp.zeros(labels.shape, dtype=labels.dtype),\n        labels\n    )\n    logits_log_s = jax.nn.log_softmax(\n        logits, -1\n    )\n    per_token_log_probs = jnp.take_along_axis(\n        logits_log_s,\n        axis=2,\n        indices=labels[:, :, None]\n    ).reshape(batch, seq_len)\n\n    if average_log_prob:\n        log_prob = jnp.sum((per_token_log_probs * loss_mask), axis=-1) / jnp.sum(loss_mask, axis=-1)\n    else:\n        log_prob = jnp.sum((per_token_log_probs * loss_mask), axis=-1)\n\n    return log_prob\n</code></pre>"},{"location":"generated-trainer-dpo-modelling_output/","title":"trainer.dpo.modelling_output","text":""},{"location":"generated-trainer-dpo-utils/","title":"trainer.dpo.utils","text":""},{"location":"generated-trainer-dpo-utils/#src.python.easydel.trainer.dpo.utils.DPODataCollatorWithPadding","title":"<code>DPODataCollatorWithPadding</code>  <code>dataclass</code>","text":"<p>DPO DataCollator class that pads the tokenized inputs to the maximum length of the batch.</p> <p>Parameters:</p> Name Type Description Default <code>pad_token_id</code> <code>int</code> <p>int: The tokenizers pad_token_id.</p> <code>0</code> <code>label_pad_token_id</code> <code>int</code> <p>int: The label used for masking.</p> <code>-100</code> <code>is_encoder_decoder</code> <code>Optional[bool]</code> <p>Optional[bool]: Whether you model has an encoder_decoder architecture</p> <code>False</code> Source code in <code>src/python/easydel/trainer/dpo/utils.py</code> <pre><code>@dataclass\nclass DPODataCollatorWithPadding:\n    r\"\"\"DPO DataCollator class that pads the tokenized inputs to the maximum length of the batch.\n\n    Args:\n        pad_token_id: int: The tokenizers pad_token_id.\n        label_pad_token_id: int: The label used for masking.\n        is_encoder_decoder: Optional[bool]: Whether you model has an\n            encoder_decoder architecture\n    \"\"\"\n    max_prompt_length: int\n    max_target_length: int\n    pad_token_id: int = 0\n    label_pad_token_id: int = -100\n    is_encoder_decoder: Optional[bool] = False\n    ids_to_pop_from_dataset: Optional[dict] = None\n    auto_fix_data: bool = True\n\n    def __call__(self, features: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        padded_batch = {}\n        for k in features[0].keys():\n            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n                if self.is_encoder_decoder:\n                    to_pad = [jnp.array(ex[k], dtype=\"i4\") for ex in features]\n\n                    if (k.startswith(\"prompt\")) and (k.endswith(\"input_ids\")):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    elif (k.startswith(\"chosen\")) or (k.startswith(\"rejected\")) or (\"decoder\" in k):\n                        padding_value = self.label_pad_token_id\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value).astype(\"i4\")\n                else:\n                    if \"prompt\" in k:\n                        to_pad = [jnp.array(ex[k][::-1], dtype=\"i4\") for ex in features]\n                    else:\n                        to_pad = [jnp.array(ex[k], dtype=\"i4\") for ex in features]\n                    if k.endswith(\"_input_ids\"):\n                        padding_value = self.pad_token_id\n                    elif k.endswith(\"_labels\"):\n                        padding_value = self.label_pad_token_id\n                    elif k.endswith(\"_attention_mask\"):\n                        padding_value = 0\n                    else:\n                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value).astype(\"i4\")\n                    if \"prompt\" in k:\n                        padded_batch[k] = jnp.flip(padded_batch[k], axis=[1])\n            elif k.endswith(\"_logps\"):\n                padded_batch[k] = jnp.array([ex[k] for ex in features])\n            else:\n                padded_batch[k] = [ex[k] for ex in features]\n        if self.ids_to_pop_from_dataset:\n            for key in self.ids_to_pop_from_dataset:\n                _ = padded_batch.pop(key, None)\n        for key in list(padded_batch.keys()):\n            if not (\n                    key.endswith(\"_input_ids\")\n                    or key.endswith(\"_attention_mask\")\n                    or key.endswith(\"_labels\")\n                    or key.endswith(\"_log_probs\")\n            ):\n                _ = padded_batch.pop(key, None)\n        for k in list(padded_batch.keys()):\n            v = padded_batch[k]\n            padded_batch[k] = v.reshape(v.shape[0], -1)\n        if self.auto_fix_data:\n            padded_batch[\"rejected_input_ids\"] = padded_batch[\"rejected_input_ids\"][..., :self.max_target_length]\n            padded_batch[\n                \"rejected_attention_mask\"\n            ] = padded_batch[\"rejected_attention_mask\"][..., :self.max_target_length]\n            padded_batch[\"rejected_labels\"] = padded_batch[\"rejected_labels\"][..., :self.max_target_length]\n\n            padded_batch[\"chosen_input_ids\"] = padded_batch[\"chosen_input_ids\"][..., :self.max_target_length]\n            padded_batch[\"chosen_attention_mask\"] = padded_batch[\"chosen_attention_mask\"][..., :self.max_target_length]\n            padded_batch[\"chosen_labels\"] = padded_batch[\"chosen_labels\"][..., :self.max_target_length]\n\n            padded_batch[\"prompt_input_ids\"] = padded_batch[\"prompt_input_ids\"][..., :self.max_prompt_length]\n            padded_batch[\n                \"prompt_attention_mask\"\n            ] = padded_batch[\"prompt_attention_mask\"][..., :self.max_prompt_length]\n\n        return padded_batch\n</code></pre>"},{"location":"generated-trainer-orpo-fwd_bwd_functions/","title":"trainer.orpo.fwd_bwd_functions","text":""},{"location":"generated-trainer-orpo-fwd_bwd_functions/#src.python.easydel.trainer.orpo.fwd_bwd_functions.concatenated_inputs","title":"<code>concatenated_inputs(batch, is_encoder_decoder=False, label_pad_token_id=-100, padding_value=0, truncation_mode='keep_end', fixed_max_length=None)</code>","text":"<p>The concatenated_inputs function takes a batch of chosen and rejected examples, and concatenates them together. This is useful for training the model to predict whether an example was chosen by the human annotator. The function also pads all inputs to the same length as the longest input in that batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Union[List, Array]]</code> <p>Dict[str,Union[List,chex.Array]]: Pass the batch of data into the function,</p> required <code>is_encoder_decoder</code> <code>bool</code> <p>bool: Determine whether the model is an encoder-decoder model</p> <code>False</code> <code>label_pad_token_id</code> <code>int</code> <p>int: Pad the labels with a value of -100</p> <code>-100</code> <code>padding_value</code> <code>int</code> <p>int: Pad the input_ids and attention_mask arrays to the same length</p> <code>0</code> <code>truncation_mode</code> <code>Literal['keep_end', 'keep_start']</code> <p>typing.Literal[\"keep_end\", \"keep_start\"]: is left padded or not should it keep start of the</p> <code>'keep_end'</code> <code>fixed_max_length</code> <code>int | None</code> <p>int|None: by providing fixed_max_length the func will always return a fixed sequence length and won't use dynamic methods.</p> <code>None</code> <p>Allow for the batch to be a list of arrays or just an array, Specify the type of data that is being passed in</p> <p>array or the end of the array?.</p> <p>Returns:</p> Type Description <code>Dict[str, Array]</code> <p>A dictionary of the concatenated inputs</p> Source code in <code>src/python/easydel/trainer/orpo/fwd_bwd_functions.py</code> <pre><code>def concatenated_inputs(\n        batch: Dict[str, Union[List, chex.Array]],\n        is_encoder_decoder: bool = False,\n        label_pad_token_id: int = -100,\n        padding_value: int = 0,\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n        fixed_max_length: int | None = None\n) -&gt; Dict[str, chex.Array]:\n    \"\"\"The concatenated_inputs function takes a batch of chosen and rejected examples,\n    and concatenates them together. This is useful for training the model to predict whether an example was chosen\n    by the human annotator. The function also pads all inputs to\n    the same length as the longest input in that batch.\n\n    Args:\n        batch: Dict[str,Union[List,chex.Array]]: Pass the batch of data\n            into the function,\n        is_encoder_decoder: bool: Determine whether the model is an\n            encoder-decoder model\n        label_pad_token_id: int: Pad the labels with a value of -100\n        padding_value: int: Pad the input_ids and attention_mask arrays\n            to the same length\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"]: is\n            left padded or not should it keep start of the\n        fixed_max_length: int|None: by providing fixed_max_length the\n            func will always return a fixed sequence length and won't\n            use dynamic methods.\n    Allow for the batch to be a list of arrays or just an array,\n    Specify the type of data that is being passed in\n\n    array or the end of the array?.\n\n    Returns:\n        A dictionary of the concatenated inputs\n    \"\"\"\n    concatenated_batch = {}\n    if fixed_max_length is None:\n        if is_encoder_decoder:\n            max_length = max(batch[\"chosen_labels\"].shape[-1], batch[\"rejected_labels\"].shape[-1])\n        else:\n            max_length = max(batch[\"chosen_input_ids\"].shape[-1], batch[\"rejected_input_ids\"].shape[-1])\n    else:\n        max_length = fixed_max_length\n    for k in batch:\n        if k.startswith(\"chosen\") and isinstance(batch[k], jax.Array):\n            if \"labels\" in k or is_encoder_decoder:\n                pad_value = label_pad_token_id\n            elif k.endswith(\"_input_ids\"):\n                pad_value = padding_value\n            elif k.endswith(\"_attention_mask\"):\n                pad_value = 0\n            else:\n                raise KeyError(\"couldn't find pad_value [Dataset Issue]\")\n            concatenated_key = k.replace(\"chosen\", \"concatenated\")\n            concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n    for k in batch:\n        if k.startswith(\"rejected\") and isinstance(batch[k], jax.Array):\n            if \"labels\" in k or is_encoder_decoder:\n                pad_value = label_pad_token_id\n            elif k.endswith(\"_input_ids\"):\n                assert padding_value is not None, \"`padding_value` can not be set as `None`\"\n                pad_value = padding_value\n            elif k.endswith(\"_attention_mask\"):\n                pad_value = 0\n            else:\n                raise KeyError(\"couldn't find pad_value [Dataset Issue]\")\n            concatenated_key = k.replace(\"rejected\", \"concatenated\")\n            v2d = lambda ar: ar.reshape(ar.shape[0], -1)\n            concatenated_batch[concatenated_key] = jnp.concatenate(\n                (\n                    v2d(concatenated_batch[concatenated_key]),\n                    pad_to_length(v2d(batch[k]), max_length, pad_value=pad_value),\n                ),\n                axis=0,\n            )\n    for k in list(concatenated_batch.keys()):\n        val = concatenated_batch[k]\n        if val.ndim == 3:\n            # making 3d array 2d\n            concatenated_batch[k] = val.reshape(val.shape[0], -1)\n    if is_encoder_decoder:\n        warnings.warn(\"`concatenated_input_ids` will be repeated (encoder decoder model detected)\")\n        concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1)\n        concatenated_batch[\"concatenated_attention_mask\"] = (\n            batch[\"prompt_attention_mask\"].repeat(2, 1)\n        )\n\n    return concatenated_batch\n</code></pre>"},{"location":"generated-trainer-orpo-fwd_bwd_functions/#src.python.easydel.trainer.orpo.fwd_bwd_functions.create_concatenated_forward","title":"<code>create_concatenated_forward(is_encoder_decoder, label_pad_token_id, padding_value, truncation_mode='keep_end', fixed_max_length=None)</code>","text":"<p>The create_concatenated_forward function is a helper function that creates a forward pass function for the model. The forward pass function takes in an apply_fn, which is the model's apply_fn, and runs it on concatenated inputs. It returns chosen log probs, rejected log probs, chosen logits and rejected logits.</p> <p>Parameters:</p> Name Type Description Default <code>is_encoder_decoder</code> <p>Determine whether the model is an encoder- decoder model or not</p> required <code>label_pad_token_id</code> <p>Pad the labels to the same length</p> required <code>padding_value</code> <p>Pad the inputs to the same length</p> required <code>truncation_mode</code> <code>Literal['keep_end', 'keep_start']</code> <p>typing.Literal[\"keep_end\",\"keep_start\"]: where to pad and where to keep.</p> <code>'keep_end'</code> <code>fixed_max_length</code> <code>int | None</code> <p>int|None: by providing fixed_max_length the func will always return a fixed sequence length</p> <code>None</code> <p>and won't use dynamic methods.</p> <p>Returns:</p> Type Description <p>A function that takes in a apply_fn, params and a batch of</p> <p>inputs,</p> Source code in <code>src/python/easydel/trainer/orpo/fwd_bwd_functions.py</code> <pre><code>def create_concatenated_forward(\n        is_encoder_decoder,\n        label_pad_token_id,\n        padding_value,\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n        fixed_max_length: int | None = None\n):\n    \"\"\"The create_concatenated_forward function is a helper function that creates a forward pass function for the\n    model. The forward pass function takes in an apply_fn, which is the model's apply_fn, and runs it on concatenated\n    inputs. It returns chosen log probs, rejected log probs, chosen logits and rejected logits.\n\n    Args:\n        is_encoder_decoder: Determine whether the model is an encoder-\n            decoder model or not\n        label_pad_token_id: Pad the labels to the same length\n        padding_value: Pad the inputs to the same length\n        truncation_mode: typing.Literal[\"keep_end\",\"keep_start\"]: where\n            to pad and where to keep.\n        fixed_max_length: int|None: by providing fixed_max_length the\n            func will always return a fixed sequence length\n    and won't use dynamic methods.\n\n    Returns:\n        A function that takes in a apply_fn, params and a batch of\n        inputs,\n    \"\"\"\n\n    def concatenated_forward(\n            apply_fn: Callable,\n            params: dict | flax.core.FrozenDict,\n            batch: Dict[str, Union[List, chex.Array]]\n\n    ) -&gt; Tuple[chex.Array, chex.Array, chex.Array, chex.Array, chex.Array]:\n        \"\"\"The concatenated_forward function is used to compute the log-probabilities of both chosen and rejected labels.\n\n        Args:\n            apply_fn: Callable: Pass in the model function\n            params: dict | flax.core.FrozenDict: Pass the model\n                parameters to the function\n            batch: Dict[str, Union[List, chex.Array]] : Pass the batch\n                of data to the concatenated_forward function\n\n        Returns:\n            The log_probs of the chosen and rejected labels, as well as\n            their corresponding logits\n        \"\"\"\n        assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n        concatenated_batch = concatenated_inputs(\n            batch,\n            is_encoder_decoder=is_encoder_decoder,\n            label_pad_token_id=label_pad_token_id,\n            padding_value=padding_value,\n            truncation_mode=truncation_mode,\n            fixed_max_length=fixed_max_length\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n        concatenated_batch[\"concatenated_input_ids\"] = concatenated_batch[\"concatenated_input_ids\"].reshape(\n            concatenated_batch[\"concatenated_input_ids\"].shape[0], -1\n        )\n        concatenated_batch[\"concatenated_labels\"] = concatenated_batch[\"concatenated_labels\"].reshape(\n            concatenated_batch[\"concatenated_labels\"].shape[0], -1\n        )\n        concatenated_batch[\"concatenated_attention_mask\"] = concatenated_batch[\"concatenated_attention_mask\"].reshape(\n            concatenated_batch[\"concatenated_attention_mask\"].shape[0], -1\n        )\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if is_encoder_decoder\n            else {}\n        )\n        all_logits = apply_fn(\n            concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n            params=params,\n            **model_kwargs,\n        ).logits\n\n        def cross_entropy_loss(logits, labels, mask):\n            if not is_encoder_decoder:\n                logits = logits[..., :-1, :]\n                labels = labels[..., 1:]\n                mask = mask[..., 1:]\n            loss = fjformer.cross_entropy_loss_and_accuracy(logits, labels, mask)[0]\n            return loss\n\n        if is_encoder_decoder:\n            labels = concatenated_batch[\"concatenated_labels\"]\n        else:\n            labels = concatenated_batch[\"concatenated_input_ids\"]\n\n        chosen_nll_loss = cross_entropy_loss(\n            all_logits[:len_chosen],\n            labels[:len_chosen],\n            concatenated_batch[\"concatenated_attention_mask\"][:len_chosen]\n        )\n        all_log_probs = get_batch_log_probs(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=False,\n            is_encoder_decoder=is_encoder_decoder,\n            label_pad_token_id=label_pad_token_id,\n        )\n\n        chosen_log_probs = all_log_probs[:len_chosen]\n        rejected_log_probs = all_log_probs[len_chosen:]\n\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n        return chosen_log_probs, rejected_log_probs, chosen_logits, rejected_logits, chosen_nll_loss\n\n    return concatenated_forward\n</code></pre>"},{"location":"generated-trainer-orpo-fwd_bwd_functions/#src.python.easydel.trainer.orpo.fwd_bwd_functions.create_orpo_step_function","title":"<code>create_orpo_step_function(concatenated_forward, beta=0.1, mode='train', batch_partition_spec=PartitionSpec(('fsdp', 'dp'), 'sp'))</code>","text":"<p>The create_orpo_step_function function is a helper function that creates the ORPO training step.</p> <p>Parameters:</p> Name Type Description Default <code>concatenated_forward</code> <code>Callable</code> <p>Callable: Define the forward pass of the model</p> required <code>beta</code> <code>float</code> <p>float: Scale the logits</p> <code>0.1</code> <code>mode</code> <code>Literal['train', 'eval']</code> <p>Literal[\"train\", \"eval\"] : \"train\", \"eval\" function modes</p> <code>'train'</code> <code>batch_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Batch PartitionSpec</p> <code>PartitionSpec(('fsdp', 'dp'), 'sp')</code> <p>Returns:</p> Type Description <p>A function that takes in a state and a batch</p> Source code in <code>src/python/easydel/trainer/orpo/fwd_bwd_functions.py</code> <pre><code>def create_orpo_step_function(\n        concatenated_forward: Callable,\n        beta: float = 0.1,\n        mode: Literal[\"train\", \"eval\"] = \"train\",\n        batch_partition_spec: PartitionSpec = PartitionSpec((\"fsdp\", \"dp\"), \"sp\")\n):\n    \"\"\"The create_orpo_step_function function is a helper function that creates the ORPO training step.\n\n    Args:\n        concatenated_forward: Callable: Define the forward pass of the\n            model\n        beta: float: Scale the logits\n        mode: Literal[\"train\", \"eval\"] : \"train\", \"eval\" function modes\n        batch_partition_spec: PartitionSpec: Batch PartitionSpec\n\n    Returns:\n        A function that takes in a state and a batch\n    \"\"\"\n\n    def orpo_step(\n            state: EasyDeLState,\n            batch: dict\n    ) -&gt; tuple[EasyDeLState, ORPOStepOut]:\n        \"\"\"The orpo_step function is the core of ORPO. It takes a state and a batch,\n        and returns an updated state. The update is done by calculating the loss\n        for each example in the batch, then taking its gradient with respect to\n        the parameters of the policy network (which are stored in `state`). This\n        gradient is then used to update `state`.\n\n        Args:\n            state: EasyDeLState: Store the parameters of the model\n            batch: dict: Pass the data to the model\n\n        Returns:\n            A new state, which is a collection of the parameters and\n            apply_fn\n        \"\"\"\n        batch = fjformer.with_sharding_constraint(batch, partition_specs=batch_partition_spec)\n\n        def calculate_loss(params: dict | flax.core.FrozenDict):\n            (\n                policy_chosen_log_probs,\n                policy_rejected_log_probs,\n                policy_chosen_logits,\n                policy_rejected_logits,\n                policy_nll_loss\n            ) = concatenated_forward(\n                state.apply_fn,\n                params,\n                batch\n            )\n\n            losses, chosen_rewards, rejected_rewards, log_odds_ratio, log_odds_chosen = odds_ratio_loss(\n                beta, policy_chosen_log_probs, policy_rejected_log_probs\n            )\n\n            loss = policy_nll_loss - losses.mean()\n\n            reward_accuracies = (chosen_rewards &gt; rejected_rewards).astype(\"float32\")\n            metrics = {}\n            prefix = \"eval_\" if mode == \"eval\" else \"\"\n            metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.mean()\n            metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.mean()\n            metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean()\n            metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).mean()\n            metrics[f\"{prefix}logps/rejected\"] = policy_rejected_log_probs.mean()\n            metrics[f\"{prefix}logps/chosen\"] = policy_chosen_log_probs.mean()\n            metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits.mean()\n            metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.mean()\n            metrics[f\"{prefix}nll_loss\"] = policy_nll_loss.mean()\n            metrics[f\"{prefix}log_odds_ratio\"] = log_odds_ratio\n            metrics[f\"{prefix}log_odds_chosen\"] = log_odds_chosen\n            return loss, metrics\n\n        if mode == \"train\":\n            grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n            (__loss, (__metrics)), grads = grad_fn(state.params)\n            new_state = state.apply_gradients(grads=grads)\n        else:\n            __loss, __metrics = calculate_loss(state.params)\n            new_state = state\n        return new_state, ORPOStepOut(\n            loss=__loss,\n            metrics=__metrics\n        )\n\n    return orpo_step\n</code></pre>"},{"location":"generated-trainer-orpo-fwd_bwd_functions/#src.python.easydel.trainer.orpo.fwd_bwd_functions.get_batch_log_probs","title":"<code>get_batch_log_probs(logits, labels, average_log_prob=False, label_pad_token_id=-100, is_encoder_decoder=False)</code>","text":"<p>The get_batch_log_probs function computes the log probability of a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>chex.Array: Compute the log_softmax of the input</p> required <code>labels</code> <code>Array</code> <p>chex.Array: Mask the logits</p> required <code>average_log_prob</code> <code>bool</code> <p>bool: Determine whether to average the log prob over the sequence length</p> <code>False</code> <code>label_pad_token_id</code> <code>int</code> <p>int: Mask out the padding tokens in the labels</p> <code>-100</code> <code>is_encoder_decoder</code> <code>bool</code> <p>bool: Indicate whether the model is an encoder-decoder model</p> <code>False</code> <p>:param : Determine whether to average the log probability over all tokens or not</p> <p>Returns:</p> Type Description <code>Array</code> <p>The log probability of the labels given the logits</p> Source code in <code>src/python/easydel/trainer/orpo/fwd_bwd_functions.py</code> <pre><code>def get_batch_log_probs(\n        logits: chex.Array,\n        labels: chex.Array,\n        average_log_prob: bool = False,\n        label_pad_token_id: int = -100,\n        is_encoder_decoder: bool = False,\n) -&gt; chex.Array:\n    \"\"\"The get_batch_log_probs function computes the log probability of a batch of sequences.\n\n    Args:\n        logits: chex.Array: Compute the log_softmax of the input\n        labels: chex.Array: Mask the logits\n        average_log_prob: bool: Determine whether to average the log\n            prob over the sequence length\n        label_pad_token_id: int: Mask out the padding tokens in the\n            labels\n        is_encoder_decoder: bool: Indicate whether the model is an\n            encoder-decoder model\n    :param : Determine whether to average the log probability over all tokens or not\n\n    Returns:\n        The log probability of the labels given the logits\n    \"\"\"\n\n    # sudo code\n    # (per_token_log_probs * loss_mask).sum(-1)\n    # or\n    # (per_token_log_probs * loss_mask).sum(-1) / loss_mask.sum(-1)\n\n    if logits.shape[:-1] != labels.shape:\n        raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n\n    if not is_encoder_decoder:\n        labels = labels[:, 1:]\n        logits = logits[:, :-1, :]\n\n    batch, seq_len, dim = logits.shape\n    loss_mask = labels != label_pad_token_id\n\n    labels = jnp.where(labels == label_pad_token_id, 0, labels)\n\n    per_token_logps = jnp.take_along_axis(\n        jax.nn.log_softmax(logits, axis=-1), axis=2, indices=labels[:, :, None]\n    ).reshape(batch, seq_len)\n\n    if average_log_prob:\n        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n    else:\n        return (per_token_logps * loss_mask).sum(-1)\n</code></pre>"},{"location":"generated-trainer-orpo-modelling_output/","title":"trainer.orpo.modelling_output","text":""},{"location":"generated-trainer-orpo-orpo_trainer/","title":"trainer.orpo.orpo_trainer","text":""},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer","title":"<code>ORPOTrainer</code>","text":"<p>               Bases: <code>BaseTrainer</code>, <code>ABC</code></p> <p>easydel ORPO Trainer Class</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>class ORPOTrainer(BaseTrainer, ABC):\n    \"\"\"\n    easydel ORPO Trainer Class\n    \"\"\"\n\n    def __init__(\n            self,\n            arguments: TrainArguments,\n            max_length: Optional[int] = None,\n            max_prompt_length: Optional[int] = None,\n            max_completion_length: Optional[int] = None,\n            beta: float = 0.1,\n            disable_dropout: bool = True,\n            label_pad_token_id: int = -100,\n            is_encoder_decoder: bool = False,\n            padding_value: int = None,\n            data_collator: Optional[DPODataCollatorWithPadding] = None,\n            train_dataset: Optional[Dataset] = None,\n            eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n            tokenizer: Optional[PreTrainedTokenizerBase] = None,\n            dataset_num_proc: Optional[int] = None,\n            _do_init_fns: bool = True,\n            dataset_map_arguments: Optional[Dict[str, Any]] = None,\n            low_mem_usage: bool = False,\n    ):\n\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object.\n\n        :param self: Refer to the object itself\n        :param beta: float: Control the strength of the regularization term\n        :param arguments: TrainArguments: Pass the arguments to the trainer\n        :param label_pad_token_id: int: Pad the labels\n        :param padding_value: int: Specify the value that is used for padding\n        :param train_dataset: Optional[Dataset]: Load the training dataset\n        :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer\n        :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer\n        :param max_length: Optional[int]: Set the maximum length of the input sequence\n        :param max_prompt_length: Optional[int]: Set the maximum length of the prompt\n        :param max_completion_length: Optional[int]: Truncate the target sequence\n        :param data_collator: Optional[Callable]: Function to be used for creating datasets.\n        tokenizing process with `dataset.map`.\n        :parma dataset_num_proc: Optional[int]: The number of processes to use for the dataset mapping.\n        :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure\n        model with provided training Arguments\n        :param : Set the padding value for the model\n        \"\"\"\n\n        assert arguments is not None, (\n            \"You Have to pass arguments that will be used for training but you have passed\"\n            \"`arguments=None`\"\n        )\n        assert isinstance(arguments, TrainArguments), (\n            f\"arguments type must be `TrainArguments` but got {type(arguments)}\"\n        )\n\n        if tokenizer is None:\n            raise ValueError(\"tokenizer must be specified to tokenize a ORPO dataset.\")\n        if max_length is None:\n            warnings.warn(\n                \"`max_length` is not set in the ORPOTrainer's init\"\n                \" it will default to `512` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_length = 512\n        if max_prompt_length is None:\n            warnings.warn(\n                \"`max_prompt_length` is not set in the ORPOTrainer's init\"\n                \" it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_prompt_length = 128\n\n        if max_completion_length is None:\n            warnings.warn(\n                \"When using an encoder decoder architecture, you should set `max_completion_length` in the \"\n                \"ORPOTrainer's init it will default to `128` by default, but you should do it yourself in the future.\",\n                UserWarning,\n            )\n            max_completion_length = 128\n\n        padding_value = padding_value if padding_value is not None else tokenizer.pad_token_id\n        self.max_length = max_length\n        self.label_pad_token_id = label_pad_token_id\n        self.padding_value = padding_value\n        self.max_prompt_length = max_prompt_length\n        self.truncation_mode = arguments.truncation_mode\n        self.disable_dropout = disable_dropout\n        self.max_completion_length = max_completion_length\n        self.tokenizer = tokenizer\n        self.is_encoder_decoder = is_encoder_decoder\n        self.low_mem_usage = low_mem_usage\n        self.beta = beta\n        self.dataset_num_proc = dataset_num_proc\n        data_collator = DPODataCollatorWithPadding(\n            max_prompt_length=self.max_prompt_length,\n            max_target_length=self.max_completion_length,\n            pad_token_id=tokenizer.pad_token_id,\n            label_pad_token_id=label_pad_token_id,\n            is_encoder_decoder=False,\n        ) if data_collator is None else data_collator\n        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n        if dataset_map_arguments is None:\n            dataset_map_arguments = {}\n        train_dataset = train_dataset.map(\n            self.tokenize_row,\n            num_proc=dataset_num_proc,\n            **dataset_map_arguments\n        )\n        if eval_dataset is not None:\n            eval_dataset = eval_dataset.map(\n                self.tokenize_row,\n                num_proc=dataset_num_proc,\n                **dataset_map_arguments\n            )\n\n        self.arguments = arguments\n        self.hp_name = None\n        self.deepspeed = None\n        self.is_in_train = False\n\n        self.data_collator = data_collator\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.tokenizer = tokenizer\n        self._loggers_initialized = False\n        self.mesh = self.arguments.get_mesh()\n        assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n\n        self.concatenated_forward = create_concatenated_forward(\n            is_encoder_decoder=self.is_encoder_decoder,\n            padding_value=padding_value,\n            label_pad_token_id=label_pad_token_id,\n        )\n\n        self._cached_p_l_s = None\n        self._cached_c_l_s = None\n        self._cached_r_l_s = None\n\n        super().__init__(\n            arguments=arguments,\n            dataset_train=train_dataset,\n            dataset_eval=eval_dataset,\n            finetune=True,\n            checkpoint_path=None,\n            _do_init_fns=_do_init_fns\n        )\n\n    def build_tokenized_answer(self, prompt, answer):\n        \"\"\"\n        Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n        It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n        \"\"\"\n\n        full_tokenized = self.tokenizer(prompt + answer, add_special_tokens=False)\n        prompt_input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n        answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids):]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids):]\n        prompt_input_ids = jnp.asarray(prompt_input_ids, dtype=\"i4\")\n        answer_input_ids = jnp.asarray(answer_input_ids, dtype=\"i4\")\n        full_concat_input_ids = jnp.concatenate(\n            (\n                prompt_input_ids,\n                answer_input_ids\n            )\n        )\n\n        # Prepare input tokens for token by token comparison\n        full_input_ids = jnp.array(full_tokenized[\"input_ids\"])\n\n        if len(full_input_ids) != len(full_concat_input_ids):\n            raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n\n        response_token_ids_start_idx = len(prompt_input_ids)\n        if prompt_input_ids.tolist() != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n            response_token_ids_start_idx -= 1\n\n        prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n        prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n\n        if len(prompt_input_ids) != len(prompt_attention_mask):\n            raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n\n        answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n        answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n\n        return dict(\n            prompt_input_ids=jnp.array(prompt_input_ids, dtype=\"i4\"),\n            prompt_attention_mask=jnp.array(prompt_attention_mask, dtype=\"i4\"),\n            input_ids=jnp.array(answer_input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(answer_attention_mask, dtype=\"i4\"),\n        )\n\n    def tokenize_row(self, feature, state: EasyDeLState = None) -&gt; Dict:\n\n        \"\"\"\n        The tokenize_row function is responsible for taking a single row of data and converting it into the format that\n        the model expects. This includes:\n        - Tokenizing the text (using HuggingFace's tokenizer)\n        - Padding/truncating sequences to a fixed length (if necessary)\n        - Creating attention masks, which tell the model which tokens are padding and which aren't.\n\n        :param self: Represent the instance of the class\n        :param feature: Pass in the data from the dataset\n        :param state: EasyDeLState: Keep track of the state of the tokenizer\n        :return: A dictionary of the following keys\n        \"\"\"\n        batch = {}\n        prompt = feature[\"prompt\"]\n        chosen = feature[\"chosen\"]\n        rejected = feature[\"rejected\"]\n\n        if not isinstance(prompt, str):\n            raise ValueError(f\"prompt should be an str but got {type(prompt)} , {prompt}\")\n        prompt_tokens = self.tokenizer(\n            prompt,\n            add_special_tokens=False,\n            return_tensors=\"np\",\n        )\n        prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n\n        if not isinstance(chosen, str):\n            raise ValueError(f\"chosen should be an str but got {type(chosen)} , {chosen}\")\n        chosen_tokens = self.build_tokenized_answer(prompt, chosen)\n\n        if not isinstance(rejected, str):\n            raise ValueError(f\"rejected should be an str but got {type(rejected)}\")\n        rejected_tokens = self.build_tokenized_answer(prompt, rejected)\n        v2d = lambda ar: ar.reshape(1, -1) if ar.ndim == 1 else ar\n\n        def add_tkn(n, ar):\n            return jnp.concatenate(\n                (\n                    jnp.array(n).reshape(1, 1),\n                    v2d(ar)\n                ), axis=-1\n            )\n\n        def add_post_tkn(n, ar):\n            return jnp.concatenate(\n                (\n                    v2d(ar),\n                    jnp.array(n).reshape(1, 1)\n                ), axis=-1\n            )\n\n        prompt_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            prompt_tokens[\"prompt_input_ids\"]\n        )\n        chosen_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            chosen_tokens[\"prompt_input_ids\"]\n        )\n        rejected_tokens[\"prompt_input_ids\"] = add_tkn(\n            self.tokenizer.bos_token_id,\n            rejected_tokens[\"prompt_input_ids\"]\n        )\n\n        prompt_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, prompt_tokens[\"prompt_attention_mask\"]\n        )\n        chosen_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, chosen_tokens[\"prompt_attention_mask\"]\n        )\n        rejected_tokens[\"prompt_attention_mask\"] = add_tkn(\n            1, rejected_tokens[\"prompt_attention_mask\"]\n        )\n\n        # add EOS token to end of answer\n        chosen_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, chosen_tokens[\"input_ids\"])\n        chosen_tokens[\"attention_mask\"] = add_post_tkn(1, chosen_tokens[\"attention_mask\"])\n\n        rejected_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, rejected_tokens[\"input_ids\"])\n        rejected_tokens[\"attention_mask\"] = add_post_tkn(1, rejected_tokens[\"attention_mask\"])\n\n        longer_response_length = max(chosen_tokens[\"input_ids\"].shape[-1], rejected_tokens[\"input_ids\"].shape[-1])\n\n        # if combined sequence is too long, truncate the prompt\n        for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n            length_rn = answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length\n            if length_rn &gt; self.max_length:\n\n                if self.truncation_mode == \"keep_start\":\n                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                        answer_tokens[k] = answer_tokens[k][:, : self.max_prompt_length]\n                elif self.truncation_mode == \"keep_end\":\n                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                        answer_tokens[k] = answer_tokens[k][:, -self.max_prompt_length:]\n                else:\n                    raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n        # if that's still too long, truncate the response\n        for answer_tokens in [chosen_tokens, rejected_tokens]:\n            if answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length &gt; self.max_length:\n                for k in [\"input_ids\", \"attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, : self.max_length - self.max_prompt_length]\n\n        chosen_sequence_tokens = {\n            k: jnp.concatenate(\n                (v2d(chosen_tokens[f\"prompt_{k}\"]), v2d(chosen_tokens[k])),\n                axis=-1\n            ) for k in [\"input_ids\", \"attention_mask\"]\n        }\n        rejected_sequence_tokens = {\n            k: jnp.concatenate(\n                (v2d(rejected_tokens[f\"prompt_{k}\"]), v2d(rejected_tokens[k])),\n                axis=-1\n            ) for k in [\"input_ids\", \"attention_mask\"]\n        }\n        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"labels\"].at[\n                                           : len(chosen_tokens[\"prompt_input_ids\"])\n                                           ].set([self.label_pad_token_id] * len(chosen_tokens[\"prompt_input_ids\"]))\n        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"labels\"].at[\n                                             : len(rejected_tokens[\"prompt_input_ids\"])\n                                             ].set(\n            ([self.label_pad_token_id] * len(rejected_tokens[\"prompt_input_ids\"]))\n        )\n\n        for k, tokens_ in {\n            \"chosen_\": chosen_sequence_tokens,\n            \"rejected_\": rejected_sequence_tokens,\n            \"\": prompt_tokens,\n        }.items():\n            for type_key, tokens in tokens_.items():\n                if type_key == \"token_type_ids\":\n                    continue\n\n                b, s = tokens.shape\n\n                if self.max_prompt_length &gt; s:\n                    if k == \"chosen_\":\n                        if type_key == \"input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n\n                        tokens = tokens[..., :self.max_completion_length]\n\n                        if tokens.shape[-1] != self.max_completion_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_completion_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                        tokens = tokens[..., :self.max_completion_length]\n                    elif k == \"rejected_\":\n                        if type_key == \"input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_completion_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        tokens = tokens[..., :self.max_completion_length]\n                        if tokens.shape[-1] != self.max_completion_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_completion_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                    elif k == \"\":\n                        if type_key == \"prompt_input_ids\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        elif type_key == \"prompt_attention_mask\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=0,\n                                axis=-1\n                            )\n                        elif type_key == \"prompt_labels\":\n                            tokens = pad_to_length(\n                                tokens,\n                                self.max_prompt_length,\n                                pad_value=self.padding_value,\n                                axis=-1\n                            )\n                        tokens = tokens[..., :self.max_prompt_length]\n                        if tokens.shape[-1] != self.max_prompt_length:\n                            raise ValueError(\n                                f\"there was an error in padding token with `type_key` of {type_key}\"\n                                f\". it must have sequence_length of {self.max_prompt_length} but we got {tokens.shape[-1]}\"\n                                f\" From {k}{type_key}\"\n                            )\n                batch[f\"{k}{type_key}\"] = tokens\n        return batch\n\n    def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n        \"\"\"\n        The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called function_configurations, which initializes the model parameters\n         and returns\n        them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n        on a batch of data, including:\n        :param self: Access the class attributes\n        :return: A TrainerConfigureFunctionFuncOutput object\n\n        \"\"\"\n\n        def initialize_state_function():\n            initialized_parameters = self.model.init_weights(\n                jax.random.PRNGKey(0),\n                self.arguments.init_input_shape\n            )\n\n            if self.arguments.dtype == jnp.bfloat16:\n                initialized_parameters = self.model.to_bf16(initialized_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n            tx = self.tx\n            parameters = flax.core.freeze({\"params\": initialized_parameters})\n            tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n            if self.rapture is not None:\n                lora_parameters = self.lora_parameters\n                if self.arguments.dtype == jnp.bfloat16:\n                    lora_parameters = self.model.to_bf16(lora_parameters)\n                elif self.arguments.dtype == jnp.float16:\n                    lora_parameters = self.model.to_fp16(lora_parameters)\n\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=lora_parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(tx_init),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n            else:\n                return EasyDeLState.create(\n                    tx=tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=tx_init,\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n\n        def create_state_from_params_function(parameters):\n            if self.rapture is None:\n                return EasyDeLState.create(\n                    tx=self.tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n            else:\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n\n        state_shape = jax.eval_shape(initialize_state_function)\n        state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            state_shape\n        )\n        create_sharded_state_from_params_function = pjit(\n            create_state_from_params_function,\n            in_shardings=(state_partition_spec.params,),\n            out_shardings=state_partition_spec,\n            donate_argnums=(0,)\n        )\n        sharded_train_step_function = pjit(\n            create_orpo_step_function(\n                mode=\"train\",\n                beta=self.beta,\n                concatenated_forward=self.concatenated_forward,\n                batch_partition_spec=self.arguments.step_partition_spec\n            ),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(state_partition_spec, PartitionSpec(),),\n\n        )\n\n        sharded_eval_step_function = pjit(\n            create_orpo_step_function(\n                mode=\"eval\",\n                beta=self.beta,\n                concatenated_forward=self.concatenated_forward,\n                batch_partition_spec=self.arguments.step_partition_spec\n            ),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(state_partition_spec, PartitionSpec(),),\n\n        )\n\n        mesh = self.arguments.get_mesh()\n        self.arguments.ckpt_path_exists()\n        checkpoint_manager = self.arguments.get_streaming_checkpointer()\n        self.state_partition_spec = state_partition_spec\n        self.state_shape = state_shape\n\n        return TrainerConfigureFunctionFuncOutput(\n            create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n            sharded_train_step_function=sharded_train_step_function,\n            sharded_eval_step_function=sharded_eval_step_function,\n            mesh=mesh,\n            checkpoint_manager=checkpoint_manager,\n            initialize_state_function=initialize_state_function\n        )\n\n    def initialize_state(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None,\n    ) -&gt; Tuple[EasyDeLState, Mapping[str, Callable], Mapping[str, Callable]]:\n        if model_parameters is None and state is None and self.rapture is None and self.checkpoint_path is None:\n            raise RuntimeError(\n                \"You are passing `model_parameters=None`, `state=None`, and `checkpoint_path=None` and also you are not\"\n                \" using LoRA, if you are \"\n                \"Using LoRA make sure to pass parameters and Rapture Config correctly otherwise pass the \"\n                \"model_parameters or state.\"\n            )\n        if model_parameters is None and state is None:\n            model_parameters = self.lora_parameters\n        with self.mesh:\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                self.state_partition_spec,\n                dtype_specs=self.dtype\n            )\n            if state is not None:\n                sharded_state = state\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n                if sharded_state.opt_state is None:\n                    prefix_print(\n                        \"Action\", \"Optimizer State is not Found!, initializing one.\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = sharded_state.init_opt_state()\n                        opt_state = sharded_state.opt_state if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                            lambda f, x: f(x),\n                            shard_fns.opt_state,\n                            sharded_state.opt_state\n                        )\n                        sharded_state = sharded_state.replace(\n                            opt_state=opt_state\n                        )\n            elif self.finetune:\n\n                if model_parameters is None and self.checkpoint_path is not None:\n                    prefix_print(\n                        \"Action\", f\"Loading Model From {self.checkpoint_path}\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = EasyDeLState.load_state(\n                            verbose=self.arguments.verbose,\n                            state_shard_fns=shard_fns,\n                            init_optimizer_state=True,\n                            checkpoint_path=self.checkpoint_path,\n                            input_shape=self.arguments.init_input_shape,\n                            config_kwargs=self.arguments.loaded_model_config_kwargs\n                        )\n                        state_shape = jax.eval_shape(lambda: sharded_state)\n                        state_partition_spec = match_partition_rules(\n                            self.config.get_partition_rules(\n                                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                            state_shape\n                        )\n                        sharded_train_step_function = pjit(\n                            create_orpo_step_function(\n                                mode=\"train\",\n                                beta=self.beta,\n                                concatenated_forward=self.concatenated_forward,\n                                batch_partition_spec=self.arguments.step_partition_spec\n                            ),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(state_partition_spec, PartitionSpec(),),\n\n                        )\n\n                        sharded_eval_step_function = pjit(\n                            create_orpo_step_function(\n                                mode=\"eval\",\n                                beta=self.beta,\n                                concatenated_forward=self.concatenated_forward,\n                                batch_partition_spec=self.arguments.step_partition_spec\n                            ),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(state_partition_spec, PartitionSpec(),),\n                        )\n\n                        self.state_partition_spec = state_partition_spec\n                        self.state_shape = state_shape\n                        self.sharded_train_step_function = sharded_train_step_function\n                        self.sharded_eval_step_function = sharded_eval_step_function\n\n                    if self.arguments.remove_ckpt_after_load:\n                        os.remove(self.checkpoint_path)\n                elif model_parameters is not None and self.checkpoint_path is None:\n                    prefix_print(\n                        \"Action\", f\"Sharding Passed Parameters\"\n                    )\n                    from flax.core import unfreeze\n                    if not isinstance(model_parameters, flax.core.FrozenDict):\n                        prefix_print(\n                            \"Warning\",\n                            \"Model Parameters should be like FrozenDict({'params': params}) make sure to \"\n                            \"pass as type FrozenDict in case of not getting UnExcepted Errors \"\n                        )\n\n                    model_parameters = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                        lambda f, x: f(x),\n                        shard_fns.params,\n                        model_parameters,\n                    )\n                    sharded_state = self.create_sharded_state_from_params_function(model_parameters)\n                elif model_parameters is not None and self.checkpoint_path is not None:\n                    raise EasyDeLTimerError(\n                        \"You can't pass `model_parameters` and `checkpoint_path` at same time\"\n                    )\n                else:\n                    raise EasyDeLTimerError(\n                        \"You should pass `model_parameters` or `checkpoint_path` to trainer in order to load model\"\n                    )\n            else:\n                sharded_state = self.initialize_state_function()\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n\n            self.sharded_state = sharded_state\n            return sharded_state, shard_fns, gather_fns\n\n    def _save_state(\n            self,\n            state: EasyDeLState,\n            gather_fns: Optional[Any | Mapping[str, Callable] | dict[Callable]],\n            milestone: bool = False\n    ) -&gt; str:\n        step = int(\n            jax.device_get(\n                state.step\n            )\n        ) + self.arguments.step_start_point if self.arguments.step_start_point is not None else int(\n            jax.device_get(\n                state.step\n            )\n        )\n\n        checkpoint_dir = os.path.join(self.arguments.save_dir, self.arguments.model_name)\n        filename_extension = \".easy\"\n        if self.arguments.save_total_limit:\n            checkpoint_files = glob(os.path.join(checkpoint_dir, f\"*{filename_extension}\"))\n            checkpoint_files.sort(key=os.path.getmtime)\n            for old_checkpoint in checkpoint_files[:-self.arguments.save_total_limit]:\n                os.remove(old_checkpoint)\n                termcolor.cprint(f\"Removed old checkpoint: {old_checkpoint}\", color=\"red\", force_color=True)\n\n        checkpoint_name = f\"{self.arguments.model_name}-S{step}\"\n        filename = f\"{checkpoint_name}_{step}\" if milestone else f\"{checkpoint_name}\"\n        filename += \".easy\"\n        termcolor.cprint(f\"Saving Model {filename}.\", color=\"cyan\", force_color=True)\n        state.save_state(\n            filename=filename,\n            checkpoint_dir=checkpoint_dir,\n            gather_fns=gather_fns,\n            float_dtype=self.dtype,\n            verbose=self.arguments.verbose,\n            save_optimizer=self.arguments.save_optimizer_state,\n        )\n        return filename\n\n    def initialize_trainer_utils(self):\n        \"\"\"\n        The initialize_trainer_utils function is responsible for initializing the following:\n            - wandb_runtime (if you use_wandb is True)\n            - timer object (for logging time taken by various functions)\n            - dataloader objects for training and evaluation data, along with max steps per epoch.\n              The configure_dataloader function accomplishes this task.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of functions\n\n        \"\"\"\n        self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n        self.timer = Timers(\n            use_wandb=False,\n            tensorboard_writer=self.arguments.get_board()\n        )\n\n        self.timer(\"configure dataloaders\").start()\n        dataset_configurations = self.configure_dataloader()\n        self.dataloader_train = dataset_configurations.dataloader_train\n        self.max_training_steps = dataset_configurations.max_training_steps\n        self.dataloader_eval = dataset_configurations.dataloader_eval\n        self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n        self.timer(\"configure dataloaders\").stop()\n\n        self.timer.log([\"configure dataloaders\"])\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n        model_configurations = self.configure_model()\n        model = model_configurations.model\n        tx = model_configurations.tx\n        scheduler = model_configurations.scheduler\n        config = model_configurations.config\n        self.model = model\n        self.tx = tx\n        self.scheduler = scheduler\n        self.config = config\n        if self.rapture is not None:\n            lora_modules = self.rapture.apply_lora(\n                module=model,\n                parameters=self.arguments.rapture_config.parameters,\n                tx=tx,\n            )\n            self.lora_parameters = lora_modules.lora_parameters\n            self.lora_apply_fn = lora_modules.lora_module.__call__\n            self.lora_opt_state = lora_modules.lora_opt_state\n            self.lora_model = lora_modules.lora_module\n            self.lora_tx = lora_modules.lora_tx\n\n        self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n        self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n\n        self.timer(\"configure functions and sharding them\").start()\n\n        function_configurations = self.configure_functions()\n        self.create_sharded_state_from_params_function = (\n            function_configurations.create_sharded_state_from_params_function\n        )\n        self.sharded_train_step_function = function_configurations.sharded_train_step_function\n        self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n        self.mesh = function_configurations.mesh\n        self.checkpoint_manager = function_configurations.checkpoint_manager\n        self.initialize_state_function = function_configurations.initialize_state_function\n        self.timer(\"configure functions and sharding them\").stop()\n        self.timer.log([\"configure functions and sharding them\"])\n\n    def create_collate_function(\n            self,\n            max_sequence_length: int,\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n    ) -&gt; Callable:\n        return self.data_collator\n\n    def shard_states(self, state, rules):\n        with self.arguments.get_mesh():\n            partition_spec = match_partition_rules(rules=rules, params=jax.eval_shape(lambda: state))\n\n            def _shard(x):\n                return x\n\n            shard = pjit(\n                _shard,\n                in_shardings=(PartitionSpec(),),\n                out_shardings=partition_spec\n            )\n            return shard(state)\n\n    def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n        dataloader_train = self.get_train_dataloader()\n        max_evaluation_steps = None\n        dataloader_eval = None\n\n        max_training_steps = self.arguments.num_train_epochs * len(\n            dataloader_train\n        ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n        if self.eval_dataset is not None:\n            dataloader_eval = self.get_eval_dataloader(self.eval_dataset)\n            max_evaluation_steps = len(dataloader_eval)\n        return TrainerConfigureDataloaderFuncOutput(\n            dataloader_train=dataloader_train,  # type:ignore\n            max_training_steps=max_training_steps,\n            dataloader_eval=dataloader_eval,\n            max_evaluation_steps=max_evaluation_steps\n        )\n\n    def _get_train_dataloader(self) -&gt; tensorflow.data.Dataset:\n\n        \"\"\"\n        The _get_train_dataloader function is used to create a tensorflow.data.Dataset object for the training dataset.\n\n        :param self: Represent the instance of the class\n        :return: A dataloader object\n        \"\"\"\n        if self.train_dataset is None:\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\n\n        train_dataset = self.train_dataset\n        data_collator = self.data_collator\n\n        return tensorflow_datasets.as_numpy(\n            train_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=True,\n                drop_remainder=True\n            )\n        )\n\n    def _get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the evaluation [`~tensorflow.data.Dataset`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n\n        return tensorflow_datasets.as_numpy(\n            eval_dataset.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                collate_fn=self.data_collator,\n                num_workers=self.arguments.dataloader_num_workers,\n                shuffle=False,\n                drop_remainder=True\n            )\n        )\n\n    def get_train_dataloader(\n            self,\n    ) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the training [`~tensorflow.data.Dataset`].\n        \"\"\"\n        return self._get_train_dataloader()\n\n    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n        \"\"\"\n        Returns the evaluation [`~tensorflow.data.Dataset`].\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        return self._get_eval_dataloader(eval_dataset=eval_dataset)\n\n    def train(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None\n    ) -&gt; ORPOTrainerOutput:\n        def get_layer_names(frozen_dict, prefix=\"\"):\n            layer_names = {}\n            for key, value in frozen_dict.items():\n                if isinstance(value, FrozenDict):\n                    layer_names.update(get_layer_names(value, prefix=f\"{prefix}_{key}\"))\n                else:\n                    layer_name = f\"{prefix}_{key}\".lstrip(\"/\")\n                    layer_names[layer_name] = value\n            return layer_names\n\n        def count_model_parameters(_p):\n            termcolor.cprint(\n                f\"Model Contain {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9} \"\n                f\"Billion Parameters\",\n                color=\"red\", force_color=True\n            )\n\n        checkpoint_path = \"SAVING_SKIPPED\"\n        if self.arguments.performance_mode:\n            termcolor.cprint(\n                \"Performance Mode is ON, we will ignore the Memory Tracking, WANDB Logging, and extra information \"\n                \"Process.\",\n                color=\"red\",\n                force_color=True\n            )\n        sharded_state, shard_fns, gather_fns = self.initialize_state(\n            model_parameters=model_parameters,\n            state=state\n        )\n        self.model_state = sharded_state\n        count_model_parameters(sharded_state.params)\n        with self.mesh:\n            with jax.default_device(jax.devices(\"cpu\")[0]) if self.low_mem_usage else leave_alone_context_manager():\n                dir_prefix: str = \"/dev/shm\" if sys.platform != \"win32\" else \".\"\n                checkpoint_path = \"SAVING_SKIPPED\"\n\n                pbar = tqdm(total=self.max_training_steps)\n                pbar.set_description(\"Training\")\n                current_step = self.model_state.step.tolist() if isinstance(\n                    self.model_state.step,\n                    jax.Array\n                ) else self.model_state.step\n\n                loss_sum = None\n\n                try:\n                    for epoch_index in range(self.arguments.num_train_epochs):\n                        for batch in self.dataloader_train:\n                            current_step += 1\n                            if self.arguments.step_start_point &gt; current_step:\n                                ...\n                            elif current_step &lt; self.max_training_steps:\n                                time_start = time.time()\n\n                                self.model_state, outputs = self.sharded_train_step_function(\n                                    self.model_state,\n                                    batch\n                                )\n                                total_time = time.time() - time_start\n                                (loss, metrics) = outputs.loss, outputs.metrics\n\n                                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n\n                                train_metrics = {\n                                    \"train/loss\": loss.tolist(),\n                                    \"train/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                                    \"train/learning_rate\": self.scheduler(\n                                        jax.device_get(self.model_state.step)).tolist(),\n                                    \"train/step\": current_step,\n                                    \"train/step_time\": total_time,\n                                    \"train/perplexity\": jnp.exp(loss).tolist(),\n                                    \"train/epoch\": epoch_index\n                                }\n                                train_metrics.update(metrics)\n                                log_metrics = copy.deepcopy(train_metrics)\n                                train_metrics.update(self.arguments.captured_memory)\n                                if self.arguments.use_wandb:\n                                    with jax.spmd_mode(\"allow_all\"):\n                                        self.wandb_runtime.log(\n                                            train_metrics\n                                        )\n                                pbar.update(1)\n                                pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in log_metrics.items()})\n                            else:\n                                break\n                except KeyboardInterrupt:\n                    termcolor.cprint(\n                        \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                        color=\"cyan\",\n                        force_color=True\n                    )\n\n                except EasyDeLTimerError:\n                    termcolor.cprint(\n                        \"Training reached out maximum training Time Killing training Process \"\n                        \"and Will return Current State of the Model with Parameters.\",\n                        color=\"cyan\",\n                        force_color=True\n                    )\n\n                if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n                    print(\n                        termcolor.colored(\n                            \"Info : \", color=\"red\", force_color=True\n                        ),\n                        termcolor.colored(\n                            \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                        )\n                    )\n                    self.model_state = self.model_state.replace(\n                        params=self.rapture.merge_parameters(self.model_state.params)\n                    )\n\n                shard_fns, gather_fns = make_shard_and_gather_fns(\n                    partition_specs=match_partition_rules(\n                        rules=self.model_state.module.config.get_partition_rules(\n                            self.arguments.fully_sharded_data_parallel\n                        ),\n                        params=jax.eval_shape(lambda: self.model_state)\n                    ),\n                    dtype_specs=self.arguments.dtype\n                )\n                output = ORPOTrainerOutput(\n                    state=self.model_state,\n                    mesh=self.mesh,\n                    shard_fns=shard_fns,\n                    gather_fns=gather_fns,\n                    checkpoint_manager=self.checkpoint_manager,\n                )\n                if self.arguments.save_steps is None and self.arguments.do_last_save:\n                    shard_fns, gather_fns = make_shard_and_gather_fns(\n                        match_partition_rules(\n                            self.config.get_partition_rules(\n                                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                            jax.eval_shape(lambda: self.model_state)\n                        ),\n                        dtype_specs=self.dtype\n                    )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n                    # crashing errors and saving errors\n                    filename = self._save_state(\n                        state=self.model_state,\n                        gather_fns=gather_fns\n                    )\n                    checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n                if self.arguments.do_eval:\n                    for _ in self.eval(\n                            self.model_state\n                    ):\n                        ...\n\n                output.checkpoint_path = checkpoint_path\n                output.last_save_file_name = filename\n                self.finish()\n\n        return output\n\n    def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n        \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n        assert self.eval_dataset is not None, \"`dataloader_eval` is required by evaluator function.\"\n        with self.mesh:\n            pbar = tqdm(total=self.max_evaluation_steps)\n            pbar.set_description(\"Evaluating\")\n            current_step = 0\n            loss_sum = None\n            try:\n                for batch in self.dataloader_eval:\n                    current_step += 1\n                    time_start = time.time()\n                    for key in self.arguments.ids_to_pop_from_dataset:\n                        _ = batch.pop(key, None)\n                    for key in list(batch.keys()):\n                        if not (\n                                key.endswith(\"_input_ids\")\n                                or key.endswith(\"_attention_mask\")\n                                or key.endswith(\"_labels\")\n                        ):\n                            _ = batch.pop(key, None)\n\n                    _, outputs = self.sharded_eval_step_function(\n                        model_state,\n                        batch\n                    )\n                    total_time = time.time() - time_start\n                    (\n                        loss, metrics\n                    ) = outputs.loss, outputs.metrics\n\n                    loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n\n                    eval_metrics = {\n                        \"eval/loss\": loss.tolist(),\n                        \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                        \"eval/step\": current_step,\n                        \"eval/step_time\": total_time,\n                        \"eval/perplexity\": jnp.exp(loss).tolist(),\n                    }\n                    eval_metrics.update(metrics)\n                    log_metrics = copy.deepcopy(eval_metrics)\n                    eval_metrics.update(self.arguments.captured_memory)\n                    if self.arguments.use_wandb:\n                        with jax.spmd_mode(\"allow_all\"):\n                            self.wandb_runtime.log(\n                                eval_metrics\n                            )\n\n                    pbar.update(1)\n                    pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                    yield eval_metrics\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n\n    def __repr__(self):\n\n        \"\"\"\n        The __repr__ function is used to generate a string representation of an object.\n        This function should return a string that can be parsed by the Python interpreter\n        to recreate the object. The __repr__ function is called when you use print() on an\n        object, or when you type its name in the REPL.\n\n        :param self: Refer to the instance of the class\n        :return: A string representation of the object\n        \"\"\"\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                try:\n                    repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n                except TypeError:\n                    repr_src = f\"\\t{k} : \" + \"EasyDeLReadingError\" + \"\\n\"\n                    string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n\n        return string + \")\"\n\n    def __str__(self):\n\n        \"\"\"\n        The __str__ function is called when you use the print function or when str() is used.\n        It should return a string representation of the object.\n\n        :param self: Refer to the instance of the class\n        :return: The object's string representation\n        \"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.__init__","title":"<code>__init__(arguments, max_length=None, max_prompt_length=None, max_completion_length=None, beta=0.1, disable_dropout=True, label_pad_token_id=-100, is_encoder_decoder=False, padding_value=None, data_collator=None, train_dataset=None, eval_dataset=None, tokenizer=None, dataset_num_proc=None, _do_init_fns=True, dataset_map_arguments=None, low_mem_usage=False)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object.</p> <p>:param self: Refer to the object itself :param beta: float: Control the strength of the regularization term :param arguments: TrainArguments: Pass the arguments to the trainer :param label_pad_token_id: int: Pad the labels :param padding_value: int: Specify the value that is used for padding :param train_dataset: Optional[Dataset]: Load the training dataset :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer :param max_length: Optional[int]: Set the maximum length of the input sequence :param max_prompt_length: Optional[int]: Set the maximum length of the prompt :param max_completion_length: Optional[int]: Truncate the target sequence :param data_collator: Optional[Callable]: Function to be used for creating datasets. tokenizing process with <code>dataset.map</code>. :parma dataset_num_proc: Optional[int]: The number of processes to use for the dataset mapping. :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure model with provided training Arguments :param : Set the padding value for the model</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def __init__(\n        self,\n        arguments: TrainArguments,\n        max_length: Optional[int] = None,\n        max_prompt_length: Optional[int] = None,\n        max_completion_length: Optional[int] = None,\n        beta: float = 0.1,\n        disable_dropout: bool = True,\n        label_pad_token_id: int = -100,\n        is_encoder_decoder: bool = False,\n        padding_value: int = None,\n        data_collator: Optional[DPODataCollatorWithPadding] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        dataset_num_proc: Optional[int] = None,\n        _do_init_fns: bool = True,\n        dataset_map_arguments: Optional[Dict[str, Any]] = None,\n        low_mem_usage: bool = False,\n):\n\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object.\n\n    :param self: Refer to the object itself\n    :param beta: float: Control the strength of the regularization term\n    :param arguments: TrainArguments: Pass the arguments to the trainer\n    :param label_pad_token_id: int: Pad the labels\n    :param padding_value: int: Specify the value that is used for padding\n    :param train_dataset: Optional[Dataset]: Load the training dataset\n    :param eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] : Pass the evaluation dataset to the trainer\n    :param tokenizer: Optional[PreTrainedTokenizerBase]: Pass the tokenizer to the trainer\n    :param max_length: Optional[int]: Set the maximum length of the input sequence\n    :param max_prompt_length: Optional[int]: Set the maximum length of the prompt\n    :param max_completion_length: Optional[int]: Truncate the target sequence\n    :param data_collator: Optional[Callable]: Function to be used for creating datasets.\n    tokenizing process with `dataset.map`.\n    :parma dataset_num_proc: Optional[int]: The number of processes to use for the dataset mapping.\n    :param _do_init_fns: bool : preferred to set ture to trainer will automatically configure\n    model with provided training Arguments\n    :param : Set the padding value for the model\n    \"\"\"\n\n    assert arguments is not None, (\n        \"You Have to pass arguments that will be used for training but you have passed\"\n        \"`arguments=None`\"\n    )\n    assert isinstance(arguments, TrainArguments), (\n        f\"arguments type must be `TrainArguments` but got {type(arguments)}\"\n    )\n\n    if tokenizer is None:\n        raise ValueError(\"tokenizer must be specified to tokenize a ORPO dataset.\")\n    if max_length is None:\n        warnings.warn(\n            \"`max_length` is not set in the ORPOTrainer's init\"\n            \" it will default to `512` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_length = 512\n    if max_prompt_length is None:\n        warnings.warn(\n            \"`max_prompt_length` is not set in the ORPOTrainer's init\"\n            \" it will default to `128` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_prompt_length = 128\n\n    if max_completion_length is None:\n        warnings.warn(\n            \"When using an encoder decoder architecture, you should set `max_completion_length` in the \"\n            \"ORPOTrainer's init it will default to `128` by default, but you should do it yourself in the future.\",\n            UserWarning,\n        )\n        max_completion_length = 128\n\n    padding_value = padding_value if padding_value is not None else tokenizer.pad_token_id\n    self.max_length = max_length\n    self.label_pad_token_id = label_pad_token_id\n    self.padding_value = padding_value\n    self.max_prompt_length = max_prompt_length\n    self.truncation_mode = arguments.truncation_mode\n    self.disable_dropout = disable_dropout\n    self.max_completion_length = max_completion_length\n    self.tokenizer = tokenizer\n    self.is_encoder_decoder = is_encoder_decoder\n    self.low_mem_usage = low_mem_usage\n    self.beta = beta\n    self.dataset_num_proc = dataset_num_proc\n    data_collator = DPODataCollatorWithPadding(\n        max_prompt_length=self.max_prompt_length,\n        max_target_length=self.max_completion_length,\n        pad_token_id=tokenizer.pad_token_id,\n        label_pad_token_id=label_pad_token_id,\n        is_encoder_decoder=False,\n    ) if data_collator is None else data_collator\n    self._stored_metrics = defaultdict(lambda: defaultdict(list))\n    if dataset_map_arguments is None:\n        dataset_map_arguments = {}\n    train_dataset = train_dataset.map(\n        self.tokenize_row,\n        num_proc=dataset_num_proc,\n        **dataset_map_arguments\n    )\n    if eval_dataset is not None:\n        eval_dataset = eval_dataset.map(\n            self.tokenize_row,\n            num_proc=dataset_num_proc,\n            **dataset_map_arguments\n        )\n\n    self.arguments = arguments\n    self.hp_name = None\n    self.deepspeed = None\n    self.is_in_train = False\n\n    self.data_collator = data_collator\n    self.train_dataset = train_dataset\n    self.eval_dataset = eval_dataset\n    self.tokenizer = tokenizer\n    self._loggers_initialized = False\n    self.mesh = self.arguments.get_mesh()\n    assert padding_value is not None, \"`padding_value` can not be set as `None` it must be an integer.\"\n\n    self.concatenated_forward = create_concatenated_forward(\n        is_encoder_decoder=self.is_encoder_decoder,\n        padding_value=padding_value,\n        label_pad_token_id=label_pad_token_id,\n    )\n\n    self._cached_p_l_s = None\n    self._cached_c_l_s = None\n    self._cached_r_l_s = None\n\n    super().__init__(\n        arguments=arguments,\n        dataset_train=train_dataset,\n        dataset_eval=eval_dataset,\n        finetune=True,\n        checkpoint_path=None,\n        _do_init_fns=_do_init_fns\n    )\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.__repr__","title":"<code>__repr__()</code>","text":"<p>The repr function is used to generate a string representation of an object. This function should return a string that can be parsed by the Python interpreter to recreate the object. The repr function is called when you use print() on an object, or when you type its name in the REPL.</p> <p>:param self: Refer to the instance of the class :return: A string representation of the object</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def __repr__(self):\n\n    \"\"\"\n    The __repr__ function is used to generate a string representation of an object.\n    This function should return a string that can be parsed by the Python interpreter\n    to recreate the object. The __repr__ function is called when you use print() on an\n    object, or when you type its name in the REPL.\n\n    :param self: Refer to the instance of the class\n    :return: A string representation of the object\n    \"\"\"\n    string = f\"{self.__class__.__name__}(\\n\"\n    for k, v in self.__dict__.items():\n        if not k.startswith(\"_\"):\n            try:\n                repr_src = f\"\\t{k} : \" + v.__str__().replace(\"\\n\", \"\\n\\t\") + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n            except TypeError:\n                repr_src = f\"\\t{k} : \" + \"EasyDeLReadingError\" + \"\\n\"\n                string += repr_src if len(repr_src) &lt; 350 else f\"\\t{k} : \" + f\"{v.__class__.__name__}(...)\" + \"\\n\"\n\n    return string + \")\"\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.__str__","title":"<code>__str__()</code>","text":"<p>The str function is called when you use the print function or when str() is used. It should return a string representation of the object.</p> <p>:param self: Refer to the instance of the class :return: The object's string representation</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def __str__(self):\n\n    \"\"\"\n    The __str__ function is called when you use the print function or when str() is used.\n    It should return a string representation of the object.\n\n    :param self: Refer to the instance of the class\n    :return: The object's string representation\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.build_tokenized_answer","title":"<code>build_tokenized_answer(prompt, answer)</code>","text":"<p>Llama tokenizer does satisfy <code>enc(a + b) = enc(a) + enc(b)</code>. It does ensure <code>enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]</code>.</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def build_tokenized_answer(self, prompt, answer):\n    \"\"\"\n    Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n    It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n    \"\"\"\n\n    full_tokenized = self.tokenizer(prompt + answer, add_special_tokens=False)\n    prompt_input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n    answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids):]\n    answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids):]\n    prompt_input_ids = jnp.asarray(prompt_input_ids, dtype=\"i4\")\n    answer_input_ids = jnp.asarray(answer_input_ids, dtype=\"i4\")\n    full_concat_input_ids = jnp.concatenate(\n        (\n            prompt_input_ids,\n            answer_input_ids\n        )\n    )\n\n    # Prepare input tokens for token by token comparison\n    full_input_ids = jnp.array(full_tokenized[\"input_ids\"])\n\n    if len(full_input_ids) != len(full_concat_input_ids):\n        raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n\n    response_token_ids_start_idx = len(prompt_input_ids)\n    if prompt_input_ids.tolist() != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n        response_token_ids_start_idx -= 1\n\n    prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n    prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n\n    if len(prompt_input_ids) != len(prompt_attention_mask):\n        raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n\n    answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n    answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n\n    return dict(\n        prompt_input_ids=jnp.array(prompt_input_ids, dtype=\"i4\"),\n        prompt_attention_mask=jnp.array(prompt_attention_mask, dtype=\"i4\"),\n        input_ids=jnp.array(answer_input_ids, dtype=\"i4\"),\n        attention_mask=jnp.array(answer_attention_mask, dtype=\"i4\"),\n    )\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.configure_functions","title":"<code>configure_functions()</code>","text":"<p>The configure_functions function is responsible for configuring the functions that will be used in training. It does this by first defining a function called function_configurations, which initializes the model parameters  and returns them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate on a batch of data, including: :param self: Access the class attributes :return: A TrainerConfigureFunctionFuncOutput object</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n    \"\"\"\n    The configure_functions function is responsible for configuring the functions that will be used in training.\n    It does this by first defining a function called function_configurations, which initializes the model parameters\n     and returns\n    them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n    on a batch of data, including:\n    :param self: Access the class attributes\n    :return: A TrainerConfigureFunctionFuncOutput object\n\n    \"\"\"\n\n    def initialize_state_function():\n        initialized_parameters = self.model.init_weights(\n            jax.random.PRNGKey(0),\n            self.arguments.init_input_shape\n        )\n\n        if self.arguments.dtype == jnp.bfloat16:\n            initialized_parameters = self.model.to_bf16(initialized_parameters)\n        elif self.arguments.dtype == jnp.float16:\n            initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n        tx = self.tx\n        parameters = flax.core.freeze({\"params\": initialized_parameters})\n        tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n        if self.rapture is not None:\n            lora_parameters = self.lora_parameters\n            if self.arguments.dtype == jnp.bfloat16:\n                lora_parameters = self.model.to_bf16(lora_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                lora_parameters = self.model.to_fp16(lora_parameters)\n\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=lora_parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(tx_init),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n        else:\n            return EasyDeLState.create(\n                tx=tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=tx_init,\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n\n    def create_state_from_params_function(parameters):\n        if self.rapture is None:\n            return EasyDeLState.create(\n                tx=self.tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n        else:\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n\n    state_shape = jax.eval_shape(initialize_state_function)\n    state_partition_spec = match_partition_rules(\n        self.config.get_partition_rules(\n            fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n        state_shape\n    )\n    create_sharded_state_from_params_function = pjit(\n        create_state_from_params_function,\n        in_shardings=(state_partition_spec.params,),\n        out_shardings=state_partition_spec,\n        donate_argnums=(0,)\n    )\n    sharded_train_step_function = pjit(\n        create_orpo_step_function(\n            mode=\"train\",\n            beta=self.beta,\n            concatenated_forward=self.concatenated_forward,\n            batch_partition_spec=self.arguments.step_partition_spec\n        ),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(state_partition_spec, PartitionSpec(),),\n\n    )\n\n    sharded_eval_step_function = pjit(\n        create_orpo_step_function(\n            mode=\"eval\",\n            beta=self.beta,\n            concatenated_forward=self.concatenated_forward,\n            batch_partition_spec=self.arguments.step_partition_spec\n        ),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(state_partition_spec, PartitionSpec(),),\n\n    )\n\n    mesh = self.arguments.get_mesh()\n    self.arguments.ckpt_path_exists()\n    checkpoint_manager = self.arguments.get_streaming_checkpointer()\n    self.state_partition_spec = state_partition_spec\n    self.state_shape = state_shape\n\n    return TrainerConfigureFunctionFuncOutput(\n        create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n        sharded_train_step_function=sharded_train_step_function,\n        sharded_eval_step_function=sharded_eval_step_function,\n        mesh=mesh,\n        checkpoint_manager=checkpoint_manager,\n        initialize_state_function=initialize_state_function\n    )\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.eval","title":"<code>eval(model_state)</code>","text":"<p>Evaluate the Given Model State and yield the eval metrics</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n    \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n    assert self.eval_dataset is not None, \"`dataloader_eval` is required by evaluator function.\"\n    with self.mesh:\n        pbar = tqdm(total=self.max_evaluation_steps)\n        pbar.set_description(\"Evaluating\")\n        current_step = 0\n        loss_sum = None\n        try:\n            for batch in self.dataloader_eval:\n                current_step += 1\n                time_start = time.time()\n                for key in self.arguments.ids_to_pop_from_dataset:\n                    _ = batch.pop(key, None)\n                for key in list(batch.keys()):\n                    if not (\n                            key.endswith(\"_input_ids\")\n                            or key.endswith(\"_attention_mask\")\n                            or key.endswith(\"_labels\")\n                    ):\n                        _ = batch.pop(key, None)\n\n                _, outputs = self.sharded_eval_step_function(\n                    model_state,\n                    batch\n                )\n                total_time = time.time() - time_start\n                (\n                    loss, metrics\n                ) = outputs.loss, outputs.metrics\n\n                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n\n                eval_metrics = {\n                    \"eval/loss\": loss.tolist(),\n                    \"eval/mean_loss\": loss_sum / (current_step - self.arguments.step_start_point),\n                    \"eval/step\": current_step,\n                    \"eval/step_time\": total_time,\n                    \"eval/perplexity\": jnp.exp(loss).tolist(),\n                }\n                eval_metrics.update(metrics)\n                log_metrics = copy.deepcopy(eval_metrics)\n                eval_metrics.update(self.arguments.captured_memory)\n                if self.arguments.use_wandb:\n                    with jax.spmd_mode(\"allow_all\"):\n                        self.wandb_runtime.log(\n                            eval_metrics\n                        )\n\n                pbar.update(1)\n                pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                yield eval_metrics\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                color=\"cyan\",\n                force_color=True\n            )\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.get_eval_dataloader","title":"<code>get_eval_dataloader(eval_dataset=None)</code>","text":"<p>Returns the evaluation [<code>~tensorflow.data.Dataset</code>].</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -&gt; tensorflow.data.Dataset:\n    \"\"\"\n    Returns the evaluation [`~tensorflow.data.Dataset`].\n    \"\"\"\n    if eval_dataset is None and self.eval_dataset is None:\n        raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n    eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n    return self._get_eval_dataloader(eval_dataset=eval_dataset)\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.get_train_dataloader","title":"<code>get_train_dataloader()</code>","text":"<p>Returns the training [<code>~tensorflow.data.Dataset</code>].</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def get_train_dataloader(\n        self,\n) -&gt; tensorflow.data.Dataset:\n    \"\"\"\n    Returns the training [`~tensorflow.data.Dataset`].\n    \"\"\"\n    return self._get_train_dataloader()\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.initialize_trainer_utils","title":"<code>initialize_trainer_utils()</code>","text":"The initialize_trainer_utils function is responsible for initializing the following <ul> <li>wandb_runtime (if you use_wandb is True)</li> <li>timer object (for logging time taken by various functions)</li> <li>dataloader objects for training and evaluation data, along with max steps per epoch.   The configure_dataloader function accomplishes this task.</li> </ul> <p>:param self: Represent the instance of the class :return: A tuple of functions</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def initialize_trainer_utils(self):\n    \"\"\"\n    The initialize_trainer_utils function is responsible for initializing the following:\n        - wandb_runtime (if you use_wandb is True)\n        - timer object (for logging time taken by various functions)\n        - dataloader objects for training and evaluation data, along with max steps per epoch.\n          The configure_dataloader function accomplishes this task.\n\n    :param self: Represent the instance of the class\n    :return: A tuple of functions\n\n    \"\"\"\n    self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n    self.timer = Timers(\n        use_wandb=False,\n        tensorboard_writer=self.arguments.get_board()\n    )\n\n    self.timer(\"configure dataloaders\").start()\n    dataset_configurations = self.configure_dataloader()\n    self.dataloader_train = dataset_configurations.dataloader_train\n    self.max_training_steps = dataset_configurations.max_training_steps\n    self.dataloader_eval = dataset_configurations.dataloader_eval\n    self.max_evaluation_steps = dataset_configurations.max_evaluation_steps\n\n    self.timer(\"configure dataloaders\").stop()\n\n    self.timer.log([\"configure dataloaders\"])\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").start()\n    model_configurations = self.configure_model()\n    model = model_configurations.model\n    tx = model_configurations.tx\n    scheduler = model_configurations.scheduler\n    config = model_configurations.config\n    self.model = model\n    self.tx = tx\n    self.scheduler = scheduler\n    self.config = config\n    if self.rapture is not None:\n        lora_modules = self.rapture.apply_lora(\n            module=model,\n            parameters=self.arguments.rapture_config.parameters,\n            tx=tx,\n        )\n        self.lora_parameters = lora_modules.lora_parameters\n        self.lora_apply_fn = lora_modules.lora_module.__call__\n        self.lora_opt_state = lora_modules.lora_opt_state\n        self.lora_model = lora_modules.lora_module\n        self.lora_tx = lora_modules.lora_tx\n\n    self.timer(\"configure Model, Optimizer, Scheduler and Config\").stop()\n    self.timer.log([\"configure Model, Optimizer, Scheduler and Config\"])\n\n    self.timer(\"configure functions and sharding them\").start()\n\n    function_configurations = self.configure_functions()\n    self.create_sharded_state_from_params_function = (\n        function_configurations.create_sharded_state_from_params_function\n    )\n    self.sharded_train_step_function = function_configurations.sharded_train_step_function\n    self.sharded_eval_step_function = function_configurations.sharded_eval_step_function\n    self.mesh = function_configurations.mesh\n    self.checkpoint_manager = function_configurations.checkpoint_manager\n    self.initialize_state_function = function_configurations.initialize_state_function\n    self.timer(\"configure functions and sharding them\").stop()\n    self.timer.log([\"configure functions and sharding them\"])\n</code></pre>"},{"location":"generated-trainer-orpo-orpo_trainer/#src.python.easydel.trainer.orpo.orpo_trainer.ORPOTrainer.tokenize_row","title":"<code>tokenize_row(feature, state=None)</code>","text":"<p>The tokenize_row function is responsible for taking a single row of data and converting it into the format that the model expects. This includes: - Tokenizing the text (using HuggingFace's tokenizer) - Padding/truncating sequences to a fixed length (if necessary) - Creating attention masks, which tell the model which tokens are padding and which aren't.</p> <p>:param self: Represent the instance of the class :param feature: Pass in the data from the dataset :param state: EasyDeLState: Keep track of the state of the tokenizer :return: A dictionary of the following keys</p> Source code in <code>src/python/easydel/trainer/orpo/orpo_trainer.py</code> <pre><code>def tokenize_row(self, feature, state: EasyDeLState = None) -&gt; Dict:\n\n    \"\"\"\n    The tokenize_row function is responsible for taking a single row of data and converting it into the format that\n    the model expects. This includes:\n    - Tokenizing the text (using HuggingFace's tokenizer)\n    - Padding/truncating sequences to a fixed length (if necessary)\n    - Creating attention masks, which tell the model which tokens are padding and which aren't.\n\n    :param self: Represent the instance of the class\n    :param feature: Pass in the data from the dataset\n    :param state: EasyDeLState: Keep track of the state of the tokenizer\n    :return: A dictionary of the following keys\n    \"\"\"\n    batch = {}\n    prompt = feature[\"prompt\"]\n    chosen = feature[\"chosen\"]\n    rejected = feature[\"rejected\"]\n\n    if not isinstance(prompt, str):\n        raise ValueError(f\"prompt should be an str but got {type(prompt)} , {prompt}\")\n    prompt_tokens = self.tokenizer(\n        prompt,\n        add_special_tokens=False,\n        return_tensors=\"np\",\n    )\n    prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n\n    if not isinstance(chosen, str):\n        raise ValueError(f\"chosen should be an str but got {type(chosen)} , {chosen}\")\n    chosen_tokens = self.build_tokenized_answer(prompt, chosen)\n\n    if not isinstance(rejected, str):\n        raise ValueError(f\"rejected should be an str but got {type(rejected)}\")\n    rejected_tokens = self.build_tokenized_answer(prompt, rejected)\n    v2d = lambda ar: ar.reshape(1, -1) if ar.ndim == 1 else ar\n\n    def add_tkn(n, ar):\n        return jnp.concatenate(\n            (\n                jnp.array(n).reshape(1, 1),\n                v2d(ar)\n            ), axis=-1\n        )\n\n    def add_post_tkn(n, ar):\n        return jnp.concatenate(\n            (\n                v2d(ar),\n                jnp.array(n).reshape(1, 1)\n            ), axis=-1\n        )\n\n    prompt_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        prompt_tokens[\"prompt_input_ids\"]\n    )\n    chosen_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        chosen_tokens[\"prompt_input_ids\"]\n    )\n    rejected_tokens[\"prompt_input_ids\"] = add_tkn(\n        self.tokenizer.bos_token_id,\n        rejected_tokens[\"prompt_input_ids\"]\n    )\n\n    prompt_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, prompt_tokens[\"prompt_attention_mask\"]\n    )\n    chosen_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, chosen_tokens[\"prompt_attention_mask\"]\n    )\n    rejected_tokens[\"prompt_attention_mask\"] = add_tkn(\n        1, rejected_tokens[\"prompt_attention_mask\"]\n    )\n\n    # add EOS token to end of answer\n    chosen_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, chosen_tokens[\"input_ids\"])\n    chosen_tokens[\"attention_mask\"] = add_post_tkn(1, chosen_tokens[\"attention_mask\"])\n\n    rejected_tokens[\"input_ids\"] = add_post_tkn(self.tokenizer.eos_token_id, rejected_tokens[\"input_ids\"])\n    rejected_tokens[\"attention_mask\"] = add_post_tkn(1, rejected_tokens[\"attention_mask\"])\n\n    longer_response_length = max(chosen_tokens[\"input_ids\"].shape[-1], rejected_tokens[\"input_ids\"].shape[-1])\n\n    # if combined sequence is too long, truncate the prompt\n    for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n        length_rn = answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length\n        if length_rn &gt; self.max_length:\n\n            if self.truncation_mode == \"keep_start\":\n                for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, : self.max_prompt_length]\n            elif self.truncation_mode == \"keep_end\":\n                for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n                    answer_tokens[k] = answer_tokens[k][:, -self.max_prompt_length:]\n            else:\n                raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n    # if that's still too long, truncate the response\n    for answer_tokens in [chosen_tokens, rejected_tokens]:\n        if answer_tokens[\"prompt_input_ids\"].shape[-1] + longer_response_length &gt; self.max_length:\n            for k in [\"input_ids\", \"attention_mask\"]:\n                answer_tokens[k] = answer_tokens[k][:, : self.max_length - self.max_prompt_length]\n\n    chosen_sequence_tokens = {\n        k: jnp.concatenate(\n            (v2d(chosen_tokens[f\"prompt_{k}\"]), v2d(chosen_tokens[k])),\n            axis=-1\n        ) for k in [\"input_ids\", \"attention_mask\"]\n    }\n    rejected_sequence_tokens = {\n        k: jnp.concatenate(\n            (v2d(rejected_tokens[f\"prompt_{k}\"]), v2d(rejected_tokens[k])),\n            axis=-1\n        ) for k in [\"input_ids\", \"attention_mask\"]\n    }\n    chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n    chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"labels\"].at[\n                                       : len(chosen_tokens[\"prompt_input_ids\"])\n                                       ].set([self.label_pad_token_id] * len(chosen_tokens[\"prompt_input_ids\"]))\n    rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n    rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"labels\"].at[\n                                         : len(rejected_tokens[\"prompt_input_ids\"])\n                                         ].set(\n        ([self.label_pad_token_id] * len(rejected_tokens[\"prompt_input_ids\"]))\n    )\n\n    for k, tokens_ in {\n        \"chosen_\": chosen_sequence_tokens,\n        \"rejected_\": rejected_sequence_tokens,\n        \"\": prompt_tokens,\n    }.items():\n        for type_key, tokens in tokens_.items():\n            if type_key == \"token_type_ids\":\n                continue\n\n            b, s = tokens.shape\n\n            if self.max_prompt_length &gt; s:\n                if k == \"chosen_\":\n                    if type_key == \"input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n\n                    tokens = tokens[..., :self.max_completion_length]\n\n                    if tokens.shape[-1] != self.max_completion_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_completion_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n                    tokens = tokens[..., :self.max_completion_length]\n                elif k == \"rejected_\":\n                    if type_key == \"input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_completion_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    tokens = tokens[..., :self.max_completion_length]\n                    if tokens.shape[-1] != self.max_completion_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_completion_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n                elif k == \"\":\n                    if type_key == \"prompt_input_ids\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    elif type_key == \"prompt_attention_mask\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=0,\n                            axis=-1\n                        )\n                    elif type_key == \"prompt_labels\":\n                        tokens = pad_to_length(\n                            tokens,\n                            self.max_prompt_length,\n                            pad_value=self.padding_value,\n                            axis=-1\n                        )\n                    tokens = tokens[..., :self.max_prompt_length]\n                    if tokens.shape[-1] != self.max_prompt_length:\n                        raise ValueError(\n                            f\"there was an error in padding token with `type_key` of {type_key}\"\n                            f\". it must have sequence_length of {self.max_prompt_length} but we got {tokens.shape[-1]}\"\n                            f\" From {k}{type_key}\"\n                        )\n            batch[f\"{k}{type_key}\"] = tokens\n    return batch\n</code></pre>"},{"location":"generated-trainer-orpo-utils/","title":"trainer.orpo.utils","text":""},{"location":"generated-trainer-sft-stf_trainer/","title":"trainer.sft.stf_trainer","text":""},{"location":"generated-trainer-sft-stf_trainer/#src.python.easydel.trainer.sft.stf_trainer.SFTTrainer","title":"<code>SFTTrainer</code>","text":"<p>               Bases: <code>CausalLanguageModelTrainer</code>, <code>ABC</code></p> Source code in <code>src/python/easydel/trainer/sft/stf_trainer.py</code> <pre><code>class SFTTrainer(CausalLanguageModelTrainer, ABC):\n\n    def __init__(\n            self,\n            arguments: TrainArguments,\n            tokenizer: PreTrainedTokenizerBase,\n            train_dataset: Optional[Dataset] = None,\n            eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n            dataset_text_field: Optional[str] = None,\n            packing: Optional[bool] = False,\n            formatting_func: Optional[Callable] = None,\n            num_of_sequences: Optional[int] = 1024,\n            chars_per_token: Optional[float] = 3.6,\n            dataset_num_proc: Optional[int] = None,\n            dataset_batch_size: int = 1000,\n            neftune_noise_alpha: Optional[float] = None,\n            dataset_kwargs: Optional[Dict] = None,\n            eval_packing: Optional[bool] = None,\n            checkpoint_path: Optional[str] = None,\n            remove_unused_columns=True,\n            _do_init_fns: bool = True\n    ):\n\n        if getattr(tokenizer, \"pad_token\", None) is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        self.dataset_num_proc = dataset_num_proc\n        self.dataset_batch_size = dataset_batch_size\n\n        self._trainer_supports_neftune = hasattr(arguments, \"neftune_noise_alpha\")\n\n        if neftune_noise_alpha is not None and self._trainer_supports_neftune:\n            arguments.neftune_noise_alpha = neftune_noise_alpha\n            warnings.warn(\n                \"You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override \"\n                \"the one in the `TrainArguments`.\"\n            )\n        elif not self._trainer_supports_neftune:\n            self.neftune_noise_alpha = neftune_noise_alpha\n\n        if formatting_func is None and dataset_text_field is None:\n            formatting_func = get_formatting_func_from_dataset(train_dataset, tokenizer)  # type: ignore\n\n        if not packing:\n            if dataset_text_field is None and formatting_func is None:\n                raise ValueError(\n                    \"You passed `packing=False` to the SFTTrainer, but you didn't pass a \"\n                    \"`dataset_text_field` or `formatting_func` argument.\"\n                )\n\n        if dataset_kwargs is None:\n            dataset_kwargs = {}\n        if train_dataset is not None:\n            train_dataset = self._prepare_dataset(\n                train_dataset,\n                tokenizer,\n                packing,\n                dataset_text_field,\n                arguments.max_sequence_length,\n                formatting_func,\n                num_of_sequences,\n                chars_per_token,\n                remove_unused_columns=remove_unused_columns,\n                **dataset_kwargs,\n            )\n        if eval_dataset is not None:\n            _multiple = isinstance(eval_dataset, dict)\n            _eval_datasets = eval_dataset if _multiple else {\"singleton\": eval_dataset}\n\n            eval_packing = packing if eval_packing is None else eval_packing\n\n            for _eval_dataset_name, _eval_dataset in _eval_datasets.items():\n                _eval_datasets[_eval_dataset_name] = self._prepare_dataset(\n                    _eval_dataset,\n                    tokenizer,\n                    eval_packing,\n                    dataset_text_field,\n                    arguments.max_sequence_length,\n                    formatting_func,\n                    num_of_sequences,\n                    chars_per_token,\n                    remove_unused_columns=remove_unused_columns,\n                    **dataset_kwargs,\n                )\n            if not _multiple:\n                eval_dataset = _eval_datasets[\"singleton\"]\n        if tokenizer.padding_side is not None and tokenizer.padding_side != \"right\":\n            warnings.warn(\n                \"You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead \"\n                \"to some unexpected behaviour due to overflow issues when training a model in half-precision. \"\n                \"You might consider adding `tokenizer.padding_side = 'right'` to your code.\"\n            )\n\n        super().__init__(\n            arguments=arguments,\n            dataset_train=train_dataset,\n            dataset_eval=eval_dataset,\n            finetune=True,\n            checkpoint_path=checkpoint_path,\n            _do_init_fns=_do_init_fns,\n        )\n\n    def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n\n        \"\"\"\n        The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n        :param self: Refer to the class instance itself\n        :return: A TrainerConfigureDataloaderFuncOutput object\n\n        \"\"\"\n\n        dataloader_train = tfds.as_numpy(\n            self.dataset_train.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                drop_remainder=True,\n                num_workers=self.arguments.dataloader_num_workers,\n                collate_fn=self.create_collate_function(\n                    max_sequence_length=self.arguments.max_sequence_length,\n                    truncation_mode=self.arguments.truncation_mode\n                )\n            )\n        )\n        max_training_steps = self.arguments.num_train_epochs * len(\n            dataloader_train\n        ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n        if self.dataset_eval is not None and self.arguments.do_eval:\n            dataloader_eval = tfds.as_numpy(\n                self.dataset_eval.to_tf_dataset(\n                    batch_size=self.arguments.total_batch_size,\n                    drop_remainder=True,\n                    shuffle=True,\n                    num_workers=self.arguments.dataloader_num_workers,\n                    collate_fn=self.create_collate_function(\n                        max_sequence_length=self.arguments.max_sequence_length,\n                        truncation_mode=self.arguments.truncation_mode\n                    )\n                )\n            )\n            max_evaluation_steps = len(\n                dataloader_eval\n            ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n        else:\n            dataloader_eval, max_evaluation_steps = None, 0\n\n        return TrainerConfigureDataloaderFuncOutput(\n            dataloader_train=dataloader_train,\n            max_training_steps=max_training_steps,\n            dataloader_eval=dataloader_eval,\n            max_evaluation_steps=max_evaluation_steps\n        )\n\n    def _prepare_dataset(\n            self,\n            dataset,\n            tokenizer,\n            packing,\n            dataset_text_field,\n            max_seq_length,\n            formatting_func,\n            num_of_sequences,\n            chars_per_token,\n            remove_unused_columns=True,\n            append_concat_token=True,\n            add_special_tokens=True,\n    ):\n        if dataset is None:\n            raise ValueError(\"The dataset should not be None\")\n\n        if not packing:\n            return self._prepare_non_packed_dataloader(\n                tokenizer,\n                dataset,\n                dataset_text_field,\n                max_seq_length,\n                formatting_func,\n                add_special_tokens,\n                remove_unused_columns,\n            )\n\n        else:\n            return self._prepare_packed_dataloader(\n                tokenizer,\n                dataset,\n                dataset_text_field,\n                max_seq_length,\n                num_of_sequences,\n                chars_per_token,\n                formatting_func,\n                append_concat_token,\n                add_special_tokens,\n            )\n\n    def _prepare_non_packed_dataloader(\n            self,\n            tokenizer,\n            dataset,\n            dataset_text_field,\n            max_seq_length,\n            formatting_func=None,\n            add_special_tokens=True,\n            remove_unused_columns=True,\n    ):\n        use_formatting_func = formatting_func is not None and dataset_text_field is None\n        self._dataset_sanity_checked = False\n\n        def tokenize(element):\n            inner = element[dataset_text_field] if not use_formatting_func else formatting_func(element)\n            outputs = tokenizer(\n                inner,\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=max_seq_length,\n                return_overflowing_tokens=False,\n                return_length=False,\n            )\n\n            if use_formatting_func and not self._dataset_sanity_checked:\n                if not isinstance(formatting_func(element), list):\n                    raise ValueError(\n                        \"The `formatting_func` should return a list of processed strings since it can lead\"\n                        \" to silent bugs.\"\n                    )\n                else:\n                    self._dataset_sanity_checked = True\n\n            return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n\n        signature_columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n\n        extra_columns = list(set(dataset.column_names) - set(signature_columns))\n\n        if not remove_unused_columns and len(extra_columns) &gt; 0:\n            warnings.warn(\n                \"You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with \"\n                \"the default collator and yield to errors. If you want to inspect dataset other columns \"\n                f\"(in this case {extra_columns}), you can subclass `DataCollatorForLanguageModeling` in case you \"\n                \"used the default collator and create your own data collator in order to inspect the \"\n                \"unused dataset columns.\"\n            )\n\n        tokenized_dataset = dataset.map(\n            tokenize,\n            batched=False,\n            remove_columns=dataset.column_names if remove_unused_columns else None,\n            num_proc=self.dataset_num_proc,\n            batch_size=self.dataset_batch_size,\n        )\n\n        return tokenized_dataset\n\n    @staticmethod\n    def _prepare_packed_dataloader(\n            tokenizer,\n            dataset,\n            dataset_text_field,\n            max_seq_length,\n            num_of_sequences,\n            chars_per_token,\n            formatting_func=None,\n            append_concat_token=True,\n            add_special_tokens=True,\n    ):\n        if dataset_text_field is not None or formatting_func is not None:\n            if tokenizer is None:\n                raise ValueError(\n                    \"You need to pass a tokenizer when using `dataset_text_field` with `SFTTrainer`.\"\n                )\n\n            constant_length_iterator = create_constant_length_dataset(\n                tokenizer=tokenizer,\n                dataset=dataset,\n                dataset_text_field=dataset_text_field,\n                formatting_func=formatting_func,\n                seq_length=max_seq_length,\n                infinite=False,\n                num_of_sequences=num_of_sequences,\n                chars_per_token=chars_per_token,\n                eos_token_id=tokenizer.eos_token_id,\n                append_concat_token=append_concat_token,\n                add_special_tokens=add_special_tokens,\n            )\n\n            def data_generator(inner_constant_length_iterator):\n                for d in inner_constant_length_iterator():\n                    yield d\n\n            try:\n                packed_dataset = Dataset.from_generator(\n                    data_generator, gen_kwargs={\"inner_constant_length_iterator\": constant_length_iterator}\n                )\n            except (DatasetGenerationError, SchemaInferenceError) as exc:\n                raise ValueError(\n                    \"Error occurred while packing the dataset. \"\n                    \"Make sure that your dataset has enough samples to at least yield one packed sequence.\\n\"\n                    \"External Information : {}\".format(exc)\n                ) from exc\n            return packed_dataset\n        else:\n            raise ValueError(\n                \"You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want \"\n                \"to use the `ConstantLengthDataset`.\"\n            )\n</code></pre>"},{"location":"generated-trainer-sft-stf_trainer/#src.python.easydel.trainer.sft.stf_trainer.SFTTrainer.configure_dataloader","title":"<code>configure_dataloader()</code>","text":"<p>The configure_dataloader function is used to configure the dataloader for training and evaluation.</p> <p>:param self: Refer to the class instance itself :return: A TrainerConfigureDataloaderFuncOutput object</p> Source code in <code>src/python/easydel/trainer/sft/stf_trainer.py</code> <pre><code>def configure_dataloader(self) -&gt; TrainerConfigureDataloaderFuncOutput:\n\n    \"\"\"\n    The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n    :param self: Refer to the class instance itself\n    :return: A TrainerConfigureDataloaderFuncOutput object\n\n    \"\"\"\n\n    dataloader_train = tfds.as_numpy(\n        self.dataset_train.to_tf_dataset(\n            batch_size=self.arguments.total_batch_size,\n            drop_remainder=True,\n            num_workers=self.arguments.dataloader_num_workers,\n            collate_fn=self.create_collate_function(\n                max_sequence_length=self.arguments.max_sequence_length,\n                truncation_mode=self.arguments.truncation_mode\n            )\n        )\n    )\n    max_training_steps = self.arguments.num_train_epochs * len(\n        dataloader_train\n    ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n    if self.dataset_eval is not None and self.arguments.do_eval:\n        dataloader_eval = tfds.as_numpy(\n            self.dataset_eval.to_tf_dataset(\n                batch_size=self.arguments.total_batch_size,\n                drop_remainder=True,\n                shuffle=True,\n                num_workers=self.arguments.dataloader_num_workers,\n                collate_fn=self.create_collate_function(\n                    max_sequence_length=self.arguments.max_sequence_length,\n                    truncation_mode=self.arguments.truncation_mode\n                )\n            )\n        )\n        max_evaluation_steps = len(\n            dataloader_eval\n        ) if self.arguments.max_training_steps is None else self.arguments.max_training_steps\n    else:\n        dataloader_eval, max_evaluation_steps = None, 0\n\n    return TrainerConfigureDataloaderFuncOutput(\n        dataloader_train=dataloader_train,\n        max_training_steps=max_training_steps,\n        dataloader_eval=dataloader_eval,\n        max_evaluation_steps=max_evaluation_steps\n    )\n</code></pre>"},{"location":"generated-trainer-sft-utils/","title":"trainer.sft.utils","text":""},{"location":"generated-trainer-training_configurations/","title":"trainer.training_configurations","text":""},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments","title":"<code>TrainArguments</code>","text":"<p>               Bases: <code>OrderedDict</code></p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>class TrainArguments(\n    OrderedDict\n):\n    def __init__(\n            self,\n            model_name: str,\n            num_train_epochs: int,\n            model_class: Optional[EasyDeLFlaxPretrainedModel | Type[EasyDeLFlaxPretrainedModel]] = None,\n            model_huggingface_repo_id: Optional[str] = None,\n            total_batch_size: int = 32,\n            max_training_steps: Optional[int] = None,\n            max_evaluation_steps: Optional[int] = None,\n            optimizer: AVAILABLE_OPTIMIZERS = EasyDeLOptimizers.ADAMW,\n            scheduler: AVAILABLE_SCHEDULERS = EasyDeLSchedulers.NONE,\n            learning_rate: Union[int, float] = 5e-5,\n            learning_rate_end: Optional[float] = 5e-6,\n            gradient_accumulation_steps: int = 1,\n            weight_decay: float = 0.01,\n            label_smoothing_factor: float = 0.0,\n            z_loss: float = 0.0,\n            gradient_checkpointing: AVAILABLE_GRADIENT_CHECKPOINTS = EasyDeLGradientCheckPointers.NOTHING_SAVEABLE,\n            max_sequence_length: Optional[int] = 4096,\n            sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n            is_fine_tuning: bool = True,\n            do_train: bool = True,\n            do_eval: bool = False,\n            do_test: Optional[bool] = False,\n            train_on_inputs: bool = True,\n            backend: Optional[str] = None,\n            extra_optimizer_kwargs: dict = None,\n            save_steps: Optional[int] = None,\n            save_dir: str = \"EasyDeL-Checkpoints\",\n            save_total_limit: Optional[int] = None,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            fully_sharded_data_parallel: bool = True,\n            use_wandb: bool = True,\n            custom_rule: Mapping[str, PartitionSpec] = None,\n            extra_configs: Optional[dict] = None,\n            ids_to_pop_from_dataset: Optional[list] = None,\n            remove_ckpt_after_load: bool = False,\n            configs_to_initialize_model_class: Optional[dict] = None,\n            do_last_save: bool = True,\n            model_parameters: Optional[dict] = None,\n            do_shard_fns: bool = True,\n            track_memory: Optional[bool] = None,\n            loss_re_mat: str = \"\",\n            loss_chunk: int = 1024,\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n            warmup_steps: int = 500,\n            init_input_shape: Tuple[int, int] = (1, 1),\n            step_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\"),\n            training_time: Optional[str] = None,\n            dataloader_num_workers: Optional[int] = 0,\n            dataloader_pin_memory: Optional[bool] = False,\n            jax_distributed_config: Optional[dict] = None,\n            log_all_workers: bool = False,\n            wandb_entity: Optional[str] = None,\n            save_optimizer_state: bool = False,\n            step_start_point: Optional[int] = None,\n            verbose: bool = True,\n            offload_device: jax.Device = jax.devices(\"cpu\")[0],\n            rapture_config: Optional[EasyDeLXRapTureConfig] = None,\n            merge_lora_rapture_parameters: bool = True,\n            state_apply_fn_kwarguments_to_model: Optional[dict] = None,\n            remove_unused_columns: bool = True,\n            force_batch_and_gradient_accumulation_steps_calculation: bool = False,\n            performance_mode: bool = False,\n            neftune_noise_alpha: Optional[float] = None,\n            log_grad_norms: bool = True,\n            loaded_model_config_kwargs: Optional[dict] = None,\n            **kwargs\n    ):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object, which are sometimes called fields or properties.\n        The __init__ function can accept arguments, just like a normal function.\n\n        Args:\n            self: Represent the instance of the class\n            model_name: str: Specify the model name\n            num_train_epochs: int: Set the number of epochs for training\n            model_huggingface_repo_id: Optional[str]: Load a pretrained\n                model from the huggingface model hub\n            model_class: Optional[EasyDeLFlaxPretrainedModel]: Pass a\n                model class to the trainer\n            total_batch_size: int: Set the batch size of the model\n            max_training_steps: Optional[int]: Set the maximum total\n                number of training steps across all epochs\n            max_evaluation_steps: Optional[int]: Set the maximum number\n                of steps to evaluate for\n            optimizer: AVAILABLE_OPTIMIZERS: Specify the optimizer used\n                to train the model\n            scheduler: AVAILABLE_SCHEDULERS: Set the learning rate\n                scheduler\n            learning_rate: Union[int, float] : Set the learning rate for\n                the optimizer\n            learning_rate_end: Optional[float]: Set the learning rate at\n                the end of training\n            gradient_accumulation_steps: int: Accumulate gradients over\n                multiple batches\n            weight_decay: float: Specify the weight decay to be used by\n                the optimizer\n            label_smoothing_factor: float: Set the label smoothing\n                factor to be used by the loss function\n            z_loss: float: Set the z loss factor to be used by the loss\n                function\n            gradient_checkpointing: AVAILABLE_GRADIENT_CHECKPOINTS:\n                Determine how to use gradient checkpointing\n            max_sequence_length: Optional[int]: Set the maximum length\n                of the input sequence\n            sharding_array: Union[tuple,int]: Specify the mesh of\n                devices to use for training\n            is_fine_tuning: bool: Tell the model whether or not to\n                initialize the weights of\n            do_train: bool: Indicate whether to train the model or not\n            do_eval: bool: Determine whether to run evaluation on the\n                validation set after training\n            do_test: Optional[bool]: Determine if the model should be\n                tested\n            train_on_inputs: bool: Use input_ids instead of labels,\n                overrides ignored (-100) tokens in the labels\n            backend: Optional[str]: Specify the backend of jax\n            extra_optimizer_kwargs: dict: Pass extra arguments to the\n                optimizer\n            save_steps: Optional[int]: Save the model after every n\n                steps\n            save_dir: str: Define the directory where the checkpoints\n                will be saved\n            save_total_limit: int: Set the maximum number of checkpoints\n                to keep, older checkpoints will be deleted\n            dtype: jnp.dtype: Set the dtype of the model parameters\n            param_dtype: jnp.dtype: Specify the data type of the model\n                parameters\n            fully_sharded_data_parallel: bool: Determine if the model\n                should be fully fsdp or not\n            use_wandb: bool: Enable or disable the wandb logging\n            custom_rule: Mapping[str, PartitionSpec]: Specify the\n                partitioning rules of the model\n            extra_configs: Optional[dict]: Pass extra configurations to\n                the model class\n            ids_to_pop_from_dataset: Optional[list]: Remove some of the\n                ids from the dataset\n            remove_ckpt_after_load: bool: Remove the checkpoint after\n                loading it\n            configs_to_initialize_model_class: Optional[dict]: Pass\n                extra configurations to the model class\n            do_last_save: bool: Save the model after training is\n                complete\n            model_parameters: Optional[dict]: Pass the model parameters\n                to the model class\n            do_shard_fns: bool: Shard the model functions across devices\n            track_memory: bool: Track the memory usage of the model\n            loss_re_mat: str: Specify the regular expression to match\n                the loss function name\n            loss_chunk: int: Chunk the loss to avoid memory overflow\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"]:\n                Determine if the input is left padded or not and which\n                side of the array should remain in case of using maximum\n                padding.\n            warmup_steps: int: Specify the number of steps to warm up\n                the learning rate\n            init_input_shape: Tuple[int, int]: Initialize the model with\n                a shape that is not (batch_size, length)\n            step_partition_spec: PartitionSpec: Partition the model for\n                training\n            training_time: Optional[str]: Set a time limit for the\n                training process\n            dataloader_num_workers: Optional[int]: Set the number of\n                workers used by pytorch's\n            dataloader_pin_memory: Optional[bool]: Pin the memory of the\n                dataloader\n            jax_distributed_config: Optional[dict]: Configure the jax\n                distributed backend\n            log_all_workers: bool: Log all workers in wandb,\n            wandb_entity: Optional[str]: Specify the entity to use when\n                logging to weights &amp;amp; biases\n            save_optimizer_state: bool: when ever to save optimizer\n                state and other args in checkpoint\n            step_start_point: Optional[int]: start training from given\n                step for example instead of starting training from step\n                0 it will start from 20000 and leave the data behind\n            verbose: bool: when ever to turn verbose mode of or on\n            offload_device: jax.Device: device to be used to offload\n                parameters on\n            rapture_config: Optional[EasyDeLXRaptureConfig]: LoRA Config\n                for models\n            merge_lora_rapture_parameters: bool: whenever to merge lora\n                parameters with original parameters before saving\n            state_apply_fn_kwarguments_to_model: Optional[dict]:\n                state_apply_fn_kwarguments_to_model is a dictionary that\n                be used to apply the parameters and extra things that\n                you want to deliver to model.\n            remove_unused_columns: bool: when ever to remove the unused\n                data columns from dataset\n            force_batch_and_gradient_accumulation_steps_calculation:\n                bool: whether to force batch and gradient to be applied\n                as total batch_size (e.g total_batch_size =\n                total_batch_size * gradient_accumulation_steps be\n                applied)\n            performance_mode: bool: whether to optimize the whole\n                training process this will cut off some logging options\n                and optimize training process.\n            neftune_noise_alpha: Optional[float]: If not `None`, this\n                will activate NEFTune noise embeddings. This has been\n                proven to drastically improve model performances for\n                instruction fine-tuning.\n            loaded_model_config_kwargs: Optional[dict]: config key\n                arguments to be passed to the model while being loaded\n            **kwargs: Pass keyword, variable-length argument list\n        from checkpoint\n        \"\"\"\n        super().__init__()\n\n        if ids_to_pop_from_dataset is None:\n            ids_to_pop_from_dataset = []\n        if extra_optimizer_kwargs is None:\n            extra_optimizer_kwargs = {}\n\n        if model_class is None and model_huggingface_repo_id is None:\n            print(\n                termcolor.colored(\n                    \"Warning : \", color=\"red\", force_color=True\n                ) + termcolor.colored(\n                    \"You should at least pass model_class or model_huggingface_repo_id if you want to use \"\n                    \"CasualLanguageModel Trainer But in case that you want to use \"\n                    \"DPOTrainer or ORPOTrainer you can ignore this warning\", color=\"white\",\n                    force_color=True\n                )\n            )\n        assert backend in AVAILABLE_BACKENDS, (\n            f\"{backend} is not recognized, \"\n            f\"available backends are {AVAILABLE_BACKENDS}\"\n        )\n\n        if track_memory is None:\n            # https://github.com/erfanzar/EasyDeL/pull/100/commits/523ce7b1515d7896d456759d0bcd0bd02369bd10\n            print(\n                termcolor.colored(\"Information : \", color=\"red\", force_color=True),\n                termcolor.colored(\n                    \"track_memory is set to False by default inorder make make training faster. \"\n                    \"you can turn it on with just passing `track_memory=True` in TrainArguments\",\n                    color=\"white\", force_color=True\n                )\n            )\n            track_memory = False\n\n        available_backends = len(jax.devices(backend))\n        if force_batch_and_gradient_accumulation_steps_calculation:\n            total_batch_size *= gradient_accumulation_steps  # Changed and will be handled inside FJFormer\n        array_devices = jnp.ones((available_backends, 1)).reshape(sharding_array)\n        JaxDistributedConfig.initialize(jax_distributed_config)\n        self.force_batch_and_gradient_accumulation_steps_calculation = (\n            force_batch_and_gradient_accumulation_steps_calculation\n        )\n        self.available_backends = available_backends\n        self.array_devices_shape = array_devices.shape\n        self.model_huggingface_repo_id = model_huggingface_repo_id\n        self.num_train_epochs = num_train_epochs\n        self.wandb_entity = wandb_entity\n        self.total_batch_size = total_batch_size\n        self.max_training_steps = max_training_steps\n        self.max_evaluation_steps = max_evaluation_steps\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.extra_optimizer_kwargs = extra_optimizer_kwargs\n        self.learning_rate = learning_rate\n        self.learning_rate_end = learning_rate_end\n        self.weight_decay = weight_decay\n        self.label_smoothing_factor = label_smoothing_factor\n        self.z_loss = z_loss\n        self.model_name = model_name\n        self.gradient_checkpointing = gradient_checkpointing\n        self.max_sequence_length = max_sequence_length\n        self.sharding_array = sharding_array\n        self.is_fine_tuning = is_fine_tuning\n        self.do_train = do_train\n        self.do_eval = do_eval\n        self.do_test = do_test\n        self.train_on_inputs = train_on_inputs\n        self.save_steps = save_steps\n        self.save_dir = save_dir\n        self.save_total_limit = save_total_limit\n        self.dtype = dtype\n        self.warmup_steps = warmup_steps\n        self.param_dtype = param_dtype\n        self.fully_sharded_data_parallel = fully_sharded_data_parallel\n        self.use_wandb = use_wandb\n        self.custom_rule = custom_rule\n        self.extra_configs = extra_configs\n        self.ids_to_pop_from_dataset = ids_to_pop_from_dataset\n        self.remove_ckpt_after_load = remove_ckpt_after_load\n        self.model_class = model_class\n        self.configs_to_initialize_model_class = configs_to_initialize_model_class\n        self.do_last_save = do_last_save\n        self.model_parameters = model_parameters\n        self.do_shard_fns = do_shard_fns\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.track_memory = track_memory\n        self.loss_chunk = loss_chunk\n        self.loss_re_mat = loss_re_mat\n        self.init_input_shape = init_input_shape\n        self.truncation_mode = truncation_mode\n        self.step_partition_spec = step_partition_spec\n        self.jax_distributed_config = jax_distributed_config\n        self.log_all_workers = log_all_workers\n        self.dataloader_num_workers = dataloader_num_workers\n        self.dataloader_pin_memory = dataloader_pin_memory\n        self.save_optimizer_state = save_optimizer_state\n        self.step_start_point = step_start_point if step_start_point is not None else 0\n        self.verbose = verbose\n        self.offload_device = offload_device\n        self.performance_mode = performance_mode\n        self.neftune_noise_alpha = neftune_noise_alpha\n        self.loaded_model_config_kwargs = loaded_model_config_kwargs\n        if use_wandb and performance_mode:\n            self.use_wandb = False\n        self.optimizer_kwargs = dict(\n            learning_rate=self.learning_rate,\n            learning_rate_end=self.learning_rate_end,\n            optimizer=self.optimizer,\n            scheduler=self.scheduler,\n            extra_optimizer_kwargs=self.extra_optimizer_kwargs,\n            warmup_steps=self.warmup_steps,\n            gradient_accumulation_steps=self.gradient_accumulation_steps,\n            weight_decay=self.weight_decay,\n            steps=self.max_training_steps,\n        )\n        self.training_time = self._time_to_seconds(training_time) if training_time is not None else None\n        self.merge_lora_rapture_parameters = merge_lora_rapture_parameters\n        self.rapture = None\n        self.rapture_config = None\n        self.remove_unused_columns = remove_unused_columns\n        self._stop_capturing_memory = False\n        self._captured_memory = {}\n        self.log_grad_norms = log_grad_norms\n        if rapture_config is not None and log_grad_norms:\n            warnings.warn(\n                \"setting `log_grad_norms` to off since using log grad norms while using LoRA is not Supported.\"\n            )\n            self.log_grad_norms = False\n        self.state_apply_fn_kwarguments_to_model = (\n            state_apply_fn_kwarguments_to_model\n        ) if state_apply_fn_kwarguments_to_model is not None else {}\n        if rapture_config is not None:\n            print(\n                termcolor.colored(\"Warning : \", color=\"red\", force_color=True),\n                termcolor.colored(\n                    \"You are using LoRA (Low-Rank Adaptation of Large Language Models) and this feature is\"\n                    \"still in Beta mode so it might act unexpected\", color=\"red\", force_color=True\n                )\n            )\n            self.rapture_config = rapture_config\n            self.rapture = XRapTure(config=rapture_config)\n        self.__dict__.update(**kwargs)\n\n    @staticmethod\n    def _time_to_seconds(time_str):\n        pattern = r\"(\\d+)\\s*(h|min)\"\n        match = re.match(pattern, time_str.lower())\n\n        if match:\n            value = int(match.group(1))\n            unit = match.group(2).lower()\n\n            if unit == \"h\":\n                return value * 3600  # Convert hours to seconds\n            elif unit == \"min\":\n                return value * 60  # Convert minutes to seconds\n        else:\n            raise SyntaxError(\n                \"Invalid input format it should be like 50Min for M and 23H for hours\")\n\n    def __call__(self):\n        return {k: v for k, v in self.__dict__.items()}\n\n    def get_meter_dict(self):\n        \"\"\"The get_meter_dict function is used to return a dictionary of the hyperparameters.\n        The function iterates through all the attributes in the class and returns a dictionary with\n        the key as &amp;quot;hyperparameters/{k}&amp;quot; and value as v for each attribute k,v in self.__dict__ if it is an\n         instance of int, float, str, bool or torch.Tensor.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A dictionary of hyperparameters\n        \"\"\"\n        return {\n            f\"hyperparameters/{k}\": v for k, v in self.__dict__.items() if\n            isinstance(v, (int, float, str, bool))\n        }\n\n    def get_wandb_init(self) -&gt; Optional[Union[\"Run\" , \"RunDisabled\"]] :  # type:ignore\n        \"\"\"The get_wandb_init function is a helper function that returns the wandb.init() call with\n        the project name, config object, and tags set to appropriate values for this model.\n\n        Args:\n            self: Pass the class instance to the function\n\n        Returns:\n            A wandb or None\n        \"\"\"\n        if wandb is None:\n            return None\n        else:\n            return wandb.init(  # noqa\n                project=f\"EasyDeL-{self.model_name}\",\n                config=self(),\n                tags=[\n                    \"EasyDeL\",\n                    \"FJFormer\",\n                    \"OST-OpenSourceTransformers\",\n                    \"Jax/Flax\"\n                ],\n                entity=self.wandb_entity\n\n            ) if self.log_all_workers or (jax.process_index() == 0) else None\n\n    def __str__(self):\n        string = f\"{self.__class__.__name__}(\\n\"\n        for k, v in self.__call__().items():\n            if isinstance(v, Callable):\n                def string_func(it_self):\n                    string_ = f\"{it_self.__class__.__name__}(\\n\"\n                    for k_, v_ in it_self.__dict__.items():\n                        string_ += f\"\\t\\t{k_} : {v_}\\n\"\n                    string_ += \"\\t)\"\n                    return string_\n\n                v.__str__ = string_func\n                v = v.__str__(v)\n            string += f\"\\t{k} : {v}\\n\"\n        string += \")\"\n        return string\n\n    def get_path(self):\n        \"\"\"The get_path function returns a pathlib.Path object, which is a class that\n        represents file paths and provides methods for interacting with the files at\n        those paths. The get_path function takes no arguments and returns an instance of\n        the Path class initialized with two arguments: self.save_dir (a string) and\n        self.model_name (also a string). The save directory is the directory where we'll\n        store our model checkpoints, while the model name will be used to create unique\n        filenames for each checkpoint.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A pathlib\n        \"\"\"\n        return pathlib.Path(\n            self.save_dir, self.model_name\n        )\n\n    def ckpt_path_exists(self):\n        \"\"\"The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A path\n        \"\"\"\n        path = self.get_path()\n        if not path.exists():\n            path.mkdir(parents=True)\n\n    def get_mesh(self):\n        \"\"\"The get_mesh function is used to create a mesh object that can be used\n        to define the geometry of the device. The mesh object contains two arrays:\n        a list of vertices and a list of faces. Each face is defined by three indices,\n        which correspond to three vertices in the vertex array. The get_mesh function\n        is called when creating an instance of DeviceGeometry, which is then passed\n        into an instance of DeviceSimulation.\n\n        Args:\n            self: Refer to the object itself\n\n        Returns:\n            A mesh object with the device array shape and the mesh names\n        \"\"\"\n        return Mesh(\n            create_device_mesh(\n                self.array_devices_shape\n            ),\n            self.get_mesh_names()\n        )\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"sp\"\n\n    def get_optimizer_and_scheduler(\n            self,\n            steps: int | None = None\n    ):\n        self.optimizer_kwargs[\"steps\"] = steps or self.optimizer_kwargs[\"steps\"]\n        return get_optimizer_and_scheduler(\n            **self.optimizer_kwargs\n        )\n\n    def get_streaming_checkpointer(self):\n        \"\"\"The get_streaming_checkpointer function is used to save the model's weights.\n        The streaming checkpointer saves the model's weights in a file called &amp;quot;checkpoint&amp;quot; and then\n        saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001,\n        checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A CheckpointManager object\n        \"\"\"\n        return CheckpointManager(\n            os.path.join(self.save_dir, self.model_name),\n            save_optimizer_state=self.save_optimizer_state,\n            verbose=self.verbose\n        )\n\n    def get_board(self):\n        \"\"\"The get_board function is a helper function that returns a TensorBoard object.\n        The TensorBoard object is used to log the training and validation loss, as well as\n        the accuracy of the model during training. The get_board function takes no arguments,\n        and returns an instance of torch.utils.tensorboard SummaryWriter class.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            A summary-writer object\n        \"\"\"\n        return flax.metrics.tensorboard.SummaryWriter(log_dir=str(self.get_path()))\n\n    @property\n    def stop_capturing_memory(self):\n        return self._stop_capturing_memory\n\n    @property\n    def captured_memory(self):\n        return self._captured_memory\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.__init__","title":"<code>__init__(model_name, num_train_epochs, model_class=None, model_huggingface_repo_id=None, total_batch_size=32, max_training_steps=None, max_evaluation_steps=None, optimizer=EasyDeLOptimizers.ADAMW, scheduler=EasyDeLSchedulers.NONE, learning_rate=5e-05, learning_rate_end=5e-06, gradient_accumulation_steps=1, weight_decay=0.01, label_smoothing_factor=0.0, z_loss=0.0, gradient_checkpointing=EasyDeLGradientCheckPointers.NOTHING_SAVEABLE, max_sequence_length=4096, sharding_array=(1, -1, 1, 1), is_fine_tuning=True, do_train=True, do_eval=False, do_test=False, train_on_inputs=True, backend=None, extra_optimizer_kwargs=None, save_steps=None, save_dir='EasyDeL-Checkpoints', save_total_limit=None, dtype=jnp.bfloat16, param_dtype=jnp.bfloat16, fully_sharded_data_parallel=True, use_wandb=True, custom_rule=None, extra_configs=None, ids_to_pop_from_dataset=None, remove_ckpt_after_load=False, configs_to_initialize_model_class=None, do_last_save=True, model_parameters=None, do_shard_fns=True, track_memory=None, loss_re_mat='', loss_chunk=1024, truncation_mode='keep_end', warmup_steps=500, init_input_shape=(1, 1), step_partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp'), training_time=None, dataloader_num_workers=0, dataloader_pin_memory=False, jax_distributed_config=None, log_all_workers=False, wandb_entity=None, save_optimizer_state=False, step_start_point=None, verbose=True, offload_device=jax.devices('cpu')[0], rapture_config=None, merge_lora_rapture_parameters=True, state_apply_fn_kwarguments_to_model=None, remove_unused_columns=True, force_batch_and_gradient_accumulation_steps_calculation=False, performance_mode=False, neftune_noise_alpha=None, log_grad_norms=True, loaded_model_config_kwargs=None, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object, which are sometimes called fields or properties. The init function can accept arguments, just like a normal function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>model_name</code> <code>str</code> <p>str: Specify the model name</p> required <code>num_train_epochs</code> <code>int</code> <p>int: Set the number of epochs for training</p> required <code>model_huggingface_repo_id</code> <code>Optional[str]</code> <p>Optional[str]: Load a pretrained model from the huggingface model hub</p> <code>None</code> <code>model_class</code> <code>Optional[EasyDeLFlaxPretrainedModel | Type[EasyDeLFlaxPretrainedModel]]</code> <p>Optional[EasyDeLFlaxPretrainedModel]: Pass a model class to the trainer</p> <code>None</code> <code>total_batch_size</code> <code>int</code> <p>int: Set the batch size of the model</p> <code>32</code> <code>max_training_steps</code> <code>Optional[int]</code> <p>Optional[int]: Set the maximum total number of training steps across all epochs</p> <code>None</code> <code>max_evaluation_steps</code> <code>Optional[int]</code> <p>Optional[int]: Set the maximum number of steps to evaluate for</p> <code>None</code> <code>optimizer</code> <code>AVAILABLE_OPTIMIZERS</code> <p>AVAILABLE_OPTIMIZERS: Specify the optimizer used to train the model</p> <code>ADAMW</code> <code>scheduler</code> <code>AVAILABLE_SCHEDULERS</code> <p>AVAILABLE_SCHEDULERS: Set the learning rate scheduler</p> <code>NONE</code> <code>learning_rate</code> <code>Union[int, float]</code> <p>Union[int, float] : Set the learning rate for the optimizer</p> <code>5e-05</code> <code>learning_rate_end</code> <code>Optional[float]</code> <p>Optional[float]: Set the learning rate at the end of training</p> <code>5e-06</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate gradients over multiple batches</p> <code>1</code> <code>weight_decay</code> <code>float</code> <p>float: Specify the weight decay to be used by the optimizer</p> <code>0.01</code> <code>label_smoothing_factor</code> <code>float</code> <p>float: Set the label smoothing factor to be used by the loss function</p> <code>0.0</code> <code>z_loss</code> <code>float</code> <p>float: Set the z loss factor to be used by the loss function</p> <code>0.0</code> <code>gradient_checkpointing</code> <code>AVAILABLE_GRADIENT_CHECKPOINTS</code> <p>AVAILABLE_GRADIENT_CHECKPOINTS: Determine how to use gradient checkpointing</p> <code>NOTHING_SAVEABLE</code> <code>max_sequence_length</code> <code>Optional[int]</code> <p>Optional[int]: Set the maximum length of the input sequence</p> <code>4096</code> <code>sharding_array</code> <code>Union[tuple, int]</code> <p>Union[tuple,int]: Specify the mesh of devices to use for training</p> <code>(1, -1, 1, 1)</code> <code>is_fine_tuning</code> <code>bool</code> <p>bool: Tell the model whether or not to initialize the weights of</p> <code>True</code> <code>do_train</code> <code>bool</code> <p>bool: Indicate whether to train the model or not</p> <code>True</code> <code>do_eval</code> <code>bool</code> <p>bool: Determine whether to run evaluation on the validation set after training</p> <code>False</code> <code>do_test</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine if the model should be tested</p> <code>False</code> <code>train_on_inputs</code> <code>bool</code> <p>bool: Use input_ids instead of labels, overrides ignored (-100) tokens in the labels</p> <code>True</code> <code>backend</code> <code>Optional[str]</code> <p>Optional[str]: Specify the backend of jax</p> <code>None</code> <code>extra_optimizer_kwargs</code> <code>dict</code> <p>dict: Pass extra arguments to the optimizer</p> <code>None</code> <code>save_steps</code> <code>Optional[int]</code> <p>Optional[int]: Save the model after every n steps</p> <code>None</code> <code>save_dir</code> <code>str</code> <p>str: Define the directory where the checkpoints will be saved</p> <code>'EasyDeL-Checkpoints'</code> <code>save_total_limit</code> <code>Optional[int]</code> <p>int: Set the maximum number of checkpoints to keep, older checkpoints will be deleted</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Set the dtype of the model parameters</p> <code>bfloat16</code> <code>param_dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the model parameters</p> <code>bfloat16</code> <code>fully_sharded_data_parallel</code> <code>bool</code> <p>bool: Determine if the model should be fully fsdp or not</p> <code>True</code> <code>use_wandb</code> <code>bool</code> <p>bool: Enable or disable the wandb logging</p> <code>True</code> <code>custom_rule</code> <code>Mapping[str, PartitionSpec]</code> <p>Mapping[str, PartitionSpec]: Specify the partitioning rules of the model</p> <code>None</code> <code>extra_configs</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass extra configurations to the model class</p> <code>None</code> <code>ids_to_pop_from_dataset</code> <code>Optional[list]</code> <p>Optional[list]: Remove some of the ids from the dataset</p> <code>None</code> <code>remove_ckpt_after_load</code> <code>bool</code> <p>bool: Remove the checkpoint after loading it</p> <code>False</code> <code>configs_to_initialize_model_class</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass extra configurations to the model class</p> <code>None</code> <code>do_last_save</code> <code>bool</code> <p>bool: Save the model after training is complete</p> <code>True</code> <code>model_parameters</code> <code>Optional[dict]</code> <p>Optional[dict]: Pass the model parameters to the model class</p> <code>None</code> <code>do_shard_fns</code> <code>bool</code> <p>bool: Shard the model functions across devices</p> <code>True</code> <code>track_memory</code> <code>Optional[bool]</code> <p>bool: Track the memory usage of the model</p> <code>None</code> <code>loss_re_mat</code> <code>str</code> <p>str: Specify the regular expression to match the loss function name</p> <code>''</code> <code>loss_chunk</code> <code>int</code> <p>int: Chunk the loss to avoid memory overflow</p> <code>1024</code> <code>truncation_mode</code> <code>Literal['keep_end', 'keep_start']</code> <p>typing.Literal[\"keep_end\", \"keep_start\"]: Determine if the input is left padded or not and which side of the array should remain in case of using maximum padding.</p> <code>'keep_end'</code> <code>warmup_steps</code> <code>int</code> <p>int: Specify the number of steps to warm up the learning rate</p> <code>500</code> <code>init_input_shape</code> <code>Tuple[int, int]</code> <p>Tuple[int, int]: Initialize the model with a shape that is not (batch_size, length)</p> <code>(1, 1)</code> <code>step_partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec: Partition the model for training</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp')</code> <code>training_time</code> <code>Optional[str]</code> <p>Optional[str]: Set a time limit for the training process</p> <code>None</code> <code>dataloader_num_workers</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of workers used by pytorch's</p> <code>0</code> <code>dataloader_pin_memory</code> <code>Optional[bool]</code> <p>Optional[bool]: Pin the memory of the dataloader</p> <code>False</code> <code>jax_distributed_config</code> <code>Optional[dict]</code> <p>Optional[dict]: Configure the jax distributed backend</p> <code>None</code> <code>log_all_workers</code> <code>bool</code> <p>bool: Log all workers in wandb,</p> <code>False</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>Optional[str]: Specify the entity to use when logging to weights &amp; biases</p> <code>None</code> <code>save_optimizer_state</code> <code>bool</code> <p>bool: when ever to save optimizer state and other args in checkpoint</p> <code>False</code> <code>step_start_point</code> <code>Optional[int]</code> <p>Optional[int]: start training from given step for example instead of starting training from step 0 it will start from 20000 and leave the data behind</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>bool: when ever to turn verbose mode of or on</p> <code>True</code> <code>offload_device</code> <code>Device</code> <p>jax.Device: device to be used to offload parameters on</p> <code>devices('cpu')[0]</code> <code>rapture_config</code> <code>Optional[EasyDeLXRapTureConfig]</code> <p>Optional[EasyDeLXRaptureConfig]: LoRA Config for models</p> <code>None</code> <code>merge_lora_rapture_parameters</code> <code>bool</code> <p>bool: whenever to merge lora parameters with original parameters before saving</p> <code>True</code> <code>state_apply_fn_kwarguments_to_model</code> <code>Optional[dict]</code> <p>Optional[dict]: state_apply_fn_kwarguments_to_model is a dictionary that be used to apply the parameters and extra things that you want to deliver to model.</p> <code>None</code> <code>remove_unused_columns</code> <code>bool</code> <p>bool: when ever to remove the unused data columns from dataset</p> <code>True</code> <code>force_batch_and_gradient_accumulation_steps_calculation</code> <code>bool</code> <p>bool: whether to force batch and gradient to be applied as total batch_size (e.g total_batch_size = total_batch_size * gradient_accumulation_steps be applied)</p> <code>False</code> <code>performance_mode</code> <code>bool</code> <p>bool: whether to optimize the whole training process this will cut off some logging options and optimize training process.</p> <code>False</code> <code>neftune_noise_alpha</code> <code>Optional[float]</code> <p>Optional[float]: If not <code>None</code>, this will activate NEFTune noise embeddings. This has been proven to drastically improve model performances for instruction fine-tuning.</p> <code>None</code> <code>loaded_model_config_kwargs</code> <code>Optional[dict]</code> <p>Optional[dict]: config key arguments to be passed to the model while being loaded</p> <code>None</code> <code>**kwargs</code> <p>Pass keyword, variable-length argument list</p> <code>{}</code> <p>from checkpoint</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def __init__(\n        self,\n        model_name: str,\n        num_train_epochs: int,\n        model_class: Optional[EasyDeLFlaxPretrainedModel | Type[EasyDeLFlaxPretrainedModel]] = None,\n        model_huggingface_repo_id: Optional[str] = None,\n        total_batch_size: int = 32,\n        max_training_steps: Optional[int] = None,\n        max_evaluation_steps: Optional[int] = None,\n        optimizer: AVAILABLE_OPTIMIZERS = EasyDeLOptimizers.ADAMW,\n        scheduler: AVAILABLE_SCHEDULERS = EasyDeLSchedulers.NONE,\n        learning_rate: Union[int, float] = 5e-5,\n        learning_rate_end: Optional[float] = 5e-6,\n        gradient_accumulation_steps: int = 1,\n        weight_decay: float = 0.01,\n        label_smoothing_factor: float = 0.0,\n        z_loss: float = 0.0,\n        gradient_checkpointing: AVAILABLE_GRADIENT_CHECKPOINTS = EasyDeLGradientCheckPointers.NOTHING_SAVEABLE,\n        max_sequence_length: Optional[int] = 4096,\n        sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n        is_fine_tuning: bool = True,\n        do_train: bool = True,\n        do_eval: bool = False,\n        do_test: Optional[bool] = False,\n        train_on_inputs: bool = True,\n        backend: Optional[str] = None,\n        extra_optimizer_kwargs: dict = None,\n        save_steps: Optional[int] = None,\n        save_dir: str = \"EasyDeL-Checkpoints\",\n        save_total_limit: Optional[int] = None,\n        dtype: jnp.dtype = jnp.bfloat16,\n        param_dtype: jnp.dtype = jnp.bfloat16,\n        fully_sharded_data_parallel: bool = True,\n        use_wandb: bool = True,\n        custom_rule: Mapping[str, PartitionSpec] = None,\n        extra_configs: Optional[dict] = None,\n        ids_to_pop_from_dataset: Optional[list] = None,\n        remove_ckpt_after_load: bool = False,\n        configs_to_initialize_model_class: Optional[dict] = None,\n        do_last_save: bool = True,\n        model_parameters: Optional[dict] = None,\n        do_shard_fns: bool = True,\n        track_memory: Optional[bool] = None,\n        loss_re_mat: str = \"\",\n        loss_chunk: int = 1024,\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n        warmup_steps: int = 500,\n        init_input_shape: Tuple[int, int] = (1, 1),\n        step_partition_spec: PartitionSpec = PartitionSpec((\"dp\", \"fsdp\"), \"sp\"),\n        training_time: Optional[str] = None,\n        dataloader_num_workers: Optional[int] = 0,\n        dataloader_pin_memory: Optional[bool] = False,\n        jax_distributed_config: Optional[dict] = None,\n        log_all_workers: bool = False,\n        wandb_entity: Optional[str] = None,\n        save_optimizer_state: bool = False,\n        step_start_point: Optional[int] = None,\n        verbose: bool = True,\n        offload_device: jax.Device = jax.devices(\"cpu\")[0],\n        rapture_config: Optional[EasyDeLXRapTureConfig] = None,\n        merge_lora_rapture_parameters: bool = True,\n        state_apply_fn_kwarguments_to_model: Optional[dict] = None,\n        remove_unused_columns: bool = True,\n        force_batch_and_gradient_accumulation_steps_calculation: bool = False,\n        performance_mode: bool = False,\n        neftune_noise_alpha: Optional[float] = None,\n        log_grad_norms: bool = True,\n        loaded_model_config_kwargs: Optional[dict] = None,\n        **kwargs\n):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object, which are sometimes called fields or properties.\n    The __init__ function can accept arguments, just like a normal function.\n\n    Args:\n        self: Represent the instance of the class\n        model_name: str: Specify the model name\n        num_train_epochs: int: Set the number of epochs for training\n        model_huggingface_repo_id: Optional[str]: Load a pretrained\n            model from the huggingface model hub\n        model_class: Optional[EasyDeLFlaxPretrainedModel]: Pass a\n            model class to the trainer\n        total_batch_size: int: Set the batch size of the model\n        max_training_steps: Optional[int]: Set the maximum total\n            number of training steps across all epochs\n        max_evaluation_steps: Optional[int]: Set the maximum number\n            of steps to evaluate for\n        optimizer: AVAILABLE_OPTIMIZERS: Specify the optimizer used\n            to train the model\n        scheduler: AVAILABLE_SCHEDULERS: Set the learning rate\n            scheduler\n        learning_rate: Union[int, float] : Set the learning rate for\n            the optimizer\n        learning_rate_end: Optional[float]: Set the learning rate at\n            the end of training\n        gradient_accumulation_steps: int: Accumulate gradients over\n            multiple batches\n        weight_decay: float: Specify the weight decay to be used by\n            the optimizer\n        label_smoothing_factor: float: Set the label smoothing\n            factor to be used by the loss function\n        z_loss: float: Set the z loss factor to be used by the loss\n            function\n        gradient_checkpointing: AVAILABLE_GRADIENT_CHECKPOINTS:\n            Determine how to use gradient checkpointing\n        max_sequence_length: Optional[int]: Set the maximum length\n            of the input sequence\n        sharding_array: Union[tuple,int]: Specify the mesh of\n            devices to use for training\n        is_fine_tuning: bool: Tell the model whether or not to\n            initialize the weights of\n        do_train: bool: Indicate whether to train the model or not\n        do_eval: bool: Determine whether to run evaluation on the\n            validation set after training\n        do_test: Optional[bool]: Determine if the model should be\n            tested\n        train_on_inputs: bool: Use input_ids instead of labels,\n            overrides ignored (-100) tokens in the labels\n        backend: Optional[str]: Specify the backend of jax\n        extra_optimizer_kwargs: dict: Pass extra arguments to the\n            optimizer\n        save_steps: Optional[int]: Save the model after every n\n            steps\n        save_dir: str: Define the directory where the checkpoints\n            will be saved\n        save_total_limit: int: Set the maximum number of checkpoints\n            to keep, older checkpoints will be deleted\n        dtype: jnp.dtype: Set the dtype of the model parameters\n        param_dtype: jnp.dtype: Specify the data type of the model\n            parameters\n        fully_sharded_data_parallel: bool: Determine if the model\n            should be fully fsdp or not\n        use_wandb: bool: Enable or disable the wandb logging\n        custom_rule: Mapping[str, PartitionSpec]: Specify the\n            partitioning rules of the model\n        extra_configs: Optional[dict]: Pass extra configurations to\n            the model class\n        ids_to_pop_from_dataset: Optional[list]: Remove some of the\n            ids from the dataset\n        remove_ckpt_after_load: bool: Remove the checkpoint after\n            loading it\n        configs_to_initialize_model_class: Optional[dict]: Pass\n            extra configurations to the model class\n        do_last_save: bool: Save the model after training is\n            complete\n        model_parameters: Optional[dict]: Pass the model parameters\n            to the model class\n        do_shard_fns: bool: Shard the model functions across devices\n        track_memory: bool: Track the memory usage of the model\n        loss_re_mat: str: Specify the regular expression to match\n            the loss function name\n        loss_chunk: int: Chunk the loss to avoid memory overflow\n        truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"]:\n            Determine if the input is left padded or not and which\n            side of the array should remain in case of using maximum\n            padding.\n        warmup_steps: int: Specify the number of steps to warm up\n            the learning rate\n        init_input_shape: Tuple[int, int]: Initialize the model with\n            a shape that is not (batch_size, length)\n        step_partition_spec: PartitionSpec: Partition the model for\n            training\n        training_time: Optional[str]: Set a time limit for the\n            training process\n        dataloader_num_workers: Optional[int]: Set the number of\n            workers used by pytorch's\n        dataloader_pin_memory: Optional[bool]: Pin the memory of the\n            dataloader\n        jax_distributed_config: Optional[dict]: Configure the jax\n            distributed backend\n        log_all_workers: bool: Log all workers in wandb,\n        wandb_entity: Optional[str]: Specify the entity to use when\n            logging to weights &amp;amp; biases\n        save_optimizer_state: bool: when ever to save optimizer\n            state and other args in checkpoint\n        step_start_point: Optional[int]: start training from given\n            step for example instead of starting training from step\n            0 it will start from 20000 and leave the data behind\n        verbose: bool: when ever to turn verbose mode of or on\n        offload_device: jax.Device: device to be used to offload\n            parameters on\n        rapture_config: Optional[EasyDeLXRaptureConfig]: LoRA Config\n            for models\n        merge_lora_rapture_parameters: bool: whenever to merge lora\n            parameters with original parameters before saving\n        state_apply_fn_kwarguments_to_model: Optional[dict]:\n            state_apply_fn_kwarguments_to_model is a dictionary that\n            be used to apply the parameters and extra things that\n            you want to deliver to model.\n        remove_unused_columns: bool: when ever to remove the unused\n            data columns from dataset\n        force_batch_and_gradient_accumulation_steps_calculation:\n            bool: whether to force batch and gradient to be applied\n            as total batch_size (e.g total_batch_size =\n            total_batch_size * gradient_accumulation_steps be\n            applied)\n        performance_mode: bool: whether to optimize the whole\n            training process this will cut off some logging options\n            and optimize training process.\n        neftune_noise_alpha: Optional[float]: If not `None`, this\n            will activate NEFTune noise embeddings. This has been\n            proven to drastically improve model performances for\n            instruction fine-tuning.\n        loaded_model_config_kwargs: Optional[dict]: config key\n            arguments to be passed to the model while being loaded\n        **kwargs: Pass keyword, variable-length argument list\n    from checkpoint\n    \"\"\"\n    super().__init__()\n\n    if ids_to_pop_from_dataset is None:\n        ids_to_pop_from_dataset = []\n    if extra_optimizer_kwargs is None:\n        extra_optimizer_kwargs = {}\n\n    if model_class is None and model_huggingface_repo_id is None:\n        print(\n            termcolor.colored(\n                \"Warning : \", color=\"red\", force_color=True\n            ) + termcolor.colored(\n                \"You should at least pass model_class or model_huggingface_repo_id if you want to use \"\n                \"CasualLanguageModel Trainer But in case that you want to use \"\n                \"DPOTrainer or ORPOTrainer you can ignore this warning\", color=\"white\",\n                force_color=True\n            )\n        )\n    assert backend in AVAILABLE_BACKENDS, (\n        f\"{backend} is not recognized, \"\n        f\"available backends are {AVAILABLE_BACKENDS}\"\n    )\n\n    if track_memory is None:\n        # https://github.com/erfanzar/EasyDeL/pull/100/commits/523ce7b1515d7896d456759d0bcd0bd02369bd10\n        print(\n            termcolor.colored(\"Information : \", color=\"red\", force_color=True),\n            termcolor.colored(\n                \"track_memory is set to False by default inorder make make training faster. \"\n                \"you can turn it on with just passing `track_memory=True` in TrainArguments\",\n                color=\"white\", force_color=True\n            )\n        )\n        track_memory = False\n\n    available_backends = len(jax.devices(backend))\n    if force_batch_and_gradient_accumulation_steps_calculation:\n        total_batch_size *= gradient_accumulation_steps  # Changed and will be handled inside FJFormer\n    array_devices = jnp.ones((available_backends, 1)).reshape(sharding_array)\n    JaxDistributedConfig.initialize(jax_distributed_config)\n    self.force_batch_and_gradient_accumulation_steps_calculation = (\n        force_batch_and_gradient_accumulation_steps_calculation\n    )\n    self.available_backends = available_backends\n    self.array_devices_shape = array_devices.shape\n    self.model_huggingface_repo_id = model_huggingface_repo_id\n    self.num_train_epochs = num_train_epochs\n    self.wandb_entity = wandb_entity\n    self.total_batch_size = total_batch_size\n    self.max_training_steps = max_training_steps\n    self.max_evaluation_steps = max_evaluation_steps\n    self.optimizer = optimizer\n    self.scheduler = scheduler\n    self.extra_optimizer_kwargs = extra_optimizer_kwargs\n    self.learning_rate = learning_rate\n    self.learning_rate_end = learning_rate_end\n    self.weight_decay = weight_decay\n    self.label_smoothing_factor = label_smoothing_factor\n    self.z_loss = z_loss\n    self.model_name = model_name\n    self.gradient_checkpointing = gradient_checkpointing\n    self.max_sequence_length = max_sequence_length\n    self.sharding_array = sharding_array\n    self.is_fine_tuning = is_fine_tuning\n    self.do_train = do_train\n    self.do_eval = do_eval\n    self.do_test = do_test\n    self.train_on_inputs = train_on_inputs\n    self.save_steps = save_steps\n    self.save_dir = save_dir\n    self.save_total_limit = save_total_limit\n    self.dtype = dtype\n    self.warmup_steps = warmup_steps\n    self.param_dtype = param_dtype\n    self.fully_sharded_data_parallel = fully_sharded_data_parallel\n    self.use_wandb = use_wandb\n    self.custom_rule = custom_rule\n    self.extra_configs = extra_configs\n    self.ids_to_pop_from_dataset = ids_to_pop_from_dataset\n    self.remove_ckpt_after_load = remove_ckpt_after_load\n    self.model_class = model_class\n    self.configs_to_initialize_model_class = configs_to_initialize_model_class\n    self.do_last_save = do_last_save\n    self.model_parameters = model_parameters\n    self.do_shard_fns = do_shard_fns\n    self.gradient_accumulation_steps = gradient_accumulation_steps\n    self.track_memory = track_memory\n    self.loss_chunk = loss_chunk\n    self.loss_re_mat = loss_re_mat\n    self.init_input_shape = init_input_shape\n    self.truncation_mode = truncation_mode\n    self.step_partition_spec = step_partition_spec\n    self.jax_distributed_config = jax_distributed_config\n    self.log_all_workers = log_all_workers\n    self.dataloader_num_workers = dataloader_num_workers\n    self.dataloader_pin_memory = dataloader_pin_memory\n    self.save_optimizer_state = save_optimizer_state\n    self.step_start_point = step_start_point if step_start_point is not None else 0\n    self.verbose = verbose\n    self.offload_device = offload_device\n    self.performance_mode = performance_mode\n    self.neftune_noise_alpha = neftune_noise_alpha\n    self.loaded_model_config_kwargs = loaded_model_config_kwargs\n    if use_wandb and performance_mode:\n        self.use_wandb = False\n    self.optimizer_kwargs = dict(\n        learning_rate=self.learning_rate,\n        learning_rate_end=self.learning_rate_end,\n        optimizer=self.optimizer,\n        scheduler=self.scheduler,\n        extra_optimizer_kwargs=self.extra_optimizer_kwargs,\n        warmup_steps=self.warmup_steps,\n        gradient_accumulation_steps=self.gradient_accumulation_steps,\n        weight_decay=self.weight_decay,\n        steps=self.max_training_steps,\n    )\n    self.training_time = self._time_to_seconds(training_time) if training_time is not None else None\n    self.merge_lora_rapture_parameters = merge_lora_rapture_parameters\n    self.rapture = None\n    self.rapture_config = None\n    self.remove_unused_columns = remove_unused_columns\n    self._stop_capturing_memory = False\n    self._captured_memory = {}\n    self.log_grad_norms = log_grad_norms\n    if rapture_config is not None and log_grad_norms:\n        warnings.warn(\n            \"setting `log_grad_norms` to off since using log grad norms while using LoRA is not Supported.\"\n        )\n        self.log_grad_norms = False\n    self.state_apply_fn_kwarguments_to_model = (\n        state_apply_fn_kwarguments_to_model\n    ) if state_apply_fn_kwarguments_to_model is not None else {}\n    if rapture_config is not None:\n        print(\n            termcolor.colored(\"Warning : \", color=\"red\", force_color=True),\n            termcolor.colored(\n                \"You are using LoRA (Low-Rank Adaptation of Large Language Models) and this feature is\"\n                \"still in Beta mode so it might act unexpected\", color=\"red\", force_color=True\n            )\n        )\n        self.rapture_config = rapture_config\n        self.rapture = XRapTure(config=rapture_config)\n    self.__dict__.update(**kwargs)\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.ckpt_path_exists","title":"<code>ckpt_path_exists()</code>","text":"<p>The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A path</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def ckpt_path_exists(self):\n    \"\"\"The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A path\n    \"\"\"\n    path = self.get_path()\n    if not path.exists():\n        path.mkdir(parents=True)\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_board","title":"<code>get_board()</code>","text":"<p>The get_board function is a helper function that returns a TensorBoard object. The TensorBoard object is used to log the training and validation loss, as well as the accuracy of the model during training. The get_board function takes no arguments, and returns an instance of torch.utils.tensorboard SummaryWriter class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A summary-writer object</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_board(self):\n    \"\"\"The get_board function is a helper function that returns a TensorBoard object.\n    The TensorBoard object is used to log the training and validation loss, as well as\n    the accuracy of the model during training. The get_board function takes no arguments,\n    and returns an instance of torch.utils.tensorboard SummaryWriter class.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A summary-writer object\n    \"\"\"\n    return flax.metrics.tensorboard.SummaryWriter(log_dir=str(self.get_path()))\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_mesh","title":"<code>get_mesh()</code>","text":"<p>The get_mesh function is used to create a mesh object that can be used to define the geometry of the device. The mesh object contains two arrays: a list of vertices and a list of faces. Each face is defined by three indices, which correspond to three vertices in the vertex array. The get_mesh function is called when creating an instance of DeviceGeometry, which is then passed into an instance of DeviceSimulation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <p>A mesh object with the device array shape and the mesh names</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_mesh(self):\n    \"\"\"The get_mesh function is used to create a mesh object that can be used\n    to define the geometry of the device. The mesh object contains two arrays:\n    a list of vertices and a list of faces. Each face is defined by three indices,\n    which correspond to three vertices in the vertex array. The get_mesh function\n    is called when creating an instance of DeviceGeometry, which is then passed\n    into an instance of DeviceSimulation.\n\n    Args:\n        self: Refer to the object itself\n\n    Returns:\n        A mesh object with the device array shape and the mesh names\n    \"\"\"\n    return Mesh(\n        create_device_mesh(\n            self.array_devices_shape\n        ),\n        self.get_mesh_names()\n    )\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_meter_dict","title":"<code>get_meter_dict()</code>","text":"<p>The get_meter_dict function is used to return a dictionary of the hyperparameters. The function iterates through all the attributes in the class and returns a dictionary with the key as \"hyperparameters/{k}\" and value as v for each attribute k,v in self.dict if it is an  instance of int, float, str, bool or torch.Tensor.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A dictionary of hyperparameters</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_meter_dict(self):\n    \"\"\"The get_meter_dict function is used to return a dictionary of the hyperparameters.\n    The function iterates through all the attributes in the class and returns a dictionary with\n    the key as &amp;quot;hyperparameters/{k}&amp;quot; and value as v for each attribute k,v in self.__dict__ if it is an\n     instance of int, float, str, bool or torch.Tensor.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A dictionary of hyperparameters\n    \"\"\"\n    return {\n        f\"hyperparameters/{k}\": v for k, v in self.__dict__.items() if\n        isinstance(v, (int, float, str, bool))\n    }\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_path","title":"<code>get_path()</code>","text":"<p>The get_path function returns a pathlib.Path object, which is a class that represents file paths and provides methods for interacting with the files at those paths. The get_path function takes no arguments and returns an instance of the Path class initialized with two arguments: self.save_dir (a string) and self.model_name (also a string). The save directory is the directory where we'll store our model checkpoints, while the model name will be used to create unique filenames for each checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A pathlib</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_path(self):\n    \"\"\"The get_path function returns a pathlib.Path object, which is a class that\n    represents file paths and provides methods for interacting with the files at\n    those paths. The get_path function takes no arguments and returns an instance of\n    the Path class initialized with two arguments: self.save_dir (a string) and\n    self.model_name (also a string). The save directory is the directory where we'll\n    store our model checkpoints, while the model name will be used to create unique\n    filenames for each checkpoint.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A pathlib\n    \"\"\"\n    return pathlib.Path(\n        self.save_dir, self.model_name\n    )\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_streaming_checkpointer","title":"<code>get_streaming_checkpointer()</code>","text":"<p>The get_streaming_checkpointer function is used to save the model's weights. The streaming checkpointer saves the model's weights in a file called \"checkpoint\" and then saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001, checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A CheckpointManager object</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_streaming_checkpointer(self):\n    \"\"\"The get_streaming_checkpointer function is used to save the model's weights.\n    The streaming checkpointer saves the model's weights in a file called &amp;quot;checkpoint&amp;quot; and then\n    saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001,\n    checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        A CheckpointManager object\n    \"\"\"\n    return CheckpointManager(\n        os.path.join(self.save_dir, self.model_name),\n        save_optimizer_state=self.save_optimizer_state,\n        verbose=self.verbose\n    )\n</code></pre>"},{"location":"generated-trainer-training_configurations/#src.python.easydel.trainer.training_configurations.TrainArguments.get_wandb_init","title":"<code>get_wandb_init()</code>","text":"<p>The get_wandb_init function is a helper function that returns the wandb.init() call with the project name, config object, and tags set to appropriate values for this model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Pass the class instance to the function</p> required <p>Returns:</p> Type Description <code>Optional[Union[Run, RunDisabled]]</code> <p>A wandb or None</p> Source code in <code>src/python/easydel/trainer/training_configurations.py</code> <pre><code>def get_wandb_init(self) -&gt; Optional[Union[\"Run\" , \"RunDisabled\"]] :  # type:ignore\n    \"\"\"The get_wandb_init function is a helper function that returns the wandb.init() call with\n    the project name, config object, and tags set to appropriate values for this model.\n\n    Args:\n        self: Pass the class instance to the function\n\n    Returns:\n        A wandb or None\n    \"\"\"\n    if wandb is None:\n        return None\n    else:\n        return wandb.init(  # noqa\n            project=f\"EasyDeL-{self.model_name}\",\n            config=self(),\n            tags=[\n                \"EasyDeL\",\n                \"FJFormer\",\n                \"OST-OpenSourceTransformers\",\n                \"Jax/Flax\"\n            ],\n            entity=self.wandb_entity\n\n        ) if self.log_all_workers or (jax.process_index() == 0) else None\n</code></pre>"},{"location":"generated-trainer-utils/","title":"trainer.utils","text":""},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.DataCollatorForCompletionOnlyLM","title":"<code>DataCollatorForCompletionOnlyLM</code>","text":"<p>Data collator used for completion tasks. It ensures that all the tokens of the labels are set to an 'ignore_index' when they do not come from the assistant. This ensures that the loss is only calculated on the completion made by the assistant.</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>class DataCollatorForCompletionOnlyLM:\n    \"\"\"Data collator used for completion tasks. It ensures that all the tokens of the labels are set to an 'ignore_index'\n    when they do not come from the assistant. This ensures that the loss is only\n    calculated on the completion made by the assistant.\n    \"\"\"\n\n    def __init__(\n            self,\n            tokenizer: Union[str, PreTrainedTokenizerBase],\n            response_template: Union[str, List[int]],\n            instruction_template: Optional[Union[str, List[int]]] = None,\n            *args,\n            mlm: bool = False,\n            ignore_index: int = -100,\n            **kwargs,\n    ):\n        if isinstance(tokenizer, str):\n            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n            self.tokenizer = tokenizer\n        self.instruction_template = instruction_template\n        if isinstance(instruction_template, str):\n            self.instruction_token_ids = self.tokenizer.encode(self.instruction_template, add_special_tokens=False)\n        else:\n            self.instruction_token_ids = instruction_template\n\n        self.response_template = response_template\n        if isinstance(response_template, str):\n            self.response_token_ids = self.tokenizer.encode(self.response_template, add_special_tokens=False)\n        else:\n            self.response_token_ids = response_template\n\n        if not mlm and self.instruction_template and self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n            warnings.warn(\n                \"The pad_token_id and eos_token_id values of this tokenizer are identical. \"\n                \"If you are planning for multi-turn training, \"\n                \"it can result in the model continuously generating questions and answers without eos token. \"\n                \"To avoid this, set the pad_token_id to a different value.\"\n            )\n\n        self.ignore_index = ignore_index\n\n    def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n        if not isinstance(self.tokenizer, (BertTokenizer, BertTokenizerFast)):\n            warnings.warn(\n                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n                \"Please refer to the documentation for more information.\"\n            )\n\n        cand_indexes = []\n        for i, token in enumerate(input_tokens):\n            if token == \"[CLS]\" or token == \"[SEP]\":\n                continue\n\n            if len(cand_indexes) &gt;= 1 and token.startswith(\"##\"):\n                cand_indexes[-1].append(i)\n            else:\n                cand_indexes.append([i])\n\n        random.shuffle(cand_indexes)\n        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * 0.15))))\n        masked_lms = []\n        covered_indexes = set()\n        for index_set in cand_indexes:\n            if len(masked_lms) &gt;= num_to_predict:\n                break\n            if len(masked_lms) + len(index_set) &gt; num_to_predict:\n                continue\n            is_any_index_covered = False\n            for index in index_set:\n                if index in covered_indexes:\n                    is_any_index_covered = True\n                    break\n            if is_any_index_covered:\n                continue\n            for index in index_set:\n                covered_indexes.add(index)\n                masked_lms.append(index)\n\n        if len(covered_indexes) != len(masked_lms):\n            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n        return mask_labels\n\n    def jax_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -&gt; Tuple[Any, Any]:\n        \"\"\"Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\"\"\"\n        labels = np.copy(inputs)\n        probability_matrix = np.full(labels.shape, 0.15)\n        if special_tokens_mask is None:\n            special_tokens_mask = [\n                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n            ]\n            special_tokens_mask = np.array(special_tokens_mask, dtype=bool)\n        else:\n            special_tokens_mask = special_tokens_mask.astype(bool)\n\n        probability_matrix[special_tokens_mask] = 0\n        masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n        labels[~masked_indices] = -100\n        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) &amp; masked_indices\n        inputs[indices_replaced] = self.tokenizer.mask_token_id\n        indices_random = (\n                np.random.binomial(1, 0.5, size=labels.shape).astype(bool) &amp; masked_indices &amp; ~indices_replaced\n        )\n        random_words = np.random.randint(\n            low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n        )\n        inputs[indices_random] = random_words\n        return inputs, labels\n\n    def jax_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -&gt; Dict[str, Any]:\n        if isinstance(examples[0], Mapping):\n            input_ids = [e[\"input_ids\"] for e in examples]\n        else:\n            input_ids = examples\n            examples = [{\"input_ids\": e} for e in examples]\n\n        batch_input = _collate_batch(input_ids, self.tokenizer, )\n\n        mask_labels = []\n        for e in examples:\n            ref_tokens = []\n            for ida in tolist(e[\"input_ids\"]):\n                token = self.tokenizer._convert_id_to_token(ida)\n                ref_tokens.append(token)\n\n            # For Chinese tokens, we need extra inf to mark sub-word, e.g [\u559c,\u6b22]-&gt; [\u559c\uff0c##\u6b22]\n            if \"chinese_ref\" in e:\n                ref_pos = tolist(e[\"chinese_ref\"])\n                len_seq = len(e[\"input_ids\"])\n                for i in range(len_seq):\n                    if i in ref_pos:\n                        ref_tokens[i] = \"##\" + ref_tokens[i]\n            mask_labels.append(self._whole_word_mask(ref_tokens))\n        batch_mask = _collate_batch(mask_labels, self.tokenizer, )\n        inputs, labels = self.jax_mask_tokens(batch_input, batch_mask)\n        return {\"input_ids\": inputs, \"labels\": labels}\n\n    def __call__(\n            self,\n            examples: List[Union[List[int], Any, Dict[str, Any]]]\n    ) -&gt; Dict[str, Any]:\n        batch = self.jax_call(examples)\n\n        if self.instruction_template is None:\n            for i in range(len(examples)):\n                response_token_ids_start_idx = None\n\n                for idx in jnp.where(batch[\"labels\"][i] == self.response_token_ids[0])[0]:\n                    if (\n                            self.response_token_ids\n                            == batch[\"labels\"][i][idx: idx + len(self.response_token_ids)].tolist()\n                    ):\n                        response_token_ids_start_idx = idx\n\n                if response_token_ids_start_idx is None:\n                    warnings.warn(\n                        f\"Could not find response key `{self.response_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n                else:\n                    response_token_ids_end_idx = response_token_ids_start_idx + len(self.response_token_ids)\n                    batch[\"labels\"][i, :response_token_ids_end_idx] = self.ignore_index\n\n        else:\n            for i in range(len(examples)):\n                response_token_ids_idxs = []\n                human_token_ids_idxs = []\n\n                for assistant_idx in jnp.where(batch[\"labels\"][i] == self.response_token_ids[0])[0]:\n                    if (\n                            self.response_token_ids\n                            == batch[\"labels\"][i][assistant_idx: assistant_idx + len(self.response_token_ids)].tolist()\n                    ):\n                        response_token_ids_idxs.append(assistant_idx + len(self.response_token_ids))\n\n                if len(response_token_ids_idxs) == 0:\n                    warnings.warn(\n                        f\"Could not find response key `{self.response_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n\n                human_token_ids = self.instruction_token_ids\n                for human_idx in jnp.where(batch[\"labels\"][i] == human_token_ids[0])[0]:\n                    if human_token_ids == batch[\"labels\"][i][human_idx: human_idx + len(human_token_ids)].tolist():\n                        human_token_ids_idxs.append(human_idx)\n\n                if len(human_token_ids_idxs) == 0:\n                    warnings.warn(\n                        f\"Could not find instruction key `{self.instruction_template}` in the \"\n                        f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n                        f\"This instance will be ignored in loss calculation. \"\n                        f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n                    )\n                    batch[\"labels\"][i, :] = self.ignore_index\n\n                if (\n                        len(human_token_ids_idxs) &gt; 0\n                        and len(response_token_ids_idxs) &gt; 0\n                        and human_token_ids_idxs[0] &gt; response_token_ids_idxs[0]\n                ):\n                    human_token_ids_idxs = [0] + human_token_ids_idxs\n\n                for idx, (start, end) in enumerate(zip(human_token_ids_idxs, response_token_ids_idxs)):\n                    if idx != 0:\n                        batch[\"labels\"][i, start:end] = self.ignore_index\n                    else:\n                        batch[\"labels\"][i, :end] = self.ignore_index\n\n                if len(response_token_ids_idxs) &lt; len(human_token_ids_idxs):\n                    batch[\"labels\"][i, human_token_ids_idxs[-1]:] = self.ignore_index\n\n        return batch\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.DataCollatorForCompletionOnlyLM.jax_mask_tokens","title":"<code>jax_mask_tokens(inputs, special_tokens_mask=None)</code>","text":"<p>Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>def jax_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -&gt; Tuple[Any, Any]:\n    \"\"\"Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\"\"\"\n    labels = np.copy(inputs)\n    probability_matrix = np.full(labels.shape, 0.15)\n    if special_tokens_mask is None:\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        special_tokens_mask = np.array(special_tokens_mask, dtype=bool)\n    else:\n        special_tokens_mask = special_tokens_mask.astype(bool)\n\n    probability_matrix[special_tokens_mask] = 0\n    masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n    labels[~masked_indices] = -100\n    indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) &amp; masked_indices\n    inputs[indices_replaced] = self.tokenizer.mask_token_id\n    indices_random = (\n            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) &amp; masked_indices &amp; ~indices_replaced\n    )\n    random_words = np.random.randint(\n        low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n    )\n    inputs[indices_random] = random_words\n    return inputs, labels\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.JaxDistributedConfig","title":"<code>JaxDistributedConfig</code>","text":"<p>               Bases: <code>object</code></p> <p>From EasyLM Utility class for initializing JAX distributed.</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>class JaxDistributedConfig(object):\n    \"\"\"\n    From EasyLM\n    Utility class for initializing JAX distributed.\n    \"\"\"\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.initialize_jax_distributed = False\n        config.coordinator_address = placeholder(str)\n        config.num_processes = placeholder(int)\n        config.process_id = placeholder(int)\n        config.local_device_ids = placeholder(str)\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    @classmethod\n    def initialize(cls, config):\n        config = cls.get_default_config(config)\n        if config.initialize_jax_distributed:\n            if config.local_device_ids is not None:\n                local_device_ids = [int(x) for x in config.local_device_ids.split(',')]\n            else:\n                local_device_ids = None\n\n            jax.distributed.initialize(\n                coordinator_address=config.coordinator_address,\n                num_processes=config.num_processes,\n                process_id=config.process_id,\n                local_device_ids=local_device_ids,\n            )\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.conversations_formatting_function","title":"<code>conversations_formatting_function(tokenizer, messages_field)</code>","text":"<p>return a callable function that takes in a \"messages\" dataset and returns a formatted dataset, based on the tokenizer apply chat template to the dataset</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>def conversations_formatting_function(tokenizer: AutoTokenizer, messages_field: Literal[\"messages\", \"conversations\"]):\n    r\"\"\"\n    return a callable function that takes in a \"messages\" dataset and returns a formatted dataset, based on the tokenizer\n    apply chat template to the dataset\n    \"\"\"\n\n    def format_dataset(examples):\n        if isinstance(examples[messages_field][0], list):\n            output_texts = []\n            for i in range(len(examples[messages_field])):\n                output_texts.append(\n                    tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))  # type: ignore\n            return output_texts\n        else:\n            return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)  # type: ignore\n\n    return format_dataset\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.get_formatting_func_from_dataset","title":"<code>get_formatting_func_from_dataset(dataset, tokenizer)</code>","text":"<p>from TRL Finds the correct formatting function based on the dataset structure. Currently supported datasets are: - <code>ChatML</code> with [{\"role\": str, \"content\": str}] - <code>instruction</code> with [{\"prompt\": str, \"completion\": str}]</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>User dataset</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>Tokenizer used for formatting</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Optional[Callable]</code> <p>Formatting function if the dataset format is supported else None</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>def get_formatting_func_from_dataset(\n        dataset: Union[Dataset, \"ConstantLengthDataset\"], tokenizer: AutoTokenizer  # type: ignore\n) -&gt; Optional[Callable]:\n    r\"\"\"from TRL\n    Finds the correct formatting function based on the dataset structure. Currently supported datasets are:\n    - `ChatML` with [{\"role\": str, \"content\": str}]\n    - `instruction` with [{\"prompt\": str, \"completion\": str}]\n\n    Args:\n        dataset (Dataset): User dataset\n        tokenizer (AutoTokenizer): Tokenizer used for formatting\n\n    Returns:\n        Callable: Formatting function if the dataset format is supported else None\n    \"\"\"\n    if isinstance(dataset, Dataset):\n        if \"messages\" in dataset.features:\n            if dataset.features[\"messages\"] == FORMAT_MAPPING[\"chatml\"]:\n                logging.info(\"Formatting dataset with chatml format\")\n                return conversations_formatting_function(tokenizer, \"messages\")\n        if \"conversations\" in dataset.features:\n            if dataset.features[\"conversations\"] == FORMAT_MAPPING[\"chatml\"]:\n                logging.info(\"Formatting dataset with chatml format\")\n                return conversations_formatting_function(tokenizer, \"conversations\")\n        elif dataset.features == FORMAT_MAPPING[\"instruction\"]:\n            logging.info(\"Formatting dataset with instruction format\")\n            return instructions_formatting_function(tokenizer)\n\n    return None\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.instructions_formatting_function","title":"<code>instructions_formatting_function(tokenizer)</code>","text":"<p>from TRL return a callable function that takes in an \"instructions\" dataset and returns a formatted dataset, based on the tokenizer apply chat template to the dataset</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>def instructions_formatting_function(tokenizer: AutoTokenizer):\n    r\"\"\"from TRL\n    return a callable function that takes in an \"instructions\" dataset and returns a formatted dataset, based on the tokenizer\n    apply chat template to the dataset\n    \"\"\"\n\n    def format_dataset(examples):\n        if isinstance(examples[\"prompt\"], list):\n            output_texts = []\n            for i in range(len(examples[\"prompt\"])):\n                converted_sample = [\n                    {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n                    {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n                ]\n                output_texts.append(tokenizer.apply_chat_template(converted_sample, tokenize=False))  # type: ignore\n            return output_texts\n        else:\n            converted_sample = [\n                {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n                {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n            ]\n            return tokenizer.apply_chat_template(converted_sample, tokenize=False)  # type: ignore\n\n    return format_dataset\n</code></pre>"},{"location":"generated-trainer-utils/#src.python.easydel.trainer.utils.tolist","title":"<code>tolist(x)</code>","text":"<p>from HF Args:     x:</p> <p>Returns: X as List</p> Source code in <code>src/python/easydel/trainer/utils.py</code> <pre><code>def tolist(x):\n    \"\"\"from HF\n    Args:\n        x:\n\n    Returns: X as List\n\n    \"\"\"\n    if isinstance(x, list):\n        return x\n    elif hasattr(x, \"numpy\"):\n        x = x.numpy()\n    return x.tolist()\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-fwd_bwd_functions/","title":"trainer.vision_causal_language_model_trainer.fwd_bwd_functions","text":""},{"location":"generated-trainer-vision_causal_language_model_trainer-fwd_bwd_functions/#src.python.easydel.trainer.vision_causal_language_model_trainer.fwd_bwd_functions.create_vision_casual_language_model_evaluation_step","title":"<code>create_vision_casual_language_model_evaluation_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp'))</code>","text":"<p>The create_vision_casual_language_model_evaluation_step function is used to create a function that calculates the  loss and accuracy of a model. It takes in a set of parameters, which are then passed into the state.apply_fn function to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify the partitioning of the model parameters</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp')</code> <p>Returns:</p> Type Description <p>A function that can be used to calculate the loss and accuracy</p> <p>of a model</p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/fwd_bwd_functions.py</code> <pre><code>def create_vision_casual_language_model_evaluation_step(partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\")):\n    \"\"\"The create_vision_casual_language_model_evaluation_step function is used to create a function that calculates the\n     loss and accuracy of a model. It takes in a set of parameters, which are then passed into the state.apply_fn function\n    to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these\n    logits.\n\n    Args:\n        partition_spec: Specify the partitioning of the model parameters\n\n    Returns:\n        A function that can be used to calculate the loss and accuracy\n        of a model\n    \"\"\"\n\n    def vision_casual_language_model_evaluation_step(state, batch) -&gt; [\n        EasyDeLState,\n        chex.Array,\n        VisionCausalLanguageModelStepOutput\n    ]:\n        \"\"\"The vision_casual_language_model_train_step function is a training step function that takes in the current state\n        of the model and a batch of data. It then calculates the loss and accuracy for this batch,\n        and returns an updated state with new parameters based on these gradients.\n\n        Args:\n            state: Store the model parameters\n            batch: Pass the data to the model\n\n        Returns:\n            A tuple of (state, loss,\n            VisionCausalLanguageModelStepOutput)\n        \"\"\"\n        batch = with_sharding_constraint(batch, partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.get(\"labels\", None)\n            if labels is None:\n                labels = batch[\"input_ids\"][..., 1:]\n            else:\n                labels = labels[..., 1:]\n            label_vision_mask = batch.pop(\"label_vision_mask\")\n            model_outputs = state.apply_fn(params=params, **batch, return_dict=True)\n            logits = model_outputs.logits\n            aux_loss = getattr(model_outputs, \"aux_loss\", None)\n\n            vision_loss, vision_accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :],\n                jnp.where(label_vision_mask, labels, 0),\n                batch[\"attention_mask\"].astype(jnp.float32)[:, 1:] * label_vision_mask\n            )\n            text_loss, text_accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :],\n                jnp.where(label_vision_mask, 0, labels),\n                batch[\"attention_mask\"].astype(jnp.float32)[:, 1:] * (1.0 - label_vision_mask)\n            )\n\n            loss = 0.5 * (vision_loss + text_loss + (aux_loss if aux_loss is not None else 0.))\n\n            return loss, VisionCausalLanguageModelStepOutput(\n                loss=loss,\n                text_accuracy=text_accuracy,\n                vision_accuracy=vision_accuracy,\n                text_loss=text_loss,\n                vision_loss=vision_loss\n            )\n\n        loss__, metrics = calculate_loss(state.params)\n        return loss__, metrics\n\n    return vision_casual_language_model_evaluation_step\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-fwd_bwd_functions/#src.python.easydel.trainer.vision_causal_language_model_trainer.fwd_bwd_functions.create_vision_casual_language_model_train_step","title":"<code>create_vision_casual_language_model_train_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'sp'))</code>","text":"<p>The create_vision_casual_language_model_train_step function is a training step function that takes in the current  state of the model,and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state with new parameters based on these gradients.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify which devices the model will be split across</p> <code>PartitionSpec(('dp', 'fsdp'), 'sp')</code> <p>Returns:</p> Type Description <p>A casual_language_model_train_step function that takes in the</p> <p>current state of the model,</p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/fwd_bwd_functions.py</code> <pre><code>def create_vision_casual_language_model_train_step(partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\")):\n    \"\"\"The create_vision_casual_language_model_train_step function is a training step function that takes in the current\n     state of the model,and a batch of data. It then calculates the loss and accuracy for this batch, and returns\n    an updated state with new parameters based on these gradients.\n\n    Args:\n        partition_spec: Specify which devices the model will be split\n            across\n\n    Returns:\n        A casual_language_model_train_step function that takes in the\n        current state of the model,\n    \"\"\"\n\n    def vision_casual_language_model_train_step(state, batch) -&gt; [\n        EasyDeLState,\n        chex.Array,\n        VisionCausalLanguageModelStepOutput\n    ]:\n        \"\"\"The vision_casual_language_model_train_step function is a training step function that takes in the current state\n        of the model and a batch of data. It then calculates the loss and accuracy for this batch,\n        and returns an updated state with new parameters based on these gradients.\n\n        Args:\n            state: Store the model parameters\n            batch: Pass the data to the model\n\n        Returns:\n            A tuple of (state, loss,\n            VisionCausalLanguageModelStepOutput)\n        \"\"\"\n        batch = with_sharding_constraint(batch, partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.get(\"labels\", None)\n            if labels is None:\n                labels = batch[\"input_ids\"][..., 1:]\n            else:\n                labels = labels[..., 1:]\n            label_vision_mask = batch.pop(\"label_vision_mask\")\n\n            model_outputs = state.apply_fn(params=params, **batch, return_dict=True)\n            logits = model_outputs.logits\n            aux_loss = getattr(model_outputs, \"aux_loss\", None)\n\n            vision_loss, vision_accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :],\n                jnp.where(label_vision_mask, labels, 0),\n                batch[\"attention_mask\"].astype(jnp.float32)[:, 1:] * label_vision_mask\n            )\n            text_loss, text_accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :],\n                jnp.where(label_vision_mask, 0, labels),\n                batch[\"attention_mask\"].astype(jnp.float32)[:, 1:] * (1.0 - label_vision_mask)\n            )\n\n            loss = 0.5 * (vision_loss + text_loss + (aux_loss if aux_loss is not None else 0.))\n\n            return loss, VisionCausalLanguageModelStepOutput(\n                loss=loss,\n                text_accuracy=text_accuracy,\n                vision_accuracy=vision_accuracy,\n                text_loss=text_loss,\n                vision_loss=vision_loss\n            )\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (loss__, metrics), grad = grad_fn(state.params)\n        state = state.apply_gradients(grads=grad)\n        return state, loss__, metrics\n\n    return vision_casual_language_model_train_step\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-modelling_output/","title":"trainer.vision_causal_language_model_trainer.modelling_output","text":""},{"location":"generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/","title":"trainer.vision_causal_language_model_trainer.vision_causal_language_model_trainer","text":""},{"location":"generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/#src.python.easydel.trainer.vision_causal_language_model_trainer.vision_causal_language_model_trainer.VisionCausalLanguageModelTrainer","title":"<code>VisionCausalLanguageModelTrainer</code>","text":"<p>               Bases: <code>CausalLanguageModelTrainer</code></p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/vision_causal_language_model_trainer.py</code> <pre><code>class VisionCausalLanguageModelTrainer(CausalLanguageModelTrainer):\n\n    def create_collate_function(\n            self,\n            max_sequence_length: int,\n            truncation_mode: typing.Literal[\"keep_end\", \"keep_start\"] = \"keep_end\",\n    ) -&gt; Callable:\n        def collate_fn(batch):\n            results = {}\n            corrected_sequence = None\n            for key in batch[0].keys():\n                if truncation_mode == \"keep_end\":\n                    corrected_sequence = [\n                        jnp.array(f[key])[..., -max_sequence_length:] for f in batch\n                    ]\n                else:\n                    corrected_sequence = [\n                        jnp.array(f[key])[..., :max_sequence_length] for f in batch\n                    ]\n                results[key] = jnp.stack(corrected_sequence).reshape(\n                    -1,\n                    corrected_sequence[0].shape[-1]\n                )\n            return results\n\n        return collate_fn\n\n    def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n        \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n        them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n        on a batch of data, including:\n\n        Args:\n            self: Access the class attributes\n\n        Returns:\n            A TrainerConfigureFunctionFuncOutput object\n        \"\"\"\n\n        def initialize_state_function():\n            initialized_parameters = self.model.init_weights(\n                jax.random.PRNGKey(0),\n                self.arguments.init_input_shape\n            )\n\n            if self.arguments.dtype == jnp.bfloat16:\n                initialized_parameters = self.model.to_bf16(initialized_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n            tx = self.tx\n            parameters = flax.core.freeze({\"params\": initialized_parameters})\n            tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n            if self.rapture is not None:\n                lora_parameters = self.lora_parameters\n                if self.arguments.dtype == jnp.bfloat16:\n                    lora_parameters = self.model.to_bf16(lora_parameters)\n                elif self.arguments.dtype == jnp.float16:\n                    lora_parameters = self.model.to_fp16(lora_parameters)\n\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=lora_parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(tx_init),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n            else:\n                return EasyDeLState.create(\n                    tx=tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=tx_init,\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n\n        def create_state_from_params_function(parameters):\n            if self.rapture is None:\n                return EasyDeLState.create(\n                    tx=self.tx,\n                    params=parameters,\n                    apply_fn=self.model.__call__,\n                    module_config=copy.deepcopy(self.model.config),\n                    tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.model,\n                    module_config_args=None\n                )\n            else:\n                return EasyDeLState(\n                    step=0,\n                    apply_fn=self.lora_apply_fn,\n                    params=parameters,\n                    tx=self.lora_tx,\n                    opt_state=self.lora_opt_state,\n                    tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                    hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                    module=self.lora_model,\n                    module_config=self.model.config,\n                    module_config_args=None,\n                )\n\n        state_shape = jax.eval_shape(initialize_state_function)\n        state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            state_shape\n        )\n        create_sharded_state_from_params_function = pjit(\n            create_state_from_params_function,\n            in_shardings=(state_partition_spec.params,),\n            out_shardings=state_partition_spec,\n            donate_argnums=(0,)\n        )\n        sharded_train_step_function = pjit(\n            create_vision_casual_language_model_train_step(self.arguments.step_partition_spec),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n\n        sharded_eval_step_function = pjit(\n            create_vision_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n            in_shardings=(state_partition_spec, PartitionSpec()),\n            out_shardings=(PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n\n        mesh = self.arguments.get_mesh()\n        self.arguments.ckpt_path_exists()\n        checkpoint_manager = self.arguments.get_streaming_checkpointer()\n        self.state_partition_spec = state_partition_spec\n        self.state_shape = state_shape\n\n        return TrainerConfigureFunctionFuncOutput(\n            create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n            sharded_train_step_function=sharded_train_step_function,\n            sharded_eval_step_function=sharded_eval_step_function,\n            mesh=mesh,\n            checkpoint_manager=checkpoint_manager,\n            initialize_state_function=initialize_state_function\n        )\n\n    def initialize_state(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None,\n    ) -&gt; typing.Tuple[EasyDeLState, Mapping[str, Callable], Mapping[str, Callable]]:\n        if model_parameters is None and state is None and self.rapture is None and self.checkpoint_path is None:\n            raise RuntimeError(\n                \"You are passing `model_parameters=None`, `state=None`, and `checkpoint_path=None` and also you are not\"\n                \" using LoRA, if you are \"\n                \"Using LoRA make sure to pass parameters and Rapture Config correctly otherwise pass the \"\n                \"model_parameters or state.\"\n            )\n        if model_parameters is None and state is None:\n            model_parameters = self.lora_parameters\n        with self.mesh:\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                self.state_partition_spec,\n                dtype_specs=self.dtype\n            )\n            if state is not None:\n                sharded_state = state\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n                if sharded_state.opt_state is None:\n                    prefix_print(\n                        \"Action\", \"Optimizer State is not Found!, initializing one.\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = sharded_state.init_opt_state()\n                        opt_state = sharded_state.opt_state if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                            lambda f, x: f(x),\n                            shard_fns.opt_state,\n                            sharded_state.opt_state\n                        )\n                        sharded_state = sharded_state.replace(\n                            opt_state=opt_state\n                        )\n            elif self.finetune:\n\n                if model_parameters is None and self.checkpoint_path is not None:\n                    prefix_print(\n                        \"Action\", f\"Loading Model From {self.checkpoint_path}\"\n                    )\n                    with jax.default_device(self.arguments.offload_device):\n                        sharded_state = EasyDeLState.load_state(\n                            verbose=self.arguments.verbose,\n                            state_shard_fns=shard_fns,\n                            init_optimizer_state=True,\n                            checkpoint_path=self.checkpoint_path,\n                            input_shape=self.arguments.init_input_shape\n                        )\n                        # sharded_state = sharded_state.replace(\n                        #     tx=self.tx,\n                        # )\n                        state_shape = jax.eval_shape(lambda: sharded_state)\n                        state_partition_spec = match_partition_rules(\n                            self.config.get_partition_rules(\n                                fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                            state_shape\n                        )\n                        sharded_train_step_function = pjit(\n                            create_vision_casual_language_model_train_step(\n                                partition_spec=self.arguments.step_partition_spec,\n                            ),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n                            donate_argnums=(0, 0),\n                        )\n\n                        sharded_eval_step_function = pjit(\n                            create_vision_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n                            in_shardings=(state_partition_spec, PartitionSpec()),\n                            out_shardings=(PartitionSpec(), PartitionSpec()),\n                            donate_argnums=(0, 0),\n                        )\n\n                        self.state_partition_spec = state_partition_spec\n                        self.state_shape = state_shape\n                        self.sharded_train_step_function = sharded_train_step_function\n                        self.sharded_eval_step_function = sharded_eval_step_function\n\n                    if self.arguments.remove_ckpt_after_load:\n                        os.remove(self.checkpoint_path)\n                elif model_parameters is not None and self.checkpoint_path is None:\n                    prefix_print(\n                        \"Action\", f\"Sharding Passed Parameters\"\n                    )\n                    from flax.core import unfreeze\n                    if not isinstance(model_parameters, flax.core.FrozenDict):\n                        prefix_print(\n                            \"Warning\",\n                            \"Model Parameters should be like FrozenDict({'params': params}) make sure to \"\n                            \"pass as type FrozenDict in case of not getting UnExcepted Errors \"\n                        )\n\n                    model_parameters = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                        lambda f, x: f(x),\n                        shard_fns.params,\n                        model_parameters,\n                    )\n                    sharded_state = self.create_sharded_state_from_params_function(model_parameters)\n                elif model_parameters is not None and self.checkpoint_path is not None:\n                    raise EasyDeLTimerError(\n                        \"You can't pass `model_parameters` and `checkpoint_path` at same time\"\n                    )\n                else:\n                    raise EasyDeLTimerError(\n                        \"You should pass `model_parameters` or `checkpoint_path` to trainer in order to load model\"\n                    )\n            else:\n                sharded_state = self.initialize_state_function()\n                params = sharded_state.params if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x),\n                    shard_fns.params,\n                    sharded_state.params\n                )\n                sharded_state.params = params\n\n            self.sharded_state = sharded_state\n            return sharded_state, shard_fns, gather_fns\n\n    def train(\n            self,\n            model_parameters: Optional[flax.core.FrozenDict] = None,\n            state: Optional[EasyDeLState] = None\n    ) -&gt; VisionCausalLMTrainerOutput:\n        \"\"\"The train function is the main function of this module.\n        It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n        The train function returns an TrainerOutput object that contains the last saved file name, predict func,\n        train state, mesh and checkpoint streamer.\n\n        Args:\n            self: Make the class methods aware of other methods and\n                attributes within the class\n            model_parameters: flax.core.FrozenDict: Load a pre-trained\n                model\n            state: Optional[EasyDeLState]: Ready to Use State\n\n        Returns:\n            An object of type \"TrainerOutput\"\n        \"\"\"\n\n        def count_model_parameters(_p):\n            termcolor.cprint(\n                f\"Model Contain {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9} \"\n                f\"Billion Parameters\",\n                color=\"red\", force_color=True\n            )\n\n        checkpoint_path = \"SAVING_SKIPPED\"\n        start_time = time.time()\n        sharded_state, shard_fns, gather_fns = self.initialize_state(\n            model_parameters=model_parameters,\n            state=state\n        )\n\n        count_model_parameters(sharded_state.params)\n        with self.mesh:\n            pbar = tqdm(total=self.max_training_steps)\n            current_step = int(jax.device_get(sharded_state.step))\n\n            loss_sum = None\n            vision_loss_sum = None\n            vision_accuracy_sum = None\n            text_loss_sum = None\n            text_accuracy_sum = None\n            pbar.update(sharded_state.step.tolist())  # type: ignore\n            learning_rates = []\n            if self.wandb_runtime is not None:\n                model_parameters_number = sum(\n                    n.size for n in\n                    jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_state.params))[0]\n                ) / 1e9\n                self.wandb_runtime.log(\n                    {\n                        \"Number of Model Parameters (Billion)\": model_parameters_number\n                    }\n                )\n                wandb.summary[\"Number of Model Parameters (Billion)\"] = model_parameters_number\n            try:\n                for epoch in range(self.arguments.num_train_epochs):\n                    for batch in self.dataloader_train:\n                        current_step += 1\n                        if (\n                                self.arguments.step_start_point is not None\n                                and\n                                self.arguments.step_start_point &gt; current_step\n                        ):\n                            pbar.update(1)\n                        elif current_step &lt; self.max_training_steps:\n\n                            for ssb in self.arguments.ids_to_pop_from_dataset:\n                                _ = batch.pop(ssb, None)\n                            time_s = time.time()\n                            outputs_and_metrics: tuple[\n                                EasyDeLState, chex.Array, VisionCausalLanguageModelStepOutput\n                            ] = self.sharded_train_step_function(\n                                sharded_state,\n                                batch\n                            )\n                            sharded_state, loss, information_and_accuracies = outputs_and_metrics\n                            ttl_time = time.time() - time_s\n                            loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                            vision_loss = information_and_accuracies.vision_loss\n                            vision_accuracy = information_and_accuracies.vision_accuracy\n                            text_loss = information_and_accuracies.text_loss\n                            text_accuracy = information_and_accuracies.text_accuracy\n\n                            loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                            vision_accuracy_sum = vision_accuracy.tolist() if vision_accuracy_sum is None else (\n                                    vision_accuracy_sum + vision_accuracy\n                            )\n                            vision_loss_sum = vision_loss.tolist() if vision_loss_sum is None else (\n                                    vision_loss_sum + vision_loss\n                            )\n                            text_loss_sum = text_loss.tolist() if text_loss_sum is None else text_loss_sum + text_loss\n                            text_accuracy_sum = text_accuracy.tolist() if text_accuracy_sum is None else (\n                                    text_accuracy_sum + text_accuracy\n                            )\n                            learning_rates.append(self.scheduler(current_step).tolist())\n                            pbar.update(1)\n\n                            trained_tokens = jnp.multiply(\n                                self.arguments.max_sequence_length, jnp.multiply(\n                                    current_step,\n                                    self.arguments.total_batch_size\n                                )\n                            )\n\n                            total_roved_steps = (current_step - self.arguments.step_start_point)\n\n                            with jax.spmd_mode(\"allow_all\"):\n                                train_metrics = {\n\n                                    \"train/loss\": loss.tolist(),\n                                    \"train/mean_loss\": loss_sum / total_roved_steps,\n\n                                    \"train/vision_accuracy\": vision_accuracy,\n                                    \"train/vision_loss\": vision_loss,\n                                    \"train/text_loss\": text_loss,\n                                    \"train/text_accuracy\": text_accuracy,\n\n                                    \"train/mean_vision_accuracy\": vision_accuracy_sum / total_roved_steps,\n                                    \"train/mean_vision_loss\": vision_loss_sum / total_roved_steps,\n                                    \"train/mean_text_loss\": text_loss_sum / total_roved_steps,\n                                    \"train/mean_text_accuracy\": text_accuracy_sum / total_roved_steps,\n\n                                    \"train/learning_rate\": self.scheduler(current_step).tolist(),\n                                    \"train/step\": current_step,\n                                    \"train/step_time\": ttl_time,\n                                    \"train/perplexity\": jnp.exp(loss).tolist(),\n                                    \"train/trained_tokens\": trained_tokens,\n                                    \"train/epoch\": epoch,\n                                }\n\n                                log_metrics = copy.deepcopy(train_metrics)\n                                train_metrics.update(\n                                    **self.arguments.captured_memory\n                                )\n                                if self.wandb_runtime is not None:\n                                    self.wandb_runtime.log(\n                                        train_metrics\n                                    )\n\n                            pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in log_metrics.items()})\n                            if self.arguments.training_time is not None:\n                                if time.time() - start_time &gt; self.arguments.training_time:\n                                    raise EasyDeLTimerError(\"Time Out\")\n                        else:\n                            break\n                        if self.arguments.save_steps is not None and current_step % self.arguments.save_steps == 0:\n                            if self.rapture is None:\n                                filename = self._save_state(\n                                    state=sharded_state,\n                                    gather_fns=gather_fns,\n                                    milestone=True\n                                )\n                                checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n                            else:\n                                print(\n                                    termcolor.colored(\n                                        \"Info : \", color=\"red\", force_color=True\n                                    ),\n                                    termcolor.colored(\n                                        \"You can not use `save_steps` while using LoRA \"\n                                        \"right now. this action will be skipped\", color=\"white\", force_color=True\n                                    )\n                                )\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n\n            except EasyDeLTimerError:\n                termcolor.cprint(\n                    \"Training reached out maximum training Time Killing training Process \"\n                    \"and Will return Current State of the Model with Parameters.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n            if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n                print(\n                    termcolor.colored(\n                        \"Info : \", color=\"red\", force_color=True\n                    ),\n                    termcolor.colored(\n                        \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                    )\n                )\n                sharded_state = sharded_state.replace(\n                    params=self.rapture.merge_parameters(sharded_state.params)\n                )\n            output = VisionCausalLMTrainerOutput(\n                state=sharded_state,\n                mesh=self.mesh,\n                shard_fns=shard_fns,\n                gather_fns=gather_fns,\n                checkpoint_manager=self.checkpoint_manager,\n            )\n            if self.arguments.save_steps is None and self.arguments.do_last_save:\n                shard_fns, gather_fns = make_shard_and_gather_fns(\n                    match_partition_rules(\n                        self.config.get_partition_rules(\n                            fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                        jax.eval_shape(lambda: sharded_state)\n                    ),\n                    dtype_specs=self.dtype\n                )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n                # crashing errors and saving errors\n                filename = self._save_state(\n                    state=sharded_state,\n                    gather_fns=gather_fns\n                )\n                checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n            if self.arguments.do_eval:\n                for _ in self.eval(\n                        sharded_state\n                ):\n                    ...\n\n            output.checkpoint_path = checkpoint_path\n            output.last_save_file_name = filename\n            self.finish()\n\n            return output\n\n    def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n        \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n        assert self.dataloader_eval is not None, \"`dataloader_eval` is required by evaluator function.\"\n        with self.mesh:\n\n            pbar = tqdm(total=self.max_evaluation_steps)\n            pbar.set_description(\"Evaluating\")\n            current_step = 0\n            loss_sum = None\n            vision_loss_sum = None\n            vision_accuracy_sum = None\n            text_loss_sum = None\n            text_accuracy_sum = None\n\n            try:\n                for batch in self.dataloader_eval:\n                    current_step += 1\n                    time_start = time.time()\n                    for key in self.arguments.ids_to_pop_from_dataset:\n                        _ = batch.pop(key, None)\n\n                    metrics: tuple[chex.Array, VisionCausalLanguageModelStepOutput] = self.sharded_eval_step_function(\n                        model_state,\n                        batch\n                    )\n                    total_time = time.time() - time_start\n                    (\n                        loss, information_and_accuracies\n                    ) = metrics\n\n                    vision_loss = information_and_accuracies.vision_loss\n                    vision_accuracy = information_and_accuracies.vision_accuracy\n                    text_loss = information_and_accuracies.text_loss\n                    text_accuracy = information_and_accuracies.text_accuracy\n\n                    loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                    vision_accuracy_sum = vision_accuracy.tolist() if vision_accuracy_sum is None else (\n                            vision_accuracy_sum + vision_accuracy\n                    )\n                    vision_loss_sum = vision_loss.tolist() if vision_loss_sum is None else vision_loss_sum + vision_loss\n                    text_loss_sum = text_loss.tolist() if text_loss_sum is None else text_loss_sum + text_loss\n                    text_accuracy_sum = text_accuracy.tolist() if text_accuracy_sum is None else (\n                            text_accuracy_sum + text_accuracy\n                    )\n\n                    total_roved_steps = (current_step - self.arguments.step_start_point)\n\n                    eval_metrics = {\n                        \"eval/loss\": loss.tolist(),\n                        \"eval/mean_loss\": loss_sum / total_roved_steps,\n\n                        \"eval/vision_accuracy\": vision_accuracy,\n                        \"eval/vision_loss\": vision_loss,\n                        \"eval/text_loss\": text_loss,\n                        \"eval/text_accuracy\": text_accuracy,\n\n                        \"eval/mean_vision_accuracy\": vision_accuracy_sum / total_roved_steps,\n                        \"eval/mean_vision_loss\": vision_loss_sum / total_roved_steps,\n                        \"eval/mean_text_loss\": text_loss_sum / total_roved_steps,\n                        \"eval/mean_text_accuracy\": text_accuracy_sum / total_roved_steps,\n\n                        \"eval/step\": current_step,\n                        \"eval/step_time\": total_time,\n                        \"eval/perplexity\": jnp.exp(loss).tolist(),\n                    }\n                    log_metrics = copy.deepcopy(eval_metrics)\n                    eval_metrics.update(**self.arguments.captured_memory)\n                    pbar.update(1)\n                    pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                    yield eval_metrics\n            except KeyboardInterrupt:\n                termcolor.cprint(\n                    \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                    color=\"cyan\",\n                    force_color=True\n                )\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/#src.python.easydel.trainer.vision_causal_language_model_trainer.vision_causal_language_model_trainer.VisionCausalLanguageModelTrainer.configure_functions","title":"<code>configure_functions()</code>","text":"<p>The configure_functions function is responsible for configuring the functions that will be used in training. It does this by first defining a function called function_configurations, which initializes the model parameters and returns them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate on a batch of data, including:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <p>Returns:</p> Type Description <code>TrainerConfigureFunctionFuncOutput</code> <p>A TrainerConfigureFunctionFuncOutput object</p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/vision_causal_language_model_trainer.py</code> <pre><code>def configure_functions(self) -&gt; TrainerConfigureFunctionFuncOutput:\n    \"\"\"The configure_functions function is responsible for configuring the functions that will be used in training.\n    It does this by first defining a function called function_configurations, which initializes the model parameters and returns\n    them as a EasyDeLState object. The EasyDeLState object contains all the information needed to train or evaluate\n    on a batch of data, including:\n\n    Args:\n        self: Access the class attributes\n\n    Returns:\n        A TrainerConfigureFunctionFuncOutput object\n    \"\"\"\n\n    def initialize_state_function():\n        initialized_parameters = self.model.init_weights(\n            jax.random.PRNGKey(0),\n            self.arguments.init_input_shape\n        )\n\n        if self.arguments.dtype == jnp.bfloat16:\n            initialized_parameters = self.model.to_bf16(initialized_parameters)\n        elif self.arguments.dtype == jnp.float16:\n            initialized_parameters = self.model.to_fp16(initialized_parameters)\n\n        tx = self.tx\n        parameters = flax.core.freeze({\"params\": initialized_parameters})\n        tx_init = copy.deepcopy(self.arguments.optimizer_kwargs)\n\n        if self.rapture is not None:\n            lora_parameters = self.lora_parameters\n            if self.arguments.dtype == jnp.bfloat16:\n                lora_parameters = self.model.to_bf16(lora_parameters)\n            elif self.arguments.dtype == jnp.float16:\n                lora_parameters = self.model.to_fp16(lora_parameters)\n\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=lora_parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(tx_init),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n        else:\n            return EasyDeLState.create(\n                tx=tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=tx_init,\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n\n    def create_state_from_params_function(parameters):\n        if self.rapture is None:\n            return EasyDeLState.create(\n                tx=self.tx,\n                params=parameters,\n                apply_fn=self.model.__call__,\n                module_config=copy.deepcopy(self.model.config),\n                tx_init=copy.deepcopy(self.arguments.optimizer_kwargs),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.model,\n                module_config_args=None\n            )\n        else:\n            return EasyDeLState(\n                step=0,\n                apply_fn=self.lora_apply_fn,\n                params=parameters,\n                tx=self.lora_tx,\n                opt_state=self.lora_opt_state,\n                tx_init=EasyDeLState.safe_dict(copy.deepcopy(self.arguments.optimizer_kwargs)),\n                hyperparameters=EasyDeLState.create_hyperparameters(self.model.config.model_type),\n                module=self.lora_model,\n                module_config=self.model.config,\n                module_config_args=None,\n            )\n\n    state_shape = jax.eval_shape(initialize_state_function)\n    state_partition_spec = match_partition_rules(\n        self.config.get_partition_rules(\n            fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n        state_shape\n    )\n    create_sharded_state_from_params_function = pjit(\n        create_state_from_params_function,\n        in_shardings=(state_partition_spec.params,),\n        out_shardings=state_partition_spec,\n        donate_argnums=(0,)\n    )\n    sharded_train_step_function = pjit(\n        create_vision_casual_language_model_train_step(self.arguments.step_partition_spec),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(state_partition_spec, PartitionSpec(), PartitionSpec()),\n        donate_argnums=(0, 0),\n    )\n\n    sharded_eval_step_function = pjit(\n        create_vision_casual_language_model_evaluation_step(self.arguments.step_partition_spec),\n        in_shardings=(state_partition_spec, PartitionSpec()),\n        out_shardings=(PartitionSpec(), PartitionSpec()),\n        donate_argnums=(0, 0),\n    )\n\n    mesh = self.arguments.get_mesh()\n    self.arguments.ckpt_path_exists()\n    checkpoint_manager = self.arguments.get_streaming_checkpointer()\n    self.state_partition_spec = state_partition_spec\n    self.state_shape = state_shape\n\n    return TrainerConfigureFunctionFuncOutput(\n        create_sharded_state_from_params_function=create_sharded_state_from_params_function,\n        sharded_train_step_function=sharded_train_step_function,\n        sharded_eval_step_function=sharded_eval_step_function,\n        mesh=mesh,\n        checkpoint_manager=checkpoint_manager,\n        initialize_state_function=initialize_state_function\n    )\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/#src.python.easydel.trainer.vision_causal_language_model_trainer.vision_causal_language_model_trainer.VisionCausalLanguageModelTrainer.eval","title":"<code>eval(model_state)</code>","text":"<p>Evaluate the Given Model State and yield the eval metrics</p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/vision_causal_language_model_trainer.py</code> <pre><code>def eval(self, model_state: EasyDeLState) -&gt; typing.Iterator[dict]:\n    \"\"\"Evaluate the Given Model State and yield the eval metrics\"\"\"\n    assert self.dataloader_eval is not None, \"`dataloader_eval` is required by evaluator function.\"\n    with self.mesh:\n\n        pbar = tqdm(total=self.max_evaluation_steps)\n        pbar.set_description(\"Evaluating\")\n        current_step = 0\n        loss_sum = None\n        vision_loss_sum = None\n        vision_accuracy_sum = None\n        text_loss_sum = None\n        text_accuracy_sum = None\n\n        try:\n            for batch in self.dataloader_eval:\n                current_step += 1\n                time_start = time.time()\n                for key in self.arguments.ids_to_pop_from_dataset:\n                    _ = batch.pop(key, None)\n\n                metrics: tuple[chex.Array, VisionCausalLanguageModelStepOutput] = self.sharded_eval_step_function(\n                    model_state,\n                    batch\n                )\n                total_time = time.time() - time_start\n                (\n                    loss, information_and_accuracies\n                ) = metrics\n\n                vision_loss = information_and_accuracies.vision_loss\n                vision_accuracy = information_and_accuracies.vision_accuracy\n                text_loss = information_and_accuracies.text_loss\n                text_accuracy = information_and_accuracies.text_accuracy\n\n                loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                vision_accuracy_sum = vision_accuracy.tolist() if vision_accuracy_sum is None else (\n                        vision_accuracy_sum + vision_accuracy\n                )\n                vision_loss_sum = vision_loss.tolist() if vision_loss_sum is None else vision_loss_sum + vision_loss\n                text_loss_sum = text_loss.tolist() if text_loss_sum is None else text_loss_sum + text_loss\n                text_accuracy_sum = text_accuracy.tolist() if text_accuracy_sum is None else (\n                        text_accuracy_sum + text_accuracy\n                )\n\n                total_roved_steps = (current_step - self.arguments.step_start_point)\n\n                eval_metrics = {\n                    \"eval/loss\": loss.tolist(),\n                    \"eval/mean_loss\": loss_sum / total_roved_steps,\n\n                    \"eval/vision_accuracy\": vision_accuracy,\n                    \"eval/vision_loss\": vision_loss,\n                    \"eval/text_loss\": text_loss,\n                    \"eval/text_accuracy\": text_accuracy,\n\n                    \"eval/mean_vision_accuracy\": vision_accuracy_sum / total_roved_steps,\n                    \"eval/mean_vision_loss\": vision_loss_sum / total_roved_steps,\n                    \"eval/mean_text_loss\": text_loss_sum / total_roved_steps,\n                    \"eval/mean_text_accuracy\": text_accuracy_sum / total_roved_steps,\n\n                    \"eval/step\": current_step,\n                    \"eval/step_time\": total_time,\n                    \"eval/perplexity\": jnp.exp(loss).tolist(),\n                }\n                log_metrics = copy.deepcopy(eval_metrics)\n                eval_metrics.update(**self.arguments.captured_memory)\n                pbar.update(1)\n                pbar.set_postfix(**{k.replace(\"eval/\", \"\"): v for k, v in log_metrics.items()})\n                yield eval_metrics\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At Evaluation model Will return Nothing and just pass.\",\n                color=\"cyan\",\n                force_color=True\n            )\n</code></pre>"},{"location":"generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/#src.python.easydel.trainer.vision_causal_language_model_trainer.vision_causal_language_model_trainer.VisionCausalLanguageModelTrainer.train","title":"<code>train(model_parameters=None, state=None)</code>","text":"<p>The train function is the main function of this module. It takes a model_parameters argument which can be used to load a pretrained model and finetune it. The train function returns an TrainerOutput object that contains the last saved file name, predict func, train state, mesh and checkpoint streamer.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the class methods aware of other methods and attributes within the class</p> required <code>model_parameters</code> <code>Optional[FrozenDict]</code> <p>flax.core.FrozenDict: Load a pre-trained model</p> <code>None</code> <code>state</code> <code>Optional[EasyDeLState]</code> <p>Optional[EasyDeLState]: Ready to Use State</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionCausalLMTrainerOutput</code> <p>An object of type \"TrainerOutput\"</p> Source code in <code>src/python/easydel/trainer/vision_causal_language_model_trainer/vision_causal_language_model_trainer.py</code> <pre><code>def train(\n        self,\n        model_parameters: Optional[flax.core.FrozenDict] = None,\n        state: Optional[EasyDeLState] = None\n) -&gt; VisionCausalLMTrainerOutput:\n    \"\"\"The train function is the main function of this module.\n    It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n    The train function returns an TrainerOutput object that contains the last saved file name, predict func,\n    train state, mesh and checkpoint streamer.\n\n    Args:\n        self: Make the class methods aware of other methods and\n            attributes within the class\n        model_parameters: flax.core.FrozenDict: Load a pre-trained\n            model\n        state: Optional[EasyDeLState]: Ready to Use State\n\n    Returns:\n        An object of type \"TrainerOutput\"\n    \"\"\"\n\n    def count_model_parameters(_p):\n        termcolor.cprint(\n            f\"Model Contain {sum(n.size for n in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9} \"\n            f\"Billion Parameters\",\n            color=\"red\", force_color=True\n        )\n\n    checkpoint_path = \"SAVING_SKIPPED\"\n    start_time = time.time()\n    sharded_state, shard_fns, gather_fns = self.initialize_state(\n        model_parameters=model_parameters,\n        state=state\n    )\n\n    count_model_parameters(sharded_state.params)\n    with self.mesh:\n        pbar = tqdm(total=self.max_training_steps)\n        current_step = int(jax.device_get(sharded_state.step))\n\n        loss_sum = None\n        vision_loss_sum = None\n        vision_accuracy_sum = None\n        text_loss_sum = None\n        text_accuracy_sum = None\n        pbar.update(sharded_state.step.tolist())  # type: ignore\n        learning_rates = []\n        if self.wandb_runtime is not None:\n            model_parameters_number = sum(\n                n.size for n in\n                jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_state.params))[0]\n            ) / 1e9\n            self.wandb_runtime.log(\n                {\n                    \"Number of Model Parameters (Billion)\": model_parameters_number\n                }\n            )\n            wandb.summary[\"Number of Model Parameters (Billion)\"] = model_parameters_number\n        try:\n            for epoch in range(self.arguments.num_train_epochs):\n                for batch in self.dataloader_train:\n                    current_step += 1\n                    if (\n                            self.arguments.step_start_point is not None\n                            and\n                            self.arguments.step_start_point &gt; current_step\n                    ):\n                        pbar.update(1)\n                    elif current_step &lt; self.max_training_steps:\n\n                        for ssb in self.arguments.ids_to_pop_from_dataset:\n                            _ = batch.pop(ssb, None)\n                        time_s = time.time()\n                        outputs_and_metrics: tuple[\n                            EasyDeLState, chex.Array, VisionCausalLanguageModelStepOutput\n                        ] = self.sharded_train_step_function(\n                            sharded_state,\n                            batch\n                        )\n                        sharded_state, loss, information_and_accuracies = outputs_and_metrics\n                        ttl_time = time.time() - time_s\n                        loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                        vision_loss = information_and_accuracies.vision_loss\n                        vision_accuracy = information_and_accuracies.vision_accuracy\n                        text_loss = information_and_accuracies.text_loss\n                        text_accuracy = information_and_accuracies.text_accuracy\n\n                        loss_sum = loss.tolist() if loss_sum is None else loss_sum + loss\n                        vision_accuracy_sum = vision_accuracy.tolist() if vision_accuracy_sum is None else (\n                                vision_accuracy_sum + vision_accuracy\n                        )\n                        vision_loss_sum = vision_loss.tolist() if vision_loss_sum is None else (\n                                vision_loss_sum + vision_loss\n                        )\n                        text_loss_sum = text_loss.tolist() if text_loss_sum is None else text_loss_sum + text_loss\n                        text_accuracy_sum = text_accuracy.tolist() if text_accuracy_sum is None else (\n                                text_accuracy_sum + text_accuracy\n                        )\n                        learning_rates.append(self.scheduler(current_step).tolist())\n                        pbar.update(1)\n\n                        trained_tokens = jnp.multiply(\n                            self.arguments.max_sequence_length, jnp.multiply(\n                                current_step,\n                                self.arguments.total_batch_size\n                            )\n                        )\n\n                        total_roved_steps = (current_step - self.arguments.step_start_point)\n\n                        with jax.spmd_mode(\"allow_all\"):\n                            train_metrics = {\n\n                                \"train/loss\": loss.tolist(),\n                                \"train/mean_loss\": loss_sum / total_roved_steps,\n\n                                \"train/vision_accuracy\": vision_accuracy,\n                                \"train/vision_loss\": vision_loss,\n                                \"train/text_loss\": text_loss,\n                                \"train/text_accuracy\": text_accuracy,\n\n                                \"train/mean_vision_accuracy\": vision_accuracy_sum / total_roved_steps,\n                                \"train/mean_vision_loss\": vision_loss_sum / total_roved_steps,\n                                \"train/mean_text_loss\": text_loss_sum / total_roved_steps,\n                                \"train/mean_text_accuracy\": text_accuracy_sum / total_roved_steps,\n\n                                \"train/learning_rate\": self.scheduler(current_step).tolist(),\n                                \"train/step\": current_step,\n                                \"train/step_time\": ttl_time,\n                                \"train/perplexity\": jnp.exp(loss).tolist(),\n                                \"train/trained_tokens\": trained_tokens,\n                                \"train/epoch\": epoch,\n                            }\n\n                            log_metrics = copy.deepcopy(train_metrics)\n                            train_metrics.update(\n                                **self.arguments.captured_memory\n                            )\n                            if self.wandb_runtime is not None:\n                                self.wandb_runtime.log(\n                                    train_metrics\n                                )\n\n                        pbar.set_postfix(**{k.replace(\"train/\", \"\"): v for k, v in log_metrics.items()})\n                        if self.arguments.training_time is not None:\n                            if time.time() - start_time &gt; self.arguments.training_time:\n                                raise EasyDeLTimerError(\"Time Out\")\n                    else:\n                        break\n                    if self.arguments.save_steps is not None and current_step % self.arguments.save_steps == 0:\n                        if self.rapture is None:\n                            filename = self._save_state(\n                                state=sharded_state,\n                                gather_fns=gather_fns,\n                                milestone=True\n                            )\n                            checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n                        else:\n                            print(\n                                termcolor.colored(\n                                    \"Info : \", color=\"red\", force_color=True\n                                ),\n                                termcolor.colored(\n                                    \"You can not use `save_steps` while using LoRA \"\n                                    \"right now. this action will be skipped\", color=\"white\", force_color=True\n                                )\n                            )\n        except KeyboardInterrupt:\n            termcolor.cprint(\n                \"KeyboardInterrupt At training model Will return Current State of the Model with Parameters.\",\n                color=\"cyan\",\n                force_color=True\n            )\n\n        except EasyDeLTimerError:\n            termcolor.cprint(\n                \"Training reached out maximum training Time Killing training Process \"\n                \"and Will return Current State of the Model with Parameters.\",\n                color=\"cyan\",\n                force_color=True\n            )\n        if self.arguments.merge_lora_rapture_parameters and self.rapture is not None:\n            print(\n                termcolor.colored(\n                    \"Info : \", color=\"red\", force_color=True\n                ),\n                termcolor.colored(\n                    \"Merging LoRA Parameters.\", color=\"white\", force_color=True\n                )\n            )\n            sharded_state = sharded_state.replace(\n                params=self.rapture.merge_parameters(sharded_state.params)\n            )\n        output = VisionCausalLMTrainerOutput(\n            state=sharded_state,\n            mesh=self.mesh,\n            shard_fns=shard_fns,\n            gather_fns=gather_fns,\n            checkpoint_manager=self.checkpoint_manager,\n        )\n        if self.arguments.save_steps is None and self.arguments.do_last_save:\n            shard_fns, gather_fns = make_shard_and_gather_fns(\n                match_partition_rules(\n                    self.config.get_partition_rules(\n                        fully_sharded_data_parallel=self.arguments.fully_sharded_data_parallel\n                    ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n                    jax.eval_shape(lambda: sharded_state)\n                ),\n                dtype_specs=self.dtype\n            )  # You have to re-init the new shard and gather functions in order to be able to skip LoRA weight\n            # crashing errors and saving errors\n            filename = self._save_state(\n                state=sharded_state,\n                gather_fns=gather_fns\n            )\n            checkpoint_path = f\"{str(self.arguments.get_path())}/{filename}\"\n\n        if self.arguments.do_eval:\n            for _ in self.eval(\n                    sharded_state\n            ):\n                ...\n\n        output.checkpoint_path = checkpoint_path\n        output.last_save_file_name = filename\n        self.finish()\n\n        return output\n</code></pre>"},{"location":"generated-transform-easydel_transform/","title":"transform.easydel_transform","text":""},{"location":"generated-transform-easydel_transform/#src.python.easydel.transform.easydel_transform.float_tensor_to_dtype","title":"<code>float_tensor_to_dtype(tensor, dtype)</code>","text":"<p>The float_tensor_to_dtype function is used to convert a tensor's dtype to the specified dtype.</p> <p>:param tensor: Convert the tensor to a float dtype :param dtype: Convert the tensor to a specific dtype :return: A tensor with the specified dtype</p> Source code in <code>src/python/easydel/transform/easydel_transform.py</code> <pre><code>def float_tensor_to_dtype(tensor, dtype):\n    \"\"\"\n    The float_tensor_to_dtype function is used to convert a tensor's dtype to the specified dtype.\n\n    :param tensor: Convert the tensor to a float dtype\n    :param dtype: Convert the tensor to a specific dtype\n    :return: A tensor with the specified dtype\n\n    \"\"\"\n    if dtype is None or dtype == \"\":\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_dtype(dtype)\n    float_dtypes = (jax.numpy.bfloat16, jax.numpy.float16, jax.numpy.float32, jax.numpy.float64)\n    if getattr(tensor, \"dtype\", None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor\n</code></pre>"},{"location":"generated-transform-easydel_transform/#src.python.easydel.transform.easydel_transform.huggingface_to_easydel","title":"<code>huggingface_to_easydel(state_dict, *, device, embedding_layer_names=None, layer_norm_names=None, shard_fns=None, convert_to_8bit=False, params_pattern_selection=None, dtype=jax.numpy.float16, rnn_based_or_rwkv=False, verbose=True, remove_state_dict=False, **kwargs)</code>","text":"<p>The huggingface_to_easydel function takes a huggingface model's state_dict and converts it to an easydel model's flax_dict. The function is designed to be used in conjunction with the load_huggingface function, which loads a huggingface model from disk. The embedding layer name must be specified as well as the device on which the conversion will take place.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <p>Load the weights from a huggingface model</p> required <code>embedding_layer_names</code> <code>Optional[List[str]]</code> <p>List[str]: Identify the embedding layer in the huggingface model</p> <code>None</code> <code>device</code> <p>Determine which device the model will be loaded on</p> required <code>layer_norm_names</code> <code>Optional[List[str]]</code> <p>Replaces weight or kernel with (scale)</p> <code>None</code> <code>shard_fns</code> <code>Optional[Mapping[tuple, Callable]]</code> <p>Optional[Mapping[tuple, Callable]]: Sharding Function to be used to shard model</p> <code>None</code> <code>convert_to_8bit</code> <code>bool</code> <p>bool: whenever to convert the into 8bit format</p> <code>False</code> <code>params_pattern_selection</code> <code>Optional[Pattern]</code> <p>Optional[re.Pattern]: patter to use to find the parameters of the model which will</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the data type of the tensors</p> <code>float16</code> <code>rnn_based_or_rwkv</code> <code>bool</code> <p>bool: rnn_based_or_rwkv is a conditioner which decide whenever it finds a value in tree</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>bool: whenever to log sharding or converting process</p> <code>True</code> <code>remove_state_dict</code> <code>bool</code> <p>bool : whether to remove state dict during the transforming process</p> <code>False</code> <p>be converted to 8bit format. that start with time_mix_ it will automatically reshape that for easydel use case</p> <p>Returns:</p> Type Description <p>A dictionary of the weights and biases in a format that can be</p> <p>used by flax (it's an UnFlattenDict)</p> Source code in <code>src/python/easydel/transform/easydel_transform.py</code> <pre><code>def huggingface_to_easydel(\n        state_dict,\n        *,\n        device,\n        embedding_layer_names: Optional[List[str]] = None,\n        layer_norm_names: Optional[List[str]] = None,\n        shard_fns: Optional[Mapping[tuple, Callable]] = None,\n        convert_to_8bit: bool = False,\n        params_pattern_selection: Optional[re.Pattern] = None,\n        dtype: jax.numpy.dtype = jax.numpy.float16,\n        rnn_based_or_rwkv: bool = False,\n        verbose: bool = True,\n        remove_state_dict: bool = False,\n        **kwargs\n):\n    \"\"\"The huggingface_to_easydel function takes a huggingface model's state_dict and converts it to an easydel\n    model's flax_dict. The function is designed to be used in conjunction with the load_huggingface function, which\n    loads a huggingface model from disk. The embedding layer name must be specified as well as the device on which\n    the conversion will take place.\n\n    Args:\n        state_dict: Load the weights from a huggingface model\n        embedding_layer_names: List[str]: Identify the embedding layer\n            in the huggingface model\n        device: Determine which device the model will be loaded on\n        layer_norm_names: Replaces weight or kernel with (scale)\n        shard_fns: Optional[Mapping[tuple, Callable]]: Sharding Function\n            to be used to shard model\n        convert_to_8bit: bool: whenever to convert the into 8bit format\n        params_pattern_selection: Optional[re.Pattern]: patter to use to\n            find the parameters of the model which will\n        dtype: jax.numpy.dtype: Specify the data type of the tensors\n        rnn_based_or_rwkv: bool: rnn_based_or_rwkv is a conditioner\n            which decide whenever it finds a value in tree\n        verbose: bool: whenever to log sharding or converting process\n        remove_state_dict: bool : whether to remove state dict during\n            the transforming process\n    be converted to 8bit format.\n    that start with time_mix_ it will automatically reshape that for easydel use case\n\n    Returns:\n        A dictionary of the weights and biases in a format that can be\n        used by flax (it's an UnFlattenDict)\n    \"\"\"\n    embedding_layer_names = set(embedding_layer_names or [])\n    layer_norm_names = set(layer_norm_names or [])\n    _l = len(\".weight\")\n    _b = len(\".bias\")\n\n    if convert_to_8bit:\n        assert params_pattern_selection is not None, (\n            \"in case of converting parameters to 8bit you should pass \"\n            \"`params_pattern_selection` too, to tell the quantizer which parameters should be quantized.\"\n        )\n\n    with jax.default_device(device):\n        flax_dict = {}\n        pbar = tqdm(total=len(state_dict), disable=not verbose)\n\n        pbar.set_description(\"Converting Model\")\n\n        for key in list(state_dict.keys()):\n            # Determine if renaming is necessary\n            tensor = state_dict.pop(key)\n            new_key = key\n            if any(layer_name in key for layer_name in embedding_layer_names):\n                new_key = key[:-_l] + \".embedding\"\n            elif rnn_based_or_rwkv and (\"time_mix_\" in key or \"time_\" in key):\n                tensor = tensor.reshape(-1)\n            elif any(layer_norm in key for layer_norm in layer_norm_names):\n                new_key = key.replace(\".weight\", \".scale\")\n            elif \"weight\" in key:\n                if len(tensor.shape) == 2:\n                    tensor = tensor.transpose(0, 1)\n                new_key = key.replace(\".weight\", \".kernel\")\n\n            key_tuple = tuple(new_key.split(\".\"))\n            # Convert tensor to jax.numpy.array without detaching and moving to CPU\n            array = jax.lax.convert_element_type(jnp.asarray(tensor.cpu().detach().numpy()), dtype)\n            if remove_state_dict:\n                del tensor\n                gc.collect()\n            # Apply sharding functions if provided\n            if shard_fns and key_tuple in shard_fns:\n                array = shard_fns[key_tuple](array)\n            if convert_to_8bit:\n                if params_pattern_selection.search(\"/\".join(key_tuple)):\n                    array = fjformer.linen.linen.LinearBitKernel(\n                        *fjformer.linen.linen.quantize(array, int_dtype=jnp.int8)  # type: ignore\n                    )\n            flax_dict[key_tuple] = array\n\n            # Update progress bar less frequently to reduce overhead\n            pbar.update(1)\n        pbar.close()\n        gc.collect()\n        return traverse_util.unflatten_dict(flax_dict)\n</code></pre>"},{"location":"generated-transform-easydel_transform/#src.python.easydel.transform.easydel_transform.match_keywords","title":"<code>match_keywords(string, ts, ns)</code>","text":"<p>The match_keywords function takes a string, and two lists of strings. The first list is the \"must-have\" keywords, and the second list is the \"not-allowed\" keywords. It returns True if all the must-have keywords are in string, but none of not allowed are in it.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <p>Pass in the text that is being searched</p> required <code>ts</code> <p>Specify the required keywords and ns is used to specify the non-required keywords</p> required <code>ns</code> <p>Specify a list of negative keywords</p> required <p>Returns:</p> Type Description <p>True if all the keywords in ts are present and none of the</p> Source code in <code>src/python/easydel/transform/easydel_transform.py</code> <pre><code>def match_keywords(string, ts, ns):\n    \"\"\"The match_keywords function takes a string, and two lists of strings.\n    The first list is the &amp;quot;must-have&amp;quot; keywords, and the second list is the &amp;quot;not-allowed&amp;quot; keywords.\n    It returns True if all the must-have keywords are in string, but none of not allowed are in it.\n\n    Args:\n        string: Pass in the text that is being searched\n        ts: Specify the required keywords and ns is used to specify the\n            non-required keywords\n        ns: Specify a list of negative keywords\n\n    Returns:\n        True if all the keywords in ts are present and none of the\n    \"\"\"\n    for t in ts:\n        if t not in string:\n            return False\n    for n in ns:\n        if n in string:\n            return False\n    return True\n</code></pre>"},{"location":"generated-transform-easydel_transform/#src.python.easydel.transform.easydel_transform.read_ckpt","title":"<code>read_ckpt(path, shard_fns=None, add_extra_past_fix=None)</code>","text":"<p>The read_ckpt function reads a checkpoint file and returns the tensors in it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>[str, PathLike]</code> <p>[str, os.PathLike]: Specify the path to the checkpoint file</p> required <code>shard_fns</code> <p>Shard the tensors</p> <code>None</code> <code>add_extra_past_fix</code> <code>list</code> <p>list: Add an extra past to the key</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of tensors</p> Source code in <code>src/python/easydel/transform/easydel_transform.py</code> <pre><code>def read_ckpt(path: [str, os.PathLike], shard_fns=None, add_extra_past_fix: list = None):\n    \"\"\"The read_ckpt function reads a checkpoint file and returns the tensors in it.\n\n    Args:\n        path: [str, os.PathLike]: Specify the path to the checkpoint\n            file\n        shard_fns: Shard the tensors\n        add_extra_past_fix: list: Add an extra past to the key\n\n    Returns:\n        A dictionary of tensors\n    \"\"\"\n    tensors = {}\n    with open(path, \"rb\") as stream:\n        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n        for key, value in unpacker:\n            if add_extra_past_fix is not None:\n                key = add_extra_past_fix + key\n            key = tuple(key)\n            tensor = from_bytes(None, value)\n            if shard_fns is not None:\n                tensor = shard_fns[key](tensor)\n            tensors[key] = tensor\n    return tensors\n</code></pre>"},{"location":"generated-transform-easydel_transform/#src.python.easydel.transform.easydel_transform.save_ckpt","title":"<code>save_ckpt(train_state, path, gather_fns=None, float_dtype=None)</code>","text":"<p>The save_ckpt function saves the state of a training run to disk.</p> <p>Parameters:</p> Name Type Description Default <code>train_state</code> <p>Store the current state of the training process</p> required <code>path</code> <p>Specify the location of the checkpoint file</p> required <code>gather_fns</code> <p>Specify a function that will be used to convert the tensor to bytes</p> <code>None</code> <code>float_dtype</code> <p>Convert the tensor to a specific dtype</p> <code>None</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/python/easydel/transform/easydel_transform.py</code> <pre><code>def save_ckpt(train_state, path, gather_fns=None, float_dtype=None):\n    \"\"\"The save_ckpt function saves the state of a training run to disk.\n\n    Args:\n        train_state: Store the current state of the training process\n        path: Specify the location of the checkpoint file\n        gather_fns: Specify a function that will be used to convert the\n            tensor to bytes\n        float_dtype: Convert the tensor to a specific dtype\n\n    Returns:\n        Nothing\n    \"\"\"\n\n    train_state = to_state_dict(train_state)\n    packer = msgpack.Packer()\n    flatten_train_state = flatten_dict(train_state)\n    if gather_fns is not None:\n        gather_fns = flatten_dict(to_state_dict(gather_fns))\n\n    with open(path, \"wb\") as stream:\n        for key, value in flatten_train_state.items():\n            if gather_fns is not None:\n                value = gather_fns[key](value)\n            value = float_tensor_to_dtype(value, float_dtype)\n            stream.write(packer.pack((key, to_bytes(value))))\n</code></pre>"},{"location":"generated-transform-utils/","title":"transform.utils","text":""},{"location":"generated-utils-checker/","title":"utils.checker","text":""},{"location":"generated-utils-prompters/","title":"utils.prompters","text":""},{"location":"generated-utils-prompters/#src.python.easydel.utils.prompters.antitoxin_prompter","title":"<code>antitoxin_prompter(history, prompt, system=None)</code>","text":"<p>The antitoxin_prompter function takes in a history of user-assistant interactions, a prompt from the user, and optionally a system response. It returns an input string that can be fed into the antitoxin model to generate an assistant response.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Pass in the history of the conversation</p> required <code>prompt</code> <code>str</code> <p>str: Pass the user's input to the assistant</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Pass the system's response to the prompt</p> <code>None</code> <p>:param : Store the history of user and assistant interaction</p> <p>Returns:</p> Type Description <p>A string that contains the user's prompt,</p> Source code in <code>src/python/easydel/utils/prompters.py</code> <pre><code>def antitoxin_prompter(\n        history: typing.List[str],\n        prompt: str,\n        system: typing.Optional[str] = None,\n):\n    \"\"\"The antitoxin_prompter function takes in a history of user-assistant interactions,\n    a prompt from the user, and optionally a system response. It returns an input string\n    that can be fed into the antitoxin model to generate an assistant response.\n\n    Args:\n        history: typing.List[str]: Pass in the history of the\n            conversation\n        prompt: str: Pass the user's input to the assistant\n        system: typing.Optional[str]: Pass the system's response to the\n            prompt\n    :param : Store the history of user and assistant interaction\n\n    Returns:\n        A string that contains the user's prompt,\n    \"\"\"\n    sys_str = f\"&lt;|im_start|&gt;system\\n{system}&lt;|im_end|&gt;\\n\" if system is not None else \"\"\n    histories = \"\"\n    for user, assistance in history:\n        histories += f\"&lt;|im_start|&gt;user\\n{user}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n{assistance}&lt;|im_end|&gt;\\n\"\n    text = f\"&lt;|im_start|&gt;user\\n{prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    return sys_str + histories + text\n</code></pre>"},{"location":"generated-utils-prompters/#src.python.easydel.utils.prompters.antitoxin_prompter_chat_format","title":"<code>antitoxin_prompter_chat_format(history, system=None)</code>","text":"<p>The antitoxin_prompter_chat_format function takes a list of strings and returns a string. The input is the history of the chat, which is a list of tuples where each tuple contains two strings: the user's message and the assistant's response. The output is formatted as follows:</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Pass in the history of user and assistant messages</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Pass in the system message</p> <code>None</code> <p>:param : Store the history of the conversation</p> <p>Returns:</p> Type Description <p>A string that contains the system message and</p> Source code in <code>src/python/easydel/utils/prompters.py</code> <pre><code>def antitoxin_prompter_chat_format(\n        history: typing.List[str],\n        system: typing.Optional[str] = None,\n):\n    \"\"\"The antitoxin_prompter_chat_format function takes a list of strings and returns a string.\n    The input is the history of the chat, which is a list of tuples where each tuple contains two strings:\n    the user's message and the assistant's response. The output is formatted as follows:\n\n    Args:\n        history: typing.List[str]: Pass in the history of user and\n            assistant messages\n        system: typing.Optional[str]: Pass in the system message\n    :param : Store the history of the conversation\n\n    Returns:\n        A string that contains the system message and\n    \"\"\"\n    sys_str = f\"&lt;|im_start|&gt;system\\n{system}&lt;|im_end|&gt;\\n\" if system is not None else \"\"\n    histories = \"\"\n    for user, assistance in history:\n        histories += f\"&lt;|im_start|&gt;user\\n{user}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n{assistance}&lt;|im_end|&gt;\\n\"\n    return sys_str + histories\n</code></pre>"},{"location":"generated-utils-prompters/#src.python.easydel.utils.prompters.llama2_prompter","title":"<code>llama2_prompter(history, prompt, system=None)</code>","text":"<p>The llama2_prompter function takes a history of user-system interactions, a prompt for the next system response, and optionally a system response. It returns an LLAMA2 formatted string that can be used as input to the LLAMA2 model.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Store the history of user input and system response</p> required <code>prompt</code> <code>str</code> <p>str: Specify the prompt to be displayed</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Indicate that the system is optional</p> <code>None</code> <p>:param : Specify the system's response</p> <p>Returns:</p> Type Description <p>A string that is a concatenation of the</p> Source code in <code>src/python/easydel/utils/prompters.py</code> <pre><code>def llama2_prompter(\n        history: typing.List[str],\n        prompt: str,\n        system: typing.Optional[str] = None,\n\n):\n    \"\"\"The llama2_prompter function takes a history of user-system interactions,\n    a prompt for the next system response, and optionally a system response.\n    It returns an LLAMA2 formatted string that can be used as input to the LLAMA2 model.\n\n    Args:\n        history: typing.List[str]: Store the history of user input and\n            system response\n        prompt: str: Specify the prompt to be displayed\n        system: typing.Optional[str]: Indicate that the system is\n            optional\n    :param : Specify the system's response\n\n    Returns:\n        A string that is a concatenation of the\n    \"\"\"\n    do_strip = False\n    if system is not None:\n        texts = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n    else:\n        texts = [f'&lt;s&gt;[INST] ']\n    for user_input, response in history:\n        user_input = user_input.strip() if do_strip else user_input\n        do_strip = True\n        texts.append(f'{user_input} [/INST] {response.strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n    prompt = prompt.strip() if do_strip else prompt\n    texts.append(f'{prompt} [/INST]')\n    return \"\".join(texts)\n</code></pre>"},{"location":"generated-utils-prompters/#src.python.easydel.utils.prompters.llama2_prompter_chat_format","title":"<code>llama2_prompter_chat_format(system, messages)</code>","text":"<p>The llama2_prompter_chat_format function takes a system message and a list of messages, and returns the formatted string that can be used to create an LLAMA2 chat file. The system message is optional, and if it is not provided then the function will return only the user messages. The user messages are expected to be in pairs: one for each speaker (system or human).  The first element of each  pair should be the name of that speaker.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>str: Store the system message</p> required <code>messages</code> <code>List[str]</code> <p>typing.List[str]: Pass in a list of strings</p> required <p>:param : Add the system message to the beginning of the chat</p> <p>Returns:</p> Type Description <p>A string that is the</p> Source code in <code>src/python/easydel/utils/prompters.py</code> <pre><code>def llama2_prompter_chat_format(\n        system: str,\n        messages: typing.List[str],\n):\n    \"\"\"The llama2_prompter_chat_format function takes a system message and a list of messages,\n    and returns the formatted string that can be used to create an LLAMA2 chat file.\n    The system message is optional, and if it is not provided then the function will return only the user messages.\n    The user messages are expected to be in pairs: one for each speaker (system or human).  The first element of each\n     pair should be the name of that speaker.\n\n    Args:\n        system: str: Store the system message\n        messages: typing.List[str]: Pass in a list of strings\n    :param : Add the system message to the beginning of the chat\n\n    Returns:\n        A string that is the\n    \"\"\"\n    if system is not None:\n        string = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n    else:\n        string = [f'&lt;s&gt;[INST] ']\n    for index in range(0, len(messages), 2):\n        string.append(\n            f'{messages[index]} [/INST] {messages[index + 1].strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n    return \"\".join(string).strip()\n</code></pre>"},{"location":"generated-utils-tensor_utils/","title":"utils.tensor_utils","text":""},{"location":"generated-utils-tensor_utils/#src.python.easydel.utils.tensor_utils.np2jax","title":"<code>np2jax(array)</code>","text":"<p>Convert Numpy Array to JAX Array</p> Source code in <code>src/python/easydel/utils/tensor_utils.py</code> <pre><code>def np2jax(array: np.array) -&gt; chex.Array:\n    \"\"\"Convert Numpy Array to JAX Array\"\"\"\n    return jnp.asarray(array)\n</code></pre>"},{"location":"generated-utils-tensor_utils/#src.python.easydel.utils.tensor_utils.pt2jax","title":"<code>pt2jax(array)</code>","text":"<p>Convert Pytorch Array to JAX Array</p> Source code in <code>src/python/easydel/utils/tensor_utils.py</code> <pre><code>def pt2jax(array: torch.Tensor) -&gt; chex.Array:\n    \"\"\"Convert Pytorch Array to JAX Array\"\"\"\n    return np2jax(pt2np(array))\n</code></pre>"},{"location":"generated-utils-tensor_utils/#src.python.easydel.utils.tensor_utils.pt2np","title":"<code>pt2np(array)</code>","text":"<p>Convert Pytorch Array to Numpy Array</p> Source code in <code>src/python/easydel/utils/tensor_utils.py</code> <pre><code>def pt2np(array: torch.Tensor) -&gt; np.array:\n    \"\"\"Convert Pytorch Array to Numpy Array\"\"\"\n    return array.detach().cpu().numpy()\n</code></pre>"},{"location":"generated-utils-utils/","title":"utils.utils","text":""},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer","title":"<code>Timer</code>","text":"Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>class Timer:\n\n    def __init__(self, name):\n        \"\"\"The __init__ function is called when the class is instantiated.\n        It sets up the object with a name and initializes other variables.\n\n        Args:\n            self: Represent the instance of the class\n            name: Give the timer a name\n\n        Returns:\n            An instance of the class\n        \"\"\"\n        self.name_ = name\n        self.elapsed_ = 0.0\n        self.started_ = False\n        self.start_time = time.time()\n\n    def start(self):\n        \"\"\"The start function starts the timer.\n                Args:\n                    None\n\n        Args:\n            self: Access the attributes and methods of the class in\n                python\n\n        Returns:\n            Nothing\n        \"\"\"\n        assert not self.started_, \"timer has already been started\"\n        self.start_time = time.time()\n        self.started_ = True\n\n    def stop(self):\n        \"\"\"The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            The time elapsed since the start function was called\n        \"\"\"\n        assert self.started_, \"timer is not started\"\n        self.elapsed_ += time.time() - self.start_time\n        self.started_ = False\n\n    def reset(self):\n        \"\"\"The reset function sets the elapsed time to 0.0 and the started flag to False.\n\n        Args:\n            self: Represent the instance of the class\n\n        Returns:\n            True if the timer was running, false otherwise\n        \"\"\"\n        self.elapsed_ = 0.0\n        self.started_ = False\n\n    def elapsed(self, reset=True):\n        \"\"\"The elapsed function returns the elapsed time in seconds since the timer was started.\n        If reset is True, then it also resets the timer to zero and restarts it.\n        If reset is False, then it leaves the timer running.\n\n        Args:\n            self: Represent the instance of the class\n            reset: Reset the timer\n\n        Returns:\n            The elapsed time in seconds\n        \"\"\"\n        started_ = self.started_\n        if self.started_:\n            self.stop()\n        elapsed_ = self.elapsed_\n        if reset:\n            self.reset()\n        if started_:\n            self.start()\n        return elapsed_\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer.__init__","title":"<code>__init__(name)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the object with a name and initializes other variables.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>name</code> <p>Give the timer a name</p> required <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def __init__(self, name):\n    \"\"\"The __init__ function is called when the class is instantiated.\n    It sets up the object with a name and initializes other variables.\n\n    Args:\n        self: Represent the instance of the class\n        name: Give the timer a name\n\n    Returns:\n        An instance of the class\n    \"\"\"\n    self.name_ = name\n    self.elapsed_ = 0.0\n    self.started_ = False\n    self.start_time = time.time()\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer.elapsed","title":"<code>elapsed(reset=True)</code>","text":"<p>The elapsed function returns the elapsed time in seconds since the timer was started. If reset is True, then it also resets the timer to zero and restarts it. If reset is False, then it leaves the timer running.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>reset</code> <p>Reset the timer</p> <code>True</code> <p>Returns:</p> Type Description <p>The elapsed time in seconds</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def elapsed(self, reset=True):\n    \"\"\"The elapsed function returns the elapsed time in seconds since the timer was started.\n    If reset is True, then it also resets the timer to zero and restarts it.\n    If reset is False, then it leaves the timer running.\n\n    Args:\n        self: Represent the instance of the class\n        reset: Reset the timer\n\n    Returns:\n        The elapsed time in seconds\n    \"\"\"\n    started_ = self.started_\n    if self.started_:\n        self.stop()\n    elapsed_ = self.elapsed_\n    if reset:\n        self.reset()\n    if started_:\n        self.start()\n    return elapsed_\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer.reset","title":"<code>reset()</code>","text":"<p>The reset function sets the elapsed time to 0.0 and the started flag to False.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>True if the timer was running, false otherwise</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def reset(self):\n    \"\"\"The reset function sets the elapsed time to 0.0 and the started flag to False.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        True if the timer was running, false otherwise\n    \"\"\"\n    self.elapsed_ = 0.0\n    self.started_ = False\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer.start","title":"<code>start()</code>","text":"<p>The start function starts the timer.         Args:             None</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes and methods of the class in python</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def start(self):\n    \"\"\"The start function starts the timer.\n            Args:\n                None\n\n    Args:\n        self: Access the attributes and methods of the class in\n            python\n\n    Returns:\n        Nothing\n    \"\"\"\n    assert not self.started_, \"timer has already been started\"\n    self.start_time = time.time()\n    self.started_ = True\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timer.stop","title":"<code>stop()</code>","text":"<p>The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The time elapsed since the start function was called</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def stop(self):\n    \"\"\"The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.\n\n    Args:\n        self: Represent the instance of the class\n\n    Returns:\n        The time elapsed since the start function was called\n    \"\"\"\n    assert self.started_, \"timer is not started\"\n    self.elapsed_ += time.time() - self.start_time\n    self.started_ = False\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timers","title":"<code>Timers</code>","text":"<p>Group of timers.</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>class Timers:\n    \"\"\"Group of timers.\"\"\"\n\n    def __init__(self, use_wandb, tensorboard_writer: flax.metrics.tensorboard.SummaryWriter):\n        self.timers = {}\n        self.use_wandb = use_wandb\n        self.tensorboard_writer = tensorboard_writer\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = Timer(name)\n        return self.timers[name]\n\n    def write(self, names, iteration, normalizer=1.0, reset=False):\n\n        \"\"\"The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp;amp; Biases.\n\n        Args:\n            self: Make the function a method of the class\n            names: Specify which timer(s) to write\n            iteration: Keep track of the number of iterations\n            normalizer: Normalize the time elapsed by a certain value\n            reset: Reset the timer after it has been written to\n                tensorboard\n\n        Returns:\n            Nothing\n        \"\"\"\n        assert normalizer &gt; 0.0\n        for name in names:\n            value = self.timers[name].elapsed(reset=reset) / normalizer\n\n            if self.tensorboard_writer:\n                self.tensorboard_writer.scalar(f\"timers/{name}\", value, iteration)\n\n            if self.use_wandb:\n                if wandb is None:\n                    warnings.warn(\"`wandb` is not installed use `pip install wandb` (use_wandb=True will be ignored)\")\n                    self.use_wandb = False\n                else:\n                    wandb.log({f\"timers/{name}\": value}, step=iteration)\n\n    def log(self, names, normalizer=1.0, reset=True):\n        \"\"\"The log function is used to print the time elapsed for a given function.\n\n        Args:\n            self: Represent the instance of the class\n            names: Specify the name of the timer that we want to log\n            normalizer: Normalize the time taken to run a function\n            reset: Reset the timer after logging\n\n        Returns:\n            The time taken for the given name\n        \"\"\"\n        assert normalizer &gt; 0.0\n\n        if isinstance(names, str):\n            names = [names]\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n            termcolor.cprint(\n                f\"Time Took to Complete Task {name} (microseconds) : \"\n                f\"{termcolor.colored(elapsed_time, color='white', force_color=True)}\",\n                color=\"cyan\",\n                force_color=True\n            )\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timers.log","title":"<code>log(names, normalizer=1.0, reset=True)</code>","text":"<p>The log function is used to print the time elapsed for a given function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>names</code> <p>Specify the name of the timer that we want to log</p> required <code>normalizer</code> <p>Normalize the time taken to run a function</p> <code>1.0</code> <code>reset</code> <p>Reset the timer after logging</p> <code>True</code> <p>Returns:</p> Type Description <p>The time taken for the given name</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def log(self, names, normalizer=1.0, reset=True):\n    \"\"\"The log function is used to print the time elapsed for a given function.\n\n    Args:\n        self: Represent the instance of the class\n        names: Specify the name of the timer that we want to log\n        normalizer: Normalize the time taken to run a function\n        reset: Reset the timer after logging\n\n    Returns:\n        The time taken for the given name\n    \"\"\"\n    assert normalizer &gt; 0.0\n\n    if isinstance(names, str):\n        names = [names]\n    for name in names:\n        elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n        termcolor.cprint(\n            f\"Time Took to Complete Task {name} (microseconds) : \"\n            f\"{termcolor.colored(elapsed_time, color='white', force_color=True)}\",\n            color=\"cyan\",\n            force_color=True\n        )\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.Timers.write","title":"<code>write(names, iteration, normalizer=1.0, reset=False)</code>","text":"<p>The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the function a method of the class</p> required <code>names</code> <p>Specify which timer(s) to write</p> required <code>iteration</code> <p>Keep track of the number of iterations</p> required <code>normalizer</code> <p>Normalize the time elapsed by a certain value</p> <code>1.0</code> <code>reset</code> <p>Reset the timer after it has been written to tensorboard</p> <code>False</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def write(self, names, iteration, normalizer=1.0, reset=False):\n\n    \"\"\"The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp;amp; Biases.\n\n    Args:\n        self: Make the function a method of the class\n        names: Specify which timer(s) to write\n        iteration: Keep track of the number of iterations\n        normalizer: Normalize the time elapsed by a certain value\n        reset: Reset the timer after it has been written to\n            tensorboard\n\n    Returns:\n        Nothing\n    \"\"\"\n    assert normalizer &gt; 0.0\n    for name in names:\n        value = self.timers[name].elapsed(reset=reset) / normalizer\n\n        if self.tensorboard_writer:\n            self.tensorboard_writer.scalar(f\"timers/{name}\", value, iteration)\n\n        if self.use_wandb:\n            if wandb is None:\n                warnings.warn(\"`wandb` is not installed use `pip install wandb` (use_wandb=True will be ignored)\")\n                self.use_wandb = False\n            else:\n                wandb.log({f\"timers/{name}\": value}, step=iteration)\n</code></pre>"},{"location":"generated-utils-utils/#src.python.easydel.utils.utils.get_mesh","title":"<code>get_mesh(shape=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'sp'))</code>","text":"<p>The get_mesh function is a helper function that creates a JAX Mesh object.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Sequence[int]</code> <p>typing.Sequence[int]: Specify the shape of the array that is used to create the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>typing.Sequence[int]: Specify the Axis Names in mesh</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>src/python/easydel/utils/utils.py</code> <pre><code>def get_mesh(\n        shape: typing.Sequence[int] = (1, -1, 1, 1),\n        axis_names: typing.Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\")\n):\n    \"\"\"The get_mesh function is a helper function that creates a JAX Mesh object.\n\n    Args:\n        shape: typing.Sequence[int]: Specify the shape of the array that\n            is used to create the mesh\n        axis_names: typing.Sequence[int]: Specify the Axis Names in mesh\n\n    Returns:\n        A mesh object\n    \"\"\"\n    from jax.sharding import Mesh\n    from jax.experimental import mesh_utils\n    array = jnp.ones((len(jax.devices()), 1)).reshape(shape)\n    return Mesh(mesh_utils.create_device_mesh(array.shape), axis_names)\n</code></pre>"}]}